import pandas as pd
import numpy as np
from tqdm import tqdm
import re
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from collections import Counter, defaultdict
import umap
import hdbscan

warnings.filterwarnings('ignore')


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
def print_step(message):
    print(f"\n{'=' * 60}")
    print(f"üöÄ {message}")
    print(f"{'=' * 60}")


# –î–æ–±–∞–≤–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ UMAP
def find_optimal_umap_params(embeddings, max_components=50, n_trials=10):
    """–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è UMAP"""
    print("üîç –ü–æ–¥–±–∏—Ä–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è UMAP...")

    best_score = -1
    best_params = {}
    best_embeddings = None

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞
    n_components_options = [20, 30, 40, 50, 60, 70, 80,90]
    n_neighbors_options = [10, 15, 20, 25, 30]
    min_dist_options = [0.1, 0.2, 0.3]

    # –°–ª—É—á–∞–π–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏
    trials = min(n_trials, len(n_components_options) * len(n_neighbors_options) * len(min_dist_options))

    for trial in tqdm(range(trials), desc="–ü–æ–∏—Å–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ UMAP"):
        # –°–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        n_components = np.random.choice(n_components_options)
        n_neighbors = np.random.choice(n_neighbors_options)
        min_dist = np.random.choice(min_dist_options)

        try:
            # –ü—Ä–∏–º–µ–Ω—è–µ–º UMAP
            umap_reducer = umap.UMAP(
                n_components=n_components,
                n_neighbors=n_neighbors,
                min_dist=min_dist,
                random_state=42,
                metric='cosine',
                low_memory=False
            )

            embeddings_reduced = umap_reducer.fit_transform(embeddings)

            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            minmax_scaler = MinMaxScaler()
            embeddings_normalized = minmax_scaler.fit_transform(embeddings_reduced)

            # –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ —Å –ø–æ–º–æ—â—å—é –º–∏–Ω–∏-–∫–ª–∞—Å—Ç–µ—Ä–∏–Ω–≥–∞
            test_clusterer = hdbscan.HDBSCAN(
                min_cluster_size=30,
                min_samples=5,
                cluster_selection_epsilon=0.1,
                metric='euclidean'
            )

            test_labels = test_clusterer.fit_predict(embeddings_normalized)

            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            non_noise_mask = test_labels != -1
            if sum(non_noise_mask) > 10:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–µ–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
                score = silhouette_score(embeddings_normalized[non_noise_mask],
                                         test_labels[non_noise_mask])

                # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–∞–∫–∂–µ –ø—Ä–æ—Ü–µ–Ω—Ç —à—É–º–∞
                noise_percentage = (list(test_labels).count(-1) / len(test_labels)) * 100
                adjusted_score = score * (1 - noise_percentage / 200)  # –®—Ç—Ä–∞—Ñ—É–µ–º –∑–∞ –≤—ã—Å–æ–∫–∏–π —à—É–º

                if adjusted_score > best_score:
                    best_score = adjusted_score
                    best_params = {
                        'n_components': n_components,
                        'n_neighbors': n_neighbors,
                        'min_dist': min_dist,
                        'silhouette_score': score,
                        'noise_percentage': noise_percentage,
                        'adjusted_score': adjusted_score
                    }
                    best_embeddings = embeddings_normalized.copy()

        except Exception as e:
            continue

    print(f"‚úÖ –ù–∞–∏–ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã UMAP: {best_params}")
    return best_params, best_embeddings

# –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –±–∞–Ω–∫–æ–≤—Å–∫–æ–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏
def improved_clean_text(text):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
    text = str(text).lower()

    # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –º–µ—à–∞—é—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
    patterns = [
        r'http\S+',  # URL
        r'\S+@\S+',  # email
        r'\+7\s?\(?\d{3}\)?\s?\d{3}[\s-]?\d{2}[\s-]?\d{2}',  # —Ç–µ–ª–µ—Ñ–æ–Ω—ã
        r'\b\d{1,2}\s?(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s?\d{4}?\b',
        # –¥–∞—Ç—ã
    ]

    for pattern in patterns:
        text = re.sub(pattern, '', text)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ü–∏—Ñ—Ä—ã –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã
    text = re.sub(r'[^\w\s\d%$‚Ç¨‚ÇΩ]', ' ', text)
    text = re.sub(r'\s+', ' ', text)

    return text.strip()


def is_valid_review(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—Ç–∑—ã–≤ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º"""
    words = text.split()
    # –û—Ç—Å–µ–∏–≤–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–∑—ã–≤—ã
    if len(words) < 5:
        return False
    # –û—Ç—Å–µ–∏–≤–∞–µ–º –æ—Ç–∑—ã–≤—ã, —Å–æ—Å—Ç–æ—è—â–∏–µ –∏–∑ –æ–¥–Ω–æ–≥–æ –ø–æ–≤—Ç–æ—Ä—è—é—â–µ–≥–æ—Å—è —Å–ª–æ–≤–∞/—Ñ—Ä–∞–∑—ã
    word_count = Counter(words)
    most_common_count = word_count.most_common(1)[0][1]
    if most_common_count / len(words) > 0.5:  # –ï—Å–ª–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ –∑–∞–Ω–∏–º–∞–µ—Ç –±–æ–ª—å—à–µ 50% —Ç–µ–∫—Å—Ç–∞
        return False
    return True


def find_optimal_hdbscan_params(embeddings):
    """–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è HDBSCAN"""
    print("üîç –ü–æ–¥–±–∏—Ä–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è HDBSCAN...")
    best_score = -1
    best_params = {}

    # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    param_combinations = [
        {'min_cluster_size': 20, 'min_samples': 3, 'epsilon': 0.05},
        {'min_cluster_size': 20, 'min_samples': 5, 'epsilon': 0.1},
        {'min_cluster_size': 20, 'min_samples': 10, 'epsilon': 0.15},
        {'min_cluster_size': 30, 'min_samples': 3, 'epsilon': 0.1},
        {'min_cluster_size': 30, 'min_samples': 5, 'epsilon': 0.15},
        {'min_cluster_size': 30, 'min_samples': 10, 'epsilon': 0.05},
        {'min_cluster_size': 40, 'min_samples': 3, 'epsilon': 0.15},
        {'min_cluster_size': 40, 'min_samples': 5, 'epsilon': 0.05},
        {'min_cluster_size': 40, 'min_samples': 10, 'epsilon': 0.1},
    ]

    for params in tqdm(param_combinations, desc="–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"):
        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=params['min_cluster_size'],
            min_samples=params['min_samples'],
            cluster_selection_epsilon=params['epsilon'],
            metric='euclidean'
        )

        labels = clusterer.fit_predict(embeddings)
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

        # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ã
        if n_clusters > 1:
            non_noise_mask = labels != -1
            if sum(non_noise_mask) > 1:
                try:
                    score = silhouette_score(embeddings[non_noise_mask],
                                             labels[non_noise_mask])
                    if score > best_score:
                        best_score = score
                        best_params = params.copy()
                        best_params['score'] = score
                        best_params['n_clusters'] = n_clusters
                        best_params['noise_percentage'] = (list(labels).count(-1) / len(labels)) * 100
                except:
                    continue

    return best_params


def refine_clusters(labels, min_size=50):
    """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–µ–ª–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ—á–µ–Ω—å –º–µ–ª–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
    label_counts = Counter(labels)
    small_clusters = [label for label, count in label_counts.items()
                      if count < min_size and label != -1]

    # –ü–µ—Ä–µ–Ω–∞–∑–Ω–∞—á–∞–µ–º –º–µ–ª–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –∫–∞–∫ —à—É–º
    refined_labels = labels.copy()
    for label in small_clusters:
        refined_labels[refined_labels == label] = -1

    return refined_labels


# –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
print_step("–®–ê–ì 1: –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
print("üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV —Ñ–∞–π–ª–∞...")

try:
    df = pd.read_csv('GaspromBank_dataset.csv', sep=';', encoding='windows-1251', on_bad_lines='warn')
    reviews = df['–û—Ç–∑—ã–≤'].dropna().tolist()
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
    exit()

# –®–∞–≥ 2: –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
print_step("–®–ê–ì 2: –£–õ–£–ß–®–ï–ù–ù–ê–Ø –û–ß–ò–°–¢–ö–ê –¢–ï–ö–°–¢–ê –ò –§–ò–õ–¨–¢–†–ê–¶–ò–Ø")
print("üßπ –¢—â–∞—Ç–µ–ª—å–Ω–æ –æ—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –º—É—Å–æ—Ä–Ω—ã–µ –æ—Ç–∑—ã–≤—ã...")

cleaned_reviews = []
original_reviews_filtered = []

for review in tqdm(reviews, desc="–û—á–∏—Å—Ç–∫–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è"):
    cleaned = improved_clean_text(review)
    if is_valid_review(cleaned):
        cleaned_reviews.append(cleaned)
        original_reviews_filtered.append(review)

print(f"‚úÖ –û—á–∏—â–µ–Ω–æ {len(cleaned_reviews)} –æ—Ç–∑—ã–≤–æ–≤ (–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(reviews) - len(cleaned_reviews)} –º—É—Å–æ—Ä–Ω—ã—Ö)")

# –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
print_step("–®–ê–ì 3: –°–û–ó–î–ê–ù–ò–ï –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–• –≠–ú–ë–ï–î–î–ò–ù–ì–û–í")
print("ü§ñ –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")

try:
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ—â–Ω—É—é –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—É—é –º–æ–¥–µ–ª—å
    model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2') #768-dim
    #model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2') #384-dim
    print("‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    print("üìä –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
    embeddings = model.encode(cleaned_reviews,
                              show_progress_bar=True,
                              batch_size=32,
                              convert_to_numpy=True)
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é {embeddings.shape[1]}")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
    exit()

# –ó–∞–º–µ–Ω—è–µ–º —à–∞–≥ 4 –Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
print_step("–®–ê–ì 4: –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø UMAP –ò –°–ù–ò–ñ–ï–ù–ò–ï –†–ê–ó–ú–ï–†–ù–û–°–¢–ò")
print("üéØ –ü–æ–¥–±–∏—Ä–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã UMAP...")

# –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ UMAP
optimal_umap_params, embeddings_normalized = find_optimal_umap_params(
    embeddings,
    max_components=50,
    n_trials=15
)

# –®–∞–≥ 5: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é HDBSCAN
print_step("–®–ê–ì 5: –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ü–ê–†–ê–ú–ï–¢–†–û–í –ò –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø")

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã UMAP –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
print("üîÑ –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã UMAP –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è...")

final_umap_reducer = umap.UMAP(
    n_components=optimal_umap_params['n_components'],
    n_neighbors=optimal_umap_params['n_neighbors'],
    min_dist=optimal_umap_params['min_dist'],
    random_state=42,
    metric='cosine',
    low_memory=False
)

embeddings_final = final_umap_reducer.fit_transform(embeddings)

# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
minmax_scaler = MinMaxScaler()
embeddings_normalized = minmax_scaler.fit_transform(embeddings_final)

print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {embeddings_normalized.shape}")

# –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
optimal_params = find_optimal_hdbscan_params(embeddings_normalized)
print(f"‚úÖ –ù–∞–∏–ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {optimal_params}")

# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
clusterer = hdbscan.HDBSCAN(
    min_cluster_size=optimal_params['min_cluster_size'],
    min_samples=optimal_params['min_samples'],
    cluster_selection_epsilon=optimal_params['epsilon'],
    metric='euclidean',
    cluster_selection_method='leaf',
    prediction_data=True
)

labels = clusterer.fit_predict(embeddings_normalized)

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –º–µ–ª–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
labels = refine_clusters(labels, min_size=100)

# –®–∞–≥ 5.1: –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —à—É–º–∞
print("üîÑ –ó–∞–ø—É—Å–∫–∞–µ–º –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —à—É–º–∞...")

# 1. –ü–µ—Ä–≤—ã–π —É—Ä–æ–≤–µ–Ω—å: DBSCAN —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
noise_mask = labels == -1
noise_embeddings = embeddings_normalized[noise_mask]

if len(noise_embeddings) > 1000:
    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ —à—É–º–µ
    for eps in [0.3, 0.4, 0.5]:
        secondary_clusterer = DBSCAN(eps=eps, min_samples=15)
        noise_labels_secondary = secondary_clusterer.fit_predict(noise_embeddings)

        # –ù–∞—Ö–æ–¥–∏–º –Ω–æ–≤—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (–Ω–µ —à—É–º)
        new_clusters_mask = noise_labels_secondary != -1
        if sum(new_clusters_mask) > 200:  # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –∑–Ω–∞—á–∏–º—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
            break

    # –ü–µ—Ä–µ–Ω–∞–∑–Ω–∞—á–∞–µ–º –º–µ—Ç–∫–∏
    max_original_label = max(labels)
    new_labels_for_noise = np.where(noise_labels_secondary != -1,
                                    noise_labels_secondary + max_original_label + 1,
                                    -1)

    labels[noise_mask] = new_labels_for_noise
    print(f"‚úÖ –í—ã–¥–µ–ª–µ–Ω–æ {len(set(new_labels_for_noise)) - 1} –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏–∑ —à—É–º–∞")

# –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
unique_labels = set(labels)
n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
n_noise = list(labels).count(-1)

print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
print(f"üìä –í—ã—è–≤–ª–µ–Ω–æ {n_noise} —à—É–º–æ–≤—ã—Ö —Ç–æ—á–µ–∫ ({n_noise / len(cleaned_reviews) * 100:.1f}%)")

# –®–∞–≥ 6: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
print_step("–®–ê–ì 6: –ü–†–û–î–í–ò–ù–£–¢–´–ô –ê–ù–ê–õ–ò–ó –ö–õ–ê–°–¢–ï–†–û–í")
print("üîç –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")

# –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
results_df = pd.DataFrame({
    'review': cleaned_reviews,
    'original_review': original_reviews_filtered,
    'cluster': labels
})

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ä—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ —Å –±–∞–Ω–∫–æ–≤—Å–∫–æ–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–µ–π
extended_russian_stopwords = [
    '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞',
    '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ',
    '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞',
    '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ',
    '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂', '–≤–∞–º', '—Å–∫–∞–∑–∞–ª', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ',
    '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å', '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö',
    '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–∂',
    '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', '–Ω–∏–º', '–∑–¥–µ—Å—å',
    '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '–∫–∞–∂–µ—Ç—Å—è', '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞',
    '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å',
    '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è',
    '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥',
    '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π', '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ',
    '–≤—Å—é', '–º–µ–∂–¥—É', '—ç—Ç–æ', '–≤–æ—Ç', '—Ç–∞–º', '—Ç—É—Ç', '–∑–¥–µ—Å—å', '–æ—á–µ–Ω—å', '–ø—Ä–æ—Å—Ç–æ', '—Å–∏–ª—å–Ω–æ', '—Å–≤–æ–π',
    # –ë–∞–Ω–∫–æ–≤—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    '–±–∞–Ω–∫', '–±–∞–Ω–∫–∞', '–±–∞–Ω–∫–µ', '–±–∞–Ω–∫—É', '–±–∞–Ω–∫–æ–º', '–±–∞–Ω–∫–∏', '–±–∞–Ω–∫–æ–≤',
    '–∫–∞—Ä—Ç–∞', '–∫–∞—Ä—Ç—ã', '–∫–∞—Ä—Ç—É', '–∫–∞—Ä—Ç–µ', '–∫–∞—Ä—Ç–æ–π', '–∫–∞—Ä—Ç',
    '–≥–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫', '–≥–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫–∞', '–≥–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫–µ', '–≥–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫—É', '–≥–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫–æ–º',
    '—Å—á—ë—Ç', '—Å—á–µ—Ç–∞', '—Å—á—ë—Ç–µ', '—Å—á–µ—Ç—É', '—Å—á–µ—Ç–æ–º', '—Å—á–µ—Ç–æ–≤',
    '–¥–µ–Ω—å–≥–∏', '–¥–µ–Ω—å–≥–∞–º', '–¥–µ–Ω—å–≥–∞–º–∏', '–¥–µ–Ω–µ–≥',
    '—Ä—É–±–ª—å', '—Ä—É–±–ª–µ–π', '—Ä—É–±–ª—è–º', '—Ä—É–±–ª—è–º–∏', '—Ä—É–±', '—Ä',
    '–æ—Ç–¥–µ–ª–µ–Ω–∏–µ', '–æ—Ç–¥–µ–ª–µ–Ω–∏—è', '–æ—Ç–¥–µ–ª–µ–Ω–∏—é', '–æ—Ç–¥–µ–ª–µ–Ω–∏–µ–º', '–æ—Ç–¥–µ–ª–µ–Ω–∏–π',
    '–∫–ª–∏–µ–Ω—Ç', '–∫–ª–∏–µ–Ω—Ç–∞', '–∫–ª–∏–µ–Ω—Ç—É', '–∫–ª–∏–µ–Ω—Ç–æ–º', '–∫–ª–∏–µ–Ω—Ç—ã', '–∫–ª–∏–µ–Ω—Ç–æ–≤',
    '—É—Å–ª—É–≥–∞', '—É—Å–ª—É–≥–∏', '—É—Å–ª—É–≥—É', '—É—Å–ª—É–≥–æ–π', '—É—Å–ª—É–≥',
    '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—é', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π',
    '–º–æ–±–∏–ª—å–Ω—ã–π', '–º–æ–±–∏–ª—å–Ω–æ–≥–æ', '–º–æ–±–∏–ª—å–Ω–æ–º—É', '–º–æ–±–∏–ª—å–Ω—ã–º', '–º–æ–±–∏–ª—å–Ω—ã–µ',
    '–æ–Ω–ª–∞–π–Ω', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '–≤–µ–±'
]


# –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–ø–æ–≤—ã—Ö —Å–ª–æ–≤
def get_enhanced_top_words(cluster_texts, n_words=15):
    """–ü–æ–ª—É—á–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Ç–æ–ø–æ–≤—ã–µ —Å–ª–æ–≤–∞ —Å —É—á–µ—Ç–æ–º n-grams –∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤"""
    # –°–æ–∑–¥–∞–µ–º CountVectorizer –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ n-grams
    count_vectorizer = CountVectorizer(
        ngram_range=(1, 3),
        stop_words=extended_russian_stopwords,
        min_df=2,
        max_df=0.8
    )

    try:
        X = count_vectorizer.fit_transform(cluster_texts)
        feature_names = count_vectorizer.get_feature_names_out()

        # –°—É–º–º–∏—Ä—É–µ–º —á–∞—Å—Ç–æ—Ç—ã
        sums = X.sum(axis=0)
        top_indices = np.argsort(sums.A1)[-n_words:][::-1]
        top_words = [feature_names[i] for i in top_indices]

        return top_words
    except:
        return ["–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ_–¥–∞–Ω–Ω—ã—Ö"]


# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º–∏ –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º–∏
category_patterns = {
    '–ö—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã': [
        r'–∫—Ä–µ–¥–∏—Ç–Ω[–∞-—è]* –∫–∞—Ä—Ç[–∞-—è]*', r'–∫—Ä–µ–¥–∏—Ç–∫[–∞-—è]*', r'–∫—ç—à–±—ç–∫',
        r'–ª—å–≥–æ—Ç–Ω[–∞-—è]* –ø–µ—Ä–∏–æ–¥', r'–≥–æ–¥–æ–≤[–∞-—è]* –æ–±—Å–ª—É–∂–∏–≤–∞–Ω', r'–∫—Ä–µ–¥–∏—Ç–Ω[–∞-—è]* –ª–∏–º–∏—Ç',
        r'–±–µ—Å–ø—Ä–æ—Ü–µ–Ω—Ç–Ω[–∞-—è]* –ø–µ—Ä–∏–æ–¥', r'–ø–ª–∞—Ç–µ–∂–Ω[–∞-—è]* —Å–∏—Å—Ç–µ–º[–∞-—è]*',
        r'–ø—Ä–æ—Ü–µ–Ω—Ç–Ω[–∞-—è]* —Å—Ç–∞–≤–∫[–∞-—è]*', r'–±–æ–Ω—É—Å–Ω[–∞-—è]* –ø—Ä–æ–≥—Ä–∞–º–º[–∞-—è]*',
        r'–º–∏–ª—å[–∏]*', r'–±–∞–ª–ª[—ã–æ–≤]*'
    ],
    '–î–µ–±–µ—Ç–æ–≤—ã–µ –∫–∞—Ä—Ç—ã': [
        r'–¥–µ–±–µ—Ç–æ–≤[–∞-—è]* –∫–∞—Ä—Ç[–∞-—è]*', r'–¥–µ–±–µ—Ç–∫[–∞-—è]*', r'–≤—ã–ø—É—Å–∫ –∫–∞—Ä—Ç[–∞-—è]*',
        r'–¥–æ—Å—Ç–∞–≤–∫[–∞-—è]* –∫–∞—Ä—Ç[–∞-—è]*', r'—Å–Ω—è—Ç[–∏—å–µ]* –Ω–∞–ª–∏—á–Ω[–∞-—è]*',
        r'–∑–∞—Ä–ø–ª–∞—Ç–Ω[–∞-—è]* –∫–∞—Ä—Ç[–∞-—è]*', r'–ø–µ–Ω—Å–∏–æ–Ω–Ω[–∞-—è]* –∫–∞—Ä—Ç[–∞-—è]*',
        r'–ø—Ä–µ–º–∏–∞–ª—å–Ω[–∞-—è]* –∫–∞—Ä—Ç[–∞-—è]*', r'–º–æ–º–µ–Ω—Ç–∞–ª—å–Ω[–∞-—è]* –∫–∞—Ä—Ç[–∞-—è]*'
    ],
    '–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ': [
        r'–º–æ–±–∏–ª—å–Ω[–∞-—è]* –ø—Ä–∏–ª–æ–∂–µ–Ω', r'–æ–Ω–ª–∞–π–Ω –±–∞–Ω–∫', r'–ª–∏—á–Ω[–∞-—è]* –∫–∞–±–∏–Ω–µ—Ç',
        r'–≤—Ö–æ–¥ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω', r'—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –ø—Ä–∏–ª–æ–∂–µ–Ω', r'–æ–±–Ω–æ–≤–ª–µ–Ω –ø—Ä–∏–ª–æ–∂–µ–Ω',
        r'–∏–Ω—Ç–µ—Ä–Ω–µ—Ç –±–∞–Ω–∫', r'—É–≤–µ–¥–æ–º–ª–µ–Ω[–∏—è]*', r'–ø—É—à —É–≤–µ–¥–æ–º–ª–µ–Ω',
        r'–±–∏–æ–º–µ—Ç—Ä[–∏—è]*', r'qr[\s-]*–∫–æ–¥'
    ],
    '–î–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ': [
        r'–¥–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω[–∞-—è]* –æ–±—Å–ª—É–∂–∏–≤–∞–Ω', r'—É–¥–∞–ª–µ–Ω–Ω[–∞-—è]* —Å–ª—É–∂–±–∞',
        r'–æ–Ω–ª–∞–π–Ω[\s-]*—É—Å–ª—É–≥[–∏]*', r'–≤–∏–¥–µ–æ–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü[–∏—è]*',
        r'—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω[–∞-—è]* –ø–æ–¥–ø–∏—Å[—å]*', r'—É–¥–∞–ª–µ–Ω–Ω[–∞-—è]* –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü[–∏—è]*',
        r'—Ü–∏—Ñ—Ä–æ–≤[–∞-—è]* —Å–ª—É–∂–±–∞'
    ],
    '–ò–ø–æ—Ç–µ–∫–∞': [
        r'–∏–ø–æ—Ç–µ–∫[–∞-—è]*', r'–∏–ø–æ—Ç–µ—á–Ω[–∞-—è]* –∫—Ä–µ–¥–∏—Ç', r'–ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω[–∞-—è]* –≤–∑–Ω–æ—Å',
        r'–∏–ø–æ—Ç–µ—á–Ω[–∞-—è]* —Å—Ç–∞–≤–∫[–∞-—è]*', r'–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å', r'–∂–∏–ª–∏—â–Ω[–∞-—è]* –∫—Ä–µ–¥–∏—Ç',
        r'–∏–ø–æ—Ç–µ—á–Ω[–∞-—è]* –¥–æ–≥–æ–≤–æ—Ä', r'–∫–≤–∞—Ä—Ç–∏—Ä[–∞—ã]* –≤ –∏–ø–æ—Ç–µ–∫',
        r'—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω[–∏–µ]* –∏–ø–æ—Ç–µ–∫', r'–ª—å–≥–æ—Ç–Ω[–∞-—è]* –∏–ø–æ—Ç–µ–∫'
    ],
    '–ê–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç': [
        r'–∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç[–∞-—è]*', r'–∞–≤—Ç–æ[\s-]*–∫—Ä–µ–¥–∏—Ç', r'–∞–≤—Ç–æ–º–æ–±–∏–ª[—å—è]* –≤ –∫—Ä–µ–¥–∏—Ç',
        r'–∞–≤—Ç–æ[\s-]*–∑–∞–π–º', r'–ø–æ–∫—É–ø–∫[–∞–∏]* –∞–≤—Ç–æ–º–æ–±–∏–ª[—è]*',
        r'–∞–≤—Ç–æ[\s-]*—Å–∞–ª–æ–Ω', r'—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω[–∞-—è]* —Å—Ä–µ–¥—Å—Ç–≤[–∞–æ]*',
        r'–ª—å–≥–æ—Ç–Ω[–∞-—è]* –∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç'
    ],
    '–ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏–µ –∫—Ä–µ–¥–∏—Ç—ã': [
        r'–∫—Ä–µ–¥–∏—Ç –Ω–∞–ª–∏—á–Ω[–∞-—è]*', r'–∑–∞—è–≤–∫[–∞-—è]* –Ω–∞ –∫—Ä–µ–¥–∏—Ç', r'–æ–¥–æ–±—Ä–µ–Ω[–∏–µ]* –∫—Ä–µ–¥–∏—Ç',
        r'–ø–æ–≥–∞—à–µ–Ω[–∏–µ]* –∫—Ä–µ–¥–∏—Ç', r'–∫—Ä–µ–¥–∏—Ç–Ω[–∞-—è]* –∏—Å—Ç–æ—Ä–∏[—è–∏]',
        r'–ø—Ä–æ—Ü–µ–Ω—Ç–Ω[–∞-—è]* —Å—Ç–∞–≤–∫[–∞-—è]*', r'–∫—Ä–µ–¥–∏—Ç–Ω[–∞-—è]* –¥–æ–≥–æ–≤–æ—Ä',
        r'–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫[–∞-—è]* –∫—Ä–µ–¥–∏—Ç', r'–Ω–µ—Ü–µ–ª–µ–≤[–∞-—è]* –∫—Ä–µ–¥–∏—Ç'
    ],
    '–í–∫–ª–∞–¥—ã –∏ —Å—á–µ—Ç–∞': [
        r'–≤–∫–ª–∞–¥[–∞-—è]*', r'–¥–µ–ø–æ–∑–∏—Ç[–∞-—è]*', r'–ø—Ä–æ—Ü–µ–Ω—Ç–Ω[–∞-—è]* —Å—Ç–∞–≤–∫[–∞-—è]*',
        r'–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω[–∞-—è]* —Å—á–µ—Ç', r'—Å–±–µ—Ä–µ–≥–∞—Ç–µ–ª—å–Ω[–∞-—è]* —Å—á–µ—Ç',
        r'–ø–æ–ø–æ–ª–Ω–µ–Ω[–∏–µ]* –≤–∫–ª–∞–¥', r'—Å—Ä–æ—á–Ω[–∞-—è]* –≤–∫–ª–∞–¥', r'–ø—Ä–æ—Ü–µ–Ω—Ç—ã –ø–æ –≤–∫–ª–∞–¥',
        r'—Ä–∞—Å—á–µ—Ç–Ω[–∞-—è]* —Å—á–µ—Ç', r'—Ç–µ–∫—É—â[–∞-—è]* —Å—á–µ—Ç'
    ],
    '–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Å–±–µ—Ä–µ–∂–µ–Ω–∏—è': [
        r'–¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω[–∞-—è]* —Å–±–µ—Ä–µ–∂–µ–Ω', r'–Ω–∞–∫–æ–ø–ª–µ–Ω[–∏–µ]* –Ω–∞ –±—É–¥—É—â–µ–µ',
        r'—Ü–µ–ª–µ–≤[–∞-—è]* –Ω–∞–∫–æ–ø–ª–µ–Ω', r'—Å–±–µ—Ä–µ–∂–µ–Ω[–∏—è]* –¥–µ—Ç[—è–µ–π]*',
        r'–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω[–∞-—è]* –Ω–∞–∫–æ–ø–ª–µ–Ω', r'—Å–±–µ—Ä–µ–≥–∞—Ç–µ–ª—å–Ω[–∞-—è]* –ø—Ä–æ–≥—Ä–∞–º–º[–∞-—è]*',
        r'–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω[–∞-—è]* –ø—Ä–æ–≥—Ä–∞–º–º[–∞-—è]*'
    ],
    '–ü–µ–Ω—Å–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã': [
        r'–ø–µ–Ω—Å–∏–æ–Ω–Ω[–∞-—è]* –Ω–∞–∫–æ–ø–ª–µ–Ω', r'–Ω–µ–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω[–∞-—è]* –ø–µ–Ω—Å–∏–æ–Ω–Ω[–∞-—è]*',
        r'–Ω–ø—Ñ', r'–ø–µ–Ω—Å–∏–æ–Ω–Ω[–∞-—è]* –ø—Ä–æ–≥—Ä–∞–º–º[–∞-—è]*',
        r'–¥–æ–±—Ä–æ–≤–æ–ª—å–Ω[–∞-—è]* –ø–µ–Ω—Å–∏–æ–Ω–Ω[–∞-—è]*', r'–ø–µ–Ω—Å–∏–æ–Ω–Ω[–∞-—è]* —Å—á–µ—Ç',
        r'–Ω–∞–∫–æ–ø–ª–µ–Ω[–∏–µ]* –Ω–∞ –ø–µ–Ω—Å–∏—é', r'–ø–µ–Ω—Å–∏–æ–Ω–Ω[–∞-—è]* –æ–±–µ—Å–ø–µ—á–µ–Ω'
    ],
    '–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏': [
        r'–∏–Ω–≤–µ—Å—Ç–∏—Ü[–∏—è]*', r'–±—Ä–æ–∫–µ—Ä—Å–∫[–∞-—è]* —Å—á–µ—Ç', r'—Ü–µ–Ω–Ω[–∞-—è]* –±—É–º–∞–≥[–∏]*',
        r'—Ñ–æ–Ω–¥–æ–≤[–∞-—è]* —Ä—ã–Ω–æ–∫', r'–∞–∫—Ü–∏[–∏—è]*', r'–æ–±–ª–∏–≥–∞—Ü[–∏—è]*',
        r'–ø–∞[–µ—ë]* —Ñ–æ–Ω–¥', r'–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω[–∞-—è]* –ø—Ä–æ–≥—Ä–∞–º–º[–∞-—è]*',
        r'—É–ø—Ä–∞–≤–ª–µ–Ω[–∏–µ]* –∞–∫—Ç–∏–≤–∞–º–∏', r'—Ç—Ä–µ–π–¥–∏–Ω–≥'
    ],
    '–ú–µ—Ç–∞–ª–ª–∏—á–µ—Å–∫–∏–µ —Å—á–µ—Ç–∞': [
        r'–º–µ—Ç–∞–ª–ª–∏—á[–∞-—è]* —Å—á–µ—Ç', r'–∑–æ–ª–æ—Ç[–∞-—è]* —Å—á–µ—Ç',
        r'–æ–±–µ–∑–ª–∏—á–µ–Ω–Ω[–∞-—è]* –º–µ—Ç–∞–ª–ª–∏—á[–∞-—è]* —Å—á–µ—Ç', r'–æ–º—Å',
        r'–ø–æ–∫—É–ø–∫[–∞–∏]* –∑–æ–ª–æ—Ç[–∞-—è]*', r'–¥—Ä–∞–≥–æ—Ü–µ–Ω–Ω[–∞-—è]* –º–µ—Ç–∞–ª–ª[—ã–æ–≤]*',
        r'—Å–µ—Ä–µ–±—Ä[–æ–∞]*', r'–ø–ª–∞—Ç–∏–Ω[–∞—ã]*'
    ],
    '–°—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ': [
        r'—Å—Ç—Ä–∞—Ö–æ–≤[–∞-—è]*', r'—Å—Ç—Ä–∞—Ö–æ–≤[–∞-—è]* –ø–æ–ª–∏—Å', r'—Å—Ç—Ä–∞—Ö–æ–≤[–∞-—è]* –∫–æ–º–ø–∞–Ω–∏—è',
        r'—Å—Ç—Ä–∞—Ö–æ–≤[–∞-—è]* –≤—ã–ø–ª–∞—Ç[–∞—ã]*', r'—Å—Ç—Ä–∞—Ö–æ–≤[–∞-—è]* —Å–ª—É—á–∞[–π—è]*',
        r'—Å—Ç—Ä–∞—Ö–æ–≤[–∞-—è]* –ø—Ä–µ–º–∏—è', r'–¥–æ–±—Ä–æ–≤–æ–ª—å–Ω[–∞-—è]* —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω',
        r'–æ–±—è–∑–∞—Ç–µ–ª—å–Ω[–∞-—è]* —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω', r'–∫–∞—Å–∫–æ', r'–æ—Å–∞–≥–æ'
    ],
    '–ü–ª–∞—Ç–µ–∂–∏ –∏ –ø–µ—Ä–µ–≤–æ–¥—ã': [
        r'–ø–ª–∞—Ç–µ–∂[–∞-—è]*', r'–ø–µ—Ä–µ–≤–æ–¥[–∞-—è]*', r'–¥–µ–Ω–µ–∂–Ω[–∞-—è]* –ø–µ—Ä–µ–≤–æ–¥',
        r'–æ–ø–ª–∞—Ç[–∞-—è]* —É—Å–ª—É–≥', r'–∫–æ–º–º—É–Ω–∞–ª—å–Ω[–∞-—è]* –ø–ª–∞—Ç–µ–∂',
        r'–∫–≤–∏—Ç–∞–Ω—Ü[–∏—è]*', r'–∞–≤—Ç–æ–ø–ª–∞—Ç–µ–∂', r'–º–µ–∂–±–∞–Ω–∫–æ–≤—Å–∫[–∞-—è]* –ø–µ—Ä–µ–≤–æ–¥',
        r'–±—ã—Å—Ç—Ä[–∞-—è]* –ø–µ—Ä–µ–≤–æ–¥', r'–ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∫–∞—Ä—Ç'
    ],
    '–≠–∫–æ—Å–∏—Å—Ç–µ–º–∞ –∏ –ø–∞—Ä—Ç–Ω–µ—Ä—ã': [
        r'—ç–∫–æ—Å–∏—Å—Ç–µ–º[–∞-—è]*', r'–ø–∞—Ä—Ç–Ω–µ—Ä—Å–∫[–∞-—è]* –ø—Ä–æ–≥—Ä–∞–º–º[–∞-—è]*',
        r'–±–æ–Ω—É—Å–Ω[–∞-—è]* —Å–∏—Å—Ç–µ–º[–∞-—è]*', r'–ª–æ—è–ª—å–Ω–æ—Å—Ç[—å–∏]*',
        r'—Å–∫–∏–¥–∫[–∞–∏]* –ø–∞—Ä—Ç–Ω–µ—Ä', r'—Å–æ–≤–º–µ—Å—Ç–Ω[–∞-—è]* –∞–∫—Ü–∏—è',
        r'–∫–æ–±—Ä–µ–Ω–¥–∏–Ω–≥[–∞-—è]*', r'—Å–æ–≤–º–µ—Å—Ç–Ω[–∞-—è]* –∫–∞—Ä—Ç[–∞-—è]*'
    ]
}


def advanced_category_detection(cluster_texts, category_patterns, top_words_list):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    if not cluster_texts:
        return '–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è'

    all_text = ' '.join(cluster_texts).lower()
    word_count = len(all_text.split())

    category_scores = {}

    # 1. –ü–æ–¥—Å—á–µ—Ç –æ—á–∫–æ–≤ –ø–æ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º
    for category, patterns in category_patterns.items():
        score = 0
        for pattern in patterns:
            matches = re.findall(pattern, all_text)
            score += len(matches) * 3
        if word_count > 0:
            normalized_score = (score / word_count) * 1000
        else:
            normalized_score = 0
        category_scores[category] = normalized_score

    # 2. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ: –ê–Ω–∞–ª–∏–∑ —Ç–æ–ø–æ–≤—ã—Ö —Å–ª–æ–≤
    top_words_str = ' '.join(top_words_list).lower()
    top_words_score = {}
    for category, patterns in category_patterns.items():
        score = 0
        for pattern in patterns:
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–µ –≤–æ –≤—Å–µ–º —Ç–µ–∫—Å—Ç–µ, –∞ –≤ —Ç–æ–ø–æ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö
            matches = re.findall(pattern, top_words_str)
            score += len(matches) * 5  # –î–∞–µ–º –±–æ–ª—å—à–∏–π –≤–µ—Å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º –≤ —Ç–æ–ø–æ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö
        top_words_score[category] = score

    # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ—Ü–µ–Ω–∫–∏ (50% - —Ä–µ–≥—É–ª—è—Ä–∫–∏, 50% - —Ç–æ–ø–æ–≤—ã–µ —Å–ª–æ–≤–∞)
    combined_scores = {}
    for cat in category_scores.keys():
        combined_scores[cat] = (category_scores.get(cat, 0) + top_words_score.get(cat, 0) * 10)  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º

    # 4. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–±–µ–¥–∏—Ç–µ–ª—è —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–º –ø–æ—Ä–æ–≥–æ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    if combined_scores:
        best_category, best_score = max(combined_scores.items(), key=lambda x: x[1])
        second_best_score = sorted(combined_scores.values())[-2] if len(combined_scores) > 1 else 0

        # –ï—Å–ª–∏ –ø–æ–±–µ–¥–∏—Ç–µ–ª—å –Ω–µ —Å–∏–ª—å–Ω–æ –æ—Ç–æ—Ä–≤–∞–ª—Å—è –æ—Ç –≤—Ç–æ—Ä–æ–≥–æ –º–µ—Å—Ç–∞, –∏–ª–∏ –æ–±—â–∏–π —Å—á–µ—Ç –Ω–∏–∑–∫–∏–π - –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ
        if best_score >= 15 and (best_score - second_best_score) > (best_score * 0.3):
            return best_category
        else:
            return '–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ'
    return '–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ'


# –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Ç–µ—Ä (–∏—Å–∫–ª—é—á–∞—è —à—É–º -1)
cluster_info = []
valid_cluster_ids = [label for label in unique_labels if label != -1]

for cluster_id in tqdm(valid_cluster_ids, desc="–ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"):
    cluster_mask = results_df['cluster'] == cluster_id
    cluster_reviews = results_df[cluster_mask]['review'].tolist()
    cluster_size = len(cluster_reviews)

    if cluster_size > 0:
        top_words = get_enhanced_top_words(cluster_reviews, n_words=12)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å —É—á–µ—Ç–æ–º —Ç–æ–ø–æ–≤—ã—Ö —Å–ª–æ–≤
        category = advanced_category_detection(cluster_reviews, category_patterns, top_words)

        # –ë–µ—Ä–µ–º —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
        sample_reviews = cluster_reviews[:2] if len(cluster_reviews) >= 2 else cluster_reviews

        cluster_info.append({
            'cluster_id': cluster_id,
            'size': cluster_size,
            'top_words': ', '.join(top_words),
            'category': category,
            'sample_reviews': sample_reviews
        })

# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É –∫–ª–∞—Å—Ç–µ—Ä–∞
cluster_info.sort(key=lambda x: x['size'], reverse=True)

# –®–∞–≥ 6.1: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
print("üîÑ –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏...")

# –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
merge_map = {
    '–ö—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã': ['–ö—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã'],
    '–î–µ–±–µ—Ç–æ–≤—ã–µ –∫–∞—Ä—Ç—ã': ['–î–µ–±–µ—Ç–æ–≤—ã–µ –∫–∞—Ä—Ç—ã'],
    '–î–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ': ['–î–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ'],
    '–°—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ': ['–°—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ'],
    '–ò–ø–æ—Ç–µ–∫–∞': ['–ò–ø–æ—Ç–µ–∫–∞'],
    '–ê–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç': ['–ê–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç'],
    '–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ': ['–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ'],
    '–ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏–µ –∫—Ä–µ–¥–∏—Ç—ã': ['–ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏–µ –∫—Ä–µ–¥–∏—Ç—ã'],
    '–í–∫–ª–∞–¥—ã –∏ —Å—á–µ—Ç–∞': ['–í–∫–ª–∞–¥—ã –∏ —Å—á–µ—Ç–∞'],
    '–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏': ['–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏'],
    '–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Å–±–µ—Ä–µ–∂–µ–Ω–∏—è': ['–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Å–±–µ—Ä–µ–∂–µ–Ω–∏—è'],
    '–ü–µ–Ω—Å–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã': ['–ü–µ–Ω—Å–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã'],
    '–ú–µ—Ç–∞–ª–ª–∏—á–µ—Å–∫–∏–µ —Å—á–µ—Ç–∞': ['–ú–µ—Ç–∞–ª–ª–∏—á–µ—Å–∫–∏–µ —Å—á–µ—Ç–∞'],
    '–≠–∫–æ—Å–∏—Å—Ç–µ–º–∞ –∏ –ø–∞—Ä—Ç–Ω–µ—Ä—ã': ['–≠–∫–æ—Å–∏—Å—Ç–µ–º–∞ –∏ –ø–∞—Ä—Ç–Ω–µ—Ä—ã'],
    '–ü–ª–∞—Ç–µ–∂–∏ –∏ –ø–µ—Ä–µ–≤–æ–¥—ã': ['–ü–ª–∞—Ç–µ–∂–∏ –∏ –ø–µ—Ä–µ–≤–æ–¥—ã']
}

# –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –∏ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–æ–≥–ª–∞—Å–Ω–æ merge_map
for info in cluster_info:
    current_cat = info['category']
    for new_cat, old_cats in merge_map.items():
        if current_cat in old_cats:
            info['category'] = new_cat
            break

# –¢–µ–ø–µ—Ä—å –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –ø–æ –Ω–æ–≤–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö
merged_cluster_info = []
category_groups = defaultdict(list)

for info in cluster_info:
    category_groups[info['category']].append(info)

# –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –æ–±—ä–µ–¥–∏–Ω—è—è —Ç–µ, —á—Ç–æ –≤ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
for category, clusters in category_groups.items():
    if len(clusters) == 1:
        # –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä –æ–¥–∏–Ω –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ
        merged_cluster_info.append(clusters[0])
    else:
        # –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö
        combined_size = sum(cluster['size'] for cluster in clusters)
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –≤—ã–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ
        all_top_words = []
        for cluster in clusters:
            all_top_words.extend(cluster['top_words'].split(', '))
        # –ë–µ—Ä–µ–º —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
        top_words_combined = [word for word, count in Counter(all_top_words).most_common(12)]
        # –ë–µ—Ä–µ–º –ø—Ä–∏–º–µ—Ä—ã –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ —Å–∞–º–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        sample_reviews = max(clusters, key=lambda x: x['size'])['sample_reviews']

        merged_cluster_info.append({
            'cluster_id': f"Merged_{category}",
            'size': combined_size,
            'top_words': ', '.join(top_words_combined),
            'category': category,
            'sample_reviews': sample_reviews
        })

# –ó–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—ã–π cluster_info –Ω–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π
cluster_info = merged_cluster_info
# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É
cluster_info.sort(key=lambda x: x['size'], reverse=True)

print("\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–¢–ï–†–û–í:")
for info in cluster_info:
    print(f"\nüî∏ –ö–ª–∞—Å—Ç–µ—Ä {info['cluster_id']} ({info['size']} –æ—Ç–∑—ã–≤–æ–≤) - {info['category']}:")
    print(f"   –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {info['top_words']}")
    if info['sample_reviews']:
        print(f"   –ü—Ä–∏–º–µ—Ä: '{info['sample_reviews'][0][:100]}...'")

# –®–∞–≥ 7: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
print_step("–®–ê–ì 7: –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")

# –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π DataFrame
category_map = {info['cluster_id']: info['category'] for info in cluster_info}
results_df['category'] = results_df['cluster'].map(category_map)
# –î–ª—è —à—É–º–æ–≤—ã—Ö —Ç–æ—á–µ–∫ —Å—Ç–∞–≤–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é "–®—É–º"
results_df.loc[results_df['cluster'] == -1, 'category'] = '–®—É–º'

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
results_df.to_csv('banking_reviews_clustered_improved.csv', index=False, encoding='utf-8')
print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: banking_reviews_clustered_improved.csv")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
cluster_details = []
for info in cluster_info:
    cluster_details.append({
        'cluster_id': info['cluster_id'],
        'size': info['size'],
        'category': info['category'],
        'top_words': info['top_words'],
        'sample_review_1': info['sample_reviews'][0] if len(info['sample_reviews']) > 0 else '',
        'sample_review_2': info['sample_reviews'][1] if len(info['sample_reviews']) > 1 else ''
    })

cluster_df = pd.DataFrame(cluster_details)
cluster_df.to_csv('clusters_info_detailed.csv', index=False, encoding='utf-8')
print("‚úÖ –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: clusters_info_detailed.csv")

# –®–∞–≥ 8: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
print_step("–®–ê–ì 8: –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
print("üìä –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
plt.figure(figsize=(14, 10))
category_sizes = {info['category']: info['size'] for info in cluster_info}
category_df = pd.DataFrame(list(category_sizes.items()), columns=['Category', 'Size'])
category_df = category_df.sort_values('Size', ascending=True)

plt.barh(category_df['Category'], category_df['Size'], color=plt.cm.Set3(np.arange(len(category_df))))
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º', fontsize=14, fontweight='bold')
plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤', fontsize=12)
plt.tight_layout()
plt.savefig('category_distribution.png', dpi=300, bbox_inches='tight')
print("‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω: category_distribution.png")

# –®–∞–≥ 9: –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
print_step("–®–ê–ì 9: –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò")
print("üìà –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...")

# –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ-—à—É–º–æ–≤—ã–µ —Ç–æ—á–∫–∏
non_noise_mask = labels != -1
if sum(non_noise_mask) > 1:
    silhouette_avg = silhouette_score(embeddings_normalized[non_noise_mask], labels[non_noise_mask])
    print(f"‚úÖ Silhouette Score: {silhouette_avg:.3f}")
else:
    print("‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–µ–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ silhouette score")

# –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
print(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
print(f"   ‚Ä¢ –í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {len(cleaned_reviews)}")
print(f"   ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}")
print(f"   ‚Ä¢ –®—É–º–æ–≤—ã–µ —Ç–æ—á–∫–∏: {n_noise} ({n_noise / len(cleaned_reviews) * 100:.1f}%)")
print(f"   ‚Ä¢ –ö–∞—Ç–µ–≥–æ—Ä–∏–π –≤—ã—è–≤–ª–µ–Ω–æ: {len(set([info['category'] for info in cluster_info]))}")

# –í—ã–≤–æ–¥–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
print(f"\nüèÜ –ö–õ–ê–°–¢–ï–†–´:")
top_categories = sorted(cluster_info, key=lambda x: x['size'], reverse=True)[:min(10, len(cluster_info))]
for i, cat in enumerate(top_categories, 1):
    print(f"   {i}. {cat['category']}")

print_step("–ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
print("üéâ –í—Å–µ —ç—Ç–∞–ø—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª—ã:")
print("   ‚Ä¢ banking_reviews_clustered_improved.csv - –ø–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
print("   ‚Ä¢ clusters_info_detailed.csv - –¥–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö")
print("   ‚Ä¢ category_distribution.png - –≥—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π")