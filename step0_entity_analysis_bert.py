# Step0: –ê–Ω–∞–ª–∏–∑ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ –±–∞–Ω–∫–æ–≤—Å–∫–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ (–£–õ–£–ß–®–ï–ù–ù–ê–Ø BERT –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø)
import pandas as pd
import re
import joblib
from tqdm import tqdm
import logging
import nltk
from rapidfuzz import fuzz
import sys
import os
import numpy as np
from sklearn.cluster import DBSCAN
from sentence_transformers import SentenceTransformer, util
from collections import defaultdict
import torch
import hashlib
import pickle
from functools import lru_cache
from transformers import AutoTokenizer, AutoModelForTokenClassification
import razdel
from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, Doc
from transformers import pipeline
import re

# +++ –î–û–ë–ê–í–õ–Ø–ï–ú –ú–ù–û–ì–û–ü–û–¢–û–ß–ù–û–°–¢–¨ +++
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing
from threading import Lock
import time

# –°–∫–∞—á–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è nltk
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    filename='bank_entity_analysis_bert_enhanced.log',
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s'
)

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ BERT –º–æ–¥–µ–ª–∏
bert_lock = Lock()

# +++ –ö–≠–®–ò–†–û–í–ê–ù–ò–ï BERT –≠–ú–ë–ï–î–î–ò–ù–ì–û–í +++
bert_embedding_cache = {}
CACHE_FILE = "bert_embedding_cache.pkl"


class ProfessionalSegmenter:
    def __init__(self):
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Natasha
            from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, Doc
            self.segmenter = Segmenter()
            self.morph_vocab = MorphVocab()
            self.emb = NewsEmbedding()
            self.morph_tagger = NewsMorphTagger(self.emb)
            self.natasha_available = True
        except ImportError:
            print_warning("Natasha –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é")
            self.natasha_available = False

        # BERT –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        try:
            from transformers import pipeline
            self.bert_classifier = pipeline(
                "text-classification",
                model="cointegrated/rubert-tiny2",
                tokenizer="cointegrated/rubert-tiny2"
            )
            self.bert_available = True
        except:
            print_warning("BERT –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            self.bert_classifier = None
            self.bert_available = False

    # –í –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞ step0_entity_analysis_bert.py, –≤ –∫–ª–∞—Å—Å–µ ProfessionalSegmenter
    def razdel_segmentation(self, text):
        """–¢–æ—á–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å razdel"""
        if text is None:
            return []
        try:
            import razdel
            return [sentence.text for sentence in razdel.sentenize(text)]
        except ImportError:
            # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç—É—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é
            print_warning("Razdel –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é")
            import re
            return re.split(r'(?<=[.!?])\s+', text)
        except Exception as e:
            print_warning(f"–û—à–∏–±–∫–∞ razdel —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é")
            import re
            return re.split(r'(?<=[.!?])\s+', text)


    def natasha_analysis(self, text):
        """–ì–ª—É–±–æ–∫–∏–π –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å Natasha"""
        if not self.natasha_available or text is None:
            return None
        try:
            from natasha import Doc
            doc = Doc(text)
            doc.segment(self.segmenter)
            doc.tag_morph(self.morph_tagger)
            return doc
        except:
            return None

    def extract_entities(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏"""
        if text is None:
            return set()

        doc = self.natasha_analysis(text)
        entities = set()

        if doc and hasattr(doc, 'tokens'):
            for token in doc.tokens:
                if hasattr(token, 'pos') and token.pos in ['NOUN', 'PROPN']:
                    entities.add(token.text.lower())

        return entities

    def detect_products(self, text):
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –±–∞–Ω–∫–æ–≤—Å–∫–∏–µ –ø—Ä–æ–¥—É–∫—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ"""
        if text is None:
            return set()

        products_found = set()
        text_lower = text.lower()

        for product, keywords in PRODUCT_ENTITIES.items():
            for keyword in keywords:
                if re.search(r'\b' + re.escape(keyword) + r'\b', text_lower):
                    products_found.add(product)
                    break

        return products_found

    def are_products_compatible(self, products1, products2):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –º–æ–∂–Ω–æ –ª–∏ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å —Ç–µ–∫—Å—Ç—ã –ø—Ä–æ —Ä–∞–∑–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã"""
        if not products1 or not products2:
            return True

        # –ì—Ä—É–ø–ø—ã —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤
        compatible_groups = [
            {'–¥–µ–±–µ—Ç–æ–≤—ã–µ –∫–∞—Ä—Ç—ã', '–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã', '–ø—Ä–µ–º–∏–∞–ª—å–Ω—ã–µ –∫–∞—Ä—Ç—ã', '–∑–∞—Ä–ø–ª–∞—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã'},
            {'–≤–∫–ª–∞–¥—ã', '–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–µ —Å—á–µ—Ç–∞', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏'},
            {'–∫—Ä–µ–¥–∏—Ç—ã', '–∏–ø–æ—Ç–µ–∫–∞', '–∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ', '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –∫—Ä–µ–¥–∏—Ç–æ–≤'},
            {'–º–æ–±–∏–ª—å–Ω–∞—è —Å–≤—è–∑—å', '–º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫'},
            {'–±–æ–Ω—É—Å–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã', '—Å—Ç—Ä–∞—Ö–æ–≤—ã–µ –∏ —Å–µ—Ä–≤–∏—Å–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã'}
        ]

        for group in compatible_groups:
            if products1.intersection(group) and products2.intersection(group):
                return True

        return False

    def hybrid_segmentation(self, text):
        """–ì–∏–±—Ä–∏–¥–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è: –ø—Ä–∞–≤–∏–ª–∞ + —Å–µ–º–∞–Ω—Ç–∏–∫–∞"""
        if text is None or len(text.strip()) == 0:
            return [text]

        # –°–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = self.razdel_segmentation(text)

        if len(sentences) <= 1:
            return [text]

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–æ—á–∫–∏ —Ä–∞–∑—Ä—ã–≤–∞ –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º
        break_points = self.find_break_points(sentences)

        # –°–æ–∑–¥–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã
        segments = []
        start_idx = 0

        for break_idx in break_points:
            segment_sentences = sentences[start_idx:break_idx]
            if segment_sentences:
                segments.append(' '.join(segment_sentences))
            start_idx = break_idx

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
        if start_idx < len(sentences):
            segments.append(' '.join(sentences[start_idx:]))

        return segments if segments else [text]

    def find_break_points(self, sentences):
        """–ù–∞—Ö–æ–¥–∏—Ç —Ç–æ—á–∫–∏ —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏"""
        break_points = []

        for i in range(1, len(sentences)):
            current_sent = sentences[i - 1]
            next_sent = sentences[i]

            # –ü—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞–∑—Ä—ã–≤–∞ —Ç–µ–º—ã
            if self.is_topic_break(current_sent, next_sent):
                break_points.append(i)

        return break_points

    def is_topic_break(self, sent1, sent2):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –µ—Å—Ç—å –ª–∏ —Å–º–µ–Ω–∞ —Ç–µ–º—ã –º–µ–∂–¥—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏"""
        if sent1 is None or sent2 is None:
            return False

        # –°–º–µ–Ω–∞ —Ç–∏–ø–∞ –ø—Ä–æ–¥—É–∫—Ç–∞
        products1 = self.detect_products(sent1)
        products2 = self.detect_products(sent2)

        if products1 and products2 and not self.are_products_compatible(products1, products2):
            return True

        # –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ä–∞–∑—Ä—ã–≤ –≤ –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–∏
        time_indicators = ['–ø–æ—Ç–æ–º', '–∑–∞—Ç–µ–º', '–¥–∞–ª–µ–µ', '–ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ', '–Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –¥–µ–Ω—å']
        if any(indicator in sent2.lower() for indicator in time_indicators):
            return True

        # –†–µ–∑–∫–∞—è —Å–º–µ–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ -> –ø—Ä–æ–¥—É–∫—Ç)
        service_indicators = ['–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ', '—Å–µ—Ä–≤–∏—Å', '–º–µ–Ω–µ–¥–∂–µ—Ä', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫', '–æ—Ç–≤–µ—Ç–∏–ª']
        sent1_has_service = any(indicator in sent1.lower() for indicator in service_indicators)
        sent2_has_service = any(indicator in sent2.lower() for indicator in service_indicators)

        products1 = self.detect_products(sent1)
        products2 = self.detect_products(sent2)

        # –ï—Å–ª–∏ –≤ –æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ, –∞ –≤ –¥—Ä—É–≥–æ–º –ø—Ä–æ–¥—É–∫—Ç - —Ä–∞–∑–¥–µ–ª—è–µ–º
        if (sent1_has_service and not sent2_has_service and products2) or \
                (sent2_has_service and not sent1_has_service and products1):
            return True

        return False


# üî• –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–ò
def professional_bert_segmentation(review_text, segmenter, verbose=False):
    """
    –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–ê–Ø —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å —Ç–æ—á–Ω—ã–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Å—É—â–Ω–æ—Å—Ç–µ–π
    """
    if review_text is None or len(review_text.strip()) == 0:
        return []

    if verbose:
        print(f"üìù –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç: {review_text[:100]}...")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä
    segments = segmenter.hybrid_segmentation(review_text)

    if verbose:
        print(f"üéØ –ü–æ–ª—É—á–µ–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segments)}")

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Å–µ–≥–º–µ–Ω—Ç
    result_segments = []

    for i, segment_text in enumerate(segments):
        if not segment_text or len(segment_text.strip()) == 0:
            continue

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –≤ —Å–µ–≥–º–µ–Ω—Ç–µ
        entities = detect_entities_with_bert(segment_text, None)

        if not entities:
            # –ï—Å–ª–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            entities = [{
                'entity_type': '–æ–±—â–µ–µ_–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ',
                'combined_name': '–æ–±—â–µ–µ_–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ',
                'entity_category': 'general_service',
                'has_service_context': contains_service_context(segment_text),
                'confidence': 'medium',
                'detection_method': 'professional_segmenter'
            }]

        for entity_info in entities:
            result_segments.append({
                'text': segment_text,
                'entity_type': entity_info['entity_type'],
                'service_channel': None,
                'combined_entity_name': entity_info['combined_name'],
                'entity_category': entity_info['entity_category'],
                'is_entity_specific': entity_info['entity_type'] in PRODUCT_ENTITIES,
                'has_service_context': entity_info['has_service_context'],
                'confidence': entity_info['confidence'],
                'segmentation_method': 'professional_hybrid',
                'semantic_coherence': 1.0,
                'detection_method': entity_info.get('detection_method', 'professional')
            })

    return result_segments if result_segments else [{
        'text': review_text,
        'entity_type': '–æ–±—â–µ–µ_–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ',
        'service_channel': None,
        'combined_entity_name': '–æ–±—â–µ–µ_–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ',
        'entity_category': 'general_service',
        'is_entity_specific': False,
        'has_service_context': contains_service_context(review_text),
        'confidence': 'medium',
        'segmentation_method': 'professional_fallback',
        'semantic_coherence': 1.0,
        'detection_method': 'fallback'
    }]

def load_embedding_cache():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞"""
    global bert_embedding_cache
    try:
        if os.path.exists(CACHE_FILE):
            with open(CACHE_FILE, 'rb') as f:
                loaded_cache = pickle.load(f)
                for key, value in loaded_cache.items():
                    if isinstance(value, torch.Tensor):
                        bert_embedding_cache[key] = value.cpu()
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –∫—ç—à BERT —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(bert_embedding_cache)} –∑–∞–ø–∏—Å–µ–π")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
        bert_embedding_cache = {}


def save_embedding_cache():
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ —Ñ–∞–π–ª"""
    try:
        if len(bert_embedding_cache) > 10000:
            keys_to_keep = list(bert_embedding_cache.keys())[-5000:]
            bert_embedding_cache.clear()
            for k in keys_to_keep:
                if k in bert_embedding_cache:
                    bert_embedding_cache[k] = bert_embedding_cache[k]

        with open(CACHE_FILE, 'wb') as f:
            pickle.dump(bert_embedding_cache, f)
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")


def get_text_hash(text):
    """–°–æ–∑–¥–∞–µ—Ç —Ö—ç—à —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–∞–∫ –∫–ª—é—á –∫—ç—à–∞"""
    return hashlib.md5(text.strip().encode('utf-8')).hexdigest()


# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫—ç—à –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
load_embedding_cache()

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫—ç—à –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
import atexit

atexit.register(save_embedding_cache)


def print_step(message):
    print(f"\n\033[1;36m>>> {message}\033[0m")


def print_success(message):
    print(f"\033[1;32m‚úì {message}\033[0m")


def print_warning(message):
    print(f"\033[1;33m‚ö† {message}\033[0m")


def print_error(message):
    print(f"\033[1;31m‚úó {message}\033[0m")


def print_info(message):
    print(f"\033[1;34m‚Ñπ {message}\033[0m")


# === –ë–ê–ù–ö–û–í–°–ö–ò–ï –°–£–©–ù–û–°–¢–ò ===
PRODUCT_ENTITIES = {
    '–±–æ–Ω—É—Å–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã': [
        '–±–æ–Ω—É—Å–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã', '–±–æ–Ω—É—Å–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞', '–±–æ–Ω—É—Å–Ω—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º', '–±–æ–Ω—É—Å–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã',
        '–±–æ–Ω—É—Å–Ω—ã–º –ø—Ä–æ–≥—Ä–∞–º–º–∞–º', '–±–æ–Ω—É—Å–Ω—ã–º–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∞–º–∏', '–±–æ–Ω—É—Å–Ω—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∞—Ö',
        '35%', '–∫–µ—à–±—ç–∫', '–∫—ç—à–±–µ–∫', '–∞–∫—Ü–∏—è –ø—Ä–∏–≤–µ–¥–∏ –¥—Ä—É–≥–∞', '–≥–∞–∑–ø—Ä–æ–º –±–æ–Ω—É—Å', '–±–æ–Ω—É—Å—ã', '–±–æ–Ω—É—Å',
        '–ø–æ–ª—É—á–∞—Ç—å –±–æ–Ω—É—Å—ã', '–ø–æ–ª—É—á–∞—é –±–æ–Ω—É—Å—ã', '–ø–æ–ª—É—á–∞–µ—à—å –±–æ–Ω—É—Å—ã', '–ø–æ–ª—É—á–∞–µ—Ç –±–æ–Ω—É—Å—ã',
        '–ø–æ–ª—É—á–∞–µ–º –±–æ–Ω—É—Å—ã', '–ø–æ–ª—É—á–∞–µ—Ç–µ –±–æ–Ω—É—Å—ã', '–ø–æ–ª—É—á–∞—é—Ç –±–æ–Ω—É—Å—ã', '–ø–æ–ª—É—á–∏—Ç—å –±–æ–Ω—É—Å—ã',
        '–ø–æ–ª—É—á–∏–ª –±–æ–Ω—É—Å—ã', '–ø–æ–ª—É—á–∏–ª–∞ –±–æ–Ω—É—Å—ã', '–ø–æ–ª—É—á–∏–ª–∏ –±–æ–Ω—É—Å—ã', '–Ω–∞—á–∏—Å–ª—è—Ç—å –±–æ–Ω—É—Å—ã',
        '–Ω–∞—á–∏—Å–ª—è—é –±–æ–Ω—É—Å—ã', '–Ω–∞—á–∏—Å–ª—è–µ—à—å –±–æ–Ω—É—Å—ã', '–Ω–∞—á–∏—Å–ª—è–µ—Ç –±–æ–Ω—É—Å—ã', '–Ω–∞—á–∏—Å–ª–∏—Ç—å –±–æ–Ω—É—Å—ã',
        '–Ω–∞—á–∏—Å–ª–∏–ª –±–æ–Ω—É—Å—ã', '–Ω–∞—á–∏—Å–ª–∏–ª–∞ –±–æ–Ω—É—Å—ã', '–Ω–∞—á–∏—Å–ª–∏–ª–∏ –±–æ–Ω—É—Å—ã', '–±–æ–Ω—É—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞',
        '–ø—Ä–æ–≥—Ä–∞–º–º–∞ –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏', '–ø—Ä–æ–≥—Ä–∞–º–º—ã –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏', '–ª–æ—è–ª—å–Ω–æ—Å—Ç—å', '–≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ',
        '–≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è', '–≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—é', '–≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ–º', '–≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–∏'
    ],

    '–¥–µ–±–µ—Ç–æ–≤—ã–µ –∫–∞—Ä—Ç—ã': [
        '–¥–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞', '–¥–µ–±–µ—Ç–æ–≤–æ–π –∫–∞—Ä—Ç—ã', '–¥–µ–±–µ—Ç–æ–≤—É—é –∫–∞—Ä—Ç—É', '–¥–µ–±–µ—Ç–æ–≤–æ–π –∫–∞—Ä—Ç–æ–π',
        '–¥–µ–±–µ—Ç–æ–≤—ã–µ –∫–∞—Ä—Ç—ã', '–¥–µ–±–µ—Ç–æ–≤—ã—Ö –∫–∞—Ä—Ç', '–¥–µ–±–µ—Ç–æ–≤—ã–º –∫–∞—Ä—Ç–∞–º', '–¥–µ–±–µ—Ç–æ–≤—ã–º–∏ –∫–∞—Ä—Ç–∞–º–∏',
        '–∫–∞—Ä—Ç–∞ –º–∏—Ä', '–∫–∞—Ä—Ç—ã –º–∏—Ä', '–∫–∞—Ä—Ç—É –º–∏—Ä', '–∫–∞—Ä—Ç–æ–π –º–∏—Ä', '–∫–∞—Ä—Ç–µ –º–∏—Ä',
        '–∫–∞—Ä—Ç–∞ –≥–∞–∑–∞', '–∫–∞—Ä—Ç—ã –≥–∞–∑–∞', '–∫–∞—Ä—Ç—É –≥–∞–∑–∞', '–∫–∞—Ä—Ç–æ–π –≥–∞–∑–∞', '–∫–∞—Ä—Ç–µ –≥–∞–∑–∞',
        '–∫–∞—Ä—Ç–∞ –≥–ø–±', '–∫–∞—Ä—Ç—ã –≥–ø–±', '–∫–∞—Ä—Ç—É –≥–ø–±', '–∫–∞—Ä—Ç–æ–π –≥–ø–±', '–∫–∞—Ä—Ç–µ –≥–ø–±',
        '–¥–µ–±–µ—Ç–∫–∞', '–¥–µ–±–µ—Ç–∫–∏', '–¥–µ–±–µ—Ç–∫—É', '–¥–µ–±–µ—Ç–∫–æ–π', '–¥–µ–±–µ—Ç–∫–µ',
        '–æ—Ñ–æ—Ä–º–∏—Ç—å –¥–µ–±–µ—Ç–æ–≤—É—é –∫–∞—Ä—Ç—É', '–æ—Ñ–æ—Ä–º–∏–ª –¥–µ–±–µ—Ç–æ–≤—É—é –∫–∞—Ä—Ç—É', '–æ—Ñ–æ—Ä–º–∏–ª–∞ –¥–µ–±–µ—Ç–æ–≤—É—é –∫–∞—Ä—Ç—É',
        '–∑–∞–∫–∞–∑–∞—Ç—å –∫–∞—Ä—Ç—É', '–∑–∞–∫–∞–∑–∞–ª –∫–∞—Ä—Ç—É', '–∑–∞–∫–∞–∑–∞–ª–∞ –∫–∞—Ä—Ç—É', '–ø–æ–ª—É—á–∏—Ç—å –∫–∞—Ä—Ç—É', '–ø–æ–ª—É—á–∏–ª –∫–∞—Ä—Ç—É',
        '–ø–æ–ª—É—á–∏–ª–∞ –∫–∞—Ä—Ç—É', '–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞—Ä—Ç—É', '–∏—Å–ø–æ–ª—å–∑—É—é –∫–∞—Ä—Ç—É', '–∏—Å–ø–æ–ª—å–∑—É–µ—à—å –∫–∞—Ä—Ç—É',
        '–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–∞—Ä—Ç—É', '–ø–ª–∞—Ç–∏—Ç—å –∫–∞—Ä—Ç–æ–π', '–ø–ª–∞—á—É –∫–∞—Ä—Ç–æ–π', '–ø–ª–∞—Ç–∏—à—å –∫–∞—Ä—Ç–æ–π', '–ø–ª–∞—Ç–∏—Ç –∫–∞—Ä—Ç–æ–π',
        '—Ä–∞—Å—á–µ—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞', '–ø–ª–∞—Ç–µ–∂–Ω–∞—è –∫–∞—Ä—Ç–∞', '–±–∞–Ω–∫–æ–≤—Å–∫–∞—è –∫–∞—Ä—Ç–∞', '–ø–ª–∞—Å—Ç–∏–∫–æ–≤–∞—è –∫–∞—Ä—Ç–∞', '–∫–∞—Ä—Ç–∞ –±–∞–Ω–∫–∞'
    ],

    '–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–µ —Å—á–µ—Ç–∞': [
        '–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–π —Å—á–µ—Ç', '–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å—á–µ—Ç–∞', '–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω–æ–º—É —Å—á–µ—Ç—É',
        '–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–º —Å—á–µ—Ç–æ–º', '–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω–æ–º —Å—á–µ—Ç–µ', '–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–µ —Å—á–µ—Ç–∞',
        '–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—á–µ—Ç–æ–≤', '–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–º —Å—á–µ—Ç–∞–º', '–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Å—á–µ—Ç–∞–º–∏',
        '–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—á–µ—Ç–∞—Ö', '–Ω—Å', '–µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç', '—Å—Ç–∞–≤–∫–∞ –Ω–∞ –æ—Å—Ç–∞—Ç–æ–∫',
        '–Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è', '–Ω–∞–∫–æ–ø–ª–µ–Ω–∏–π', '–Ω–∞–∫–æ–ø–ª–µ–Ω–∏—é', '–Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è–º–∏',
        '–æ—Ç–∫—Ä—ã—Ç—å –Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–π —Å—á–µ—Ç', '–æ—Ç–∫—Ä—ã–ª –Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–π —Å—á–µ—Ç', '–æ—Ç–∫—Ä—ã–ª–∞ –Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–π —Å—á–µ—Ç',
        '–ø–æ–ø–æ–ª–Ω—è—Ç—å —Å—á–µ—Ç', '–ø–æ–ø–æ–ª–Ω—è—é —Å—á–µ—Ç', '–ø–æ–ø–æ–ª–Ω—è–µ—à—å —Å—á–µ—Ç', '–ø–æ–ø–æ–ª–Ω—è–µ—Ç —Å—á–µ—Ç',
        '–ø–æ–ø–æ–ª–Ω–∏—Ç—å —Å—á–µ—Ç', '–ø–æ–ø–æ–ª–Ω–∏–ª —Å—á–µ—Ç', '–ø–æ–ø–æ–ª–Ω–∏–ª–∞ —Å—á–µ—Ç', '—Å–Ω–∏–º–∞—Ç—å —Å–æ —Å—á–µ—Ç–∞',
        '—Å–Ω–∏–º–∞—é —Å–æ —Å—á–µ—Ç–∞', '—Å–Ω–∏–º–∞–µ—à—å —Å–æ —Å—á–µ—Ç–∞', '—Å–Ω–∏–º–∞–µ—Ç —Å–æ —Å—á–µ—Ç–∞', '–Ω–∞–∫–æ–ø–ª—è—Ç—å –¥–µ–Ω—å–≥–∏',
        '–Ω–∞–∫–æ–ø–ª—è—é –¥–µ–Ω—å–≥–∏', '–Ω–∞–∫–æ–ø–ª—è–µ—à—å –¥–µ–Ω—å–≥–∏', '–Ω–∞–∫–æ–ø–ª—è–µ—Ç –¥–µ–Ω—å–≥–∏', '–∫–æ–ø–∏—Ç—å –¥–µ–Ω—å–≥–∏',
        '–∫–æ–ø–ª—é –¥–µ–Ω—å–≥–∏', '–∫–æ–ø–∏—à—å –¥–µ–Ω—å–≥–∏', '–∫–æ–ø–∏—Ç –¥–µ–Ω—å–≥–∏', '–Ω–∞–∫–æ–ø–∏—Ç—å –¥–µ–Ω—å–≥–∏',
        '–Ω–∞–∫–æ–ø–∏–ª –¥–µ–Ω—å–≥–∏', '–Ω–∞–∫–æ–ø–∏–ª–∞ –¥–µ–Ω—å–≥–∏', '–ø—Ä–æ—Ü–µ–Ω—Ç—ã –Ω–∞ –æ—Å—Ç–∞—Ç–æ–∫', '–Ω–∞—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤'
    ],

    '–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã': [
        '–∫—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞', '–∫—Ä–µ–¥–∏—Ç–Ω–æ–π –∫–∞—Ä—Ç—ã', '–∫—Ä–µ–¥–∏—Ç–Ω—É—é –∫–∞—Ä—Ç—É', '–∫—Ä–µ–¥–∏—Ç–Ω–æ–π –∫–∞—Ä—Ç–æ–π',
        '–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã', '–∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –∫–∞—Ä—Ç', '–∫—Ä–µ–¥–∏—Ç–Ω—ã–º –∫–∞—Ä—Ç–∞–º', '–∫—Ä–µ–¥–∏—Ç–Ω—ã–º–∏ –∫–∞—Ä—Ç–∞–º–∏',
        '–∫—Ä–µ–¥–∏—Ç–∫–∞', '–∫—Ä–µ–¥–∏—Ç–∫–∏', '–∫—Ä–µ–¥–∏—Ç–∫—É', '–∫—Ä–µ–¥–∏—Ç–∫–æ–π', '–∫—Ä–µ–¥–∏—Ç–∫–µ',
        '–∫–∞—Ä—Ç–∞ —Å –∫—Ä–µ–¥–∏—Ç–Ω—ã–º –ª–∏–º–∏—Ç–æ–º', '–∫–∞—Ä—Ç—ã —Å –∫—Ä–µ–¥–∏—Ç–Ω—ã–º –ª–∏–º–∏—Ç–æ–º', '–∫–∞—Ä—Ç—É —Å –∫—Ä–µ–¥–∏—Ç–Ω—ã–º –ª–∏–º–∏—Ç–æ–º',
        '–∫—Ä–µ–¥–∏—Ç–Ω—ã–π –ª–∏–º–∏—Ç', '–∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞', '–∫—Ä–µ–¥–∏—Ç–Ω–æ–º—É –ª–∏–º–∏—Ç—É', '–∫—Ä–µ–¥–∏—Ç–Ω—ã–º –ª–∏–º–∏—Ç–æ–º',
        '–æ—Ñ–æ—Ä–º–∏—Ç—å –∫—Ä–µ–¥–∏—Ç–Ω—É—é –∫–∞—Ä—Ç—É', '–æ—Ñ–æ—Ä–º–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—É—é –∫–∞—Ä—Ç—É', '–æ—Ñ–æ—Ä–º–∏–ª–∞ –∫—Ä–µ–¥–∏—Ç–Ω—É—é –∫–∞—Ä—Ç—É',
        '–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∫—Ä–µ–¥–∏—Ç–∫–æ–π', '–ø–æ–ª—å–∑—É—é—Å—å –∫—Ä–µ–¥–∏—Ç–∫–æ–π', '–ø–æ–ª—å–∑—É–µ—à—å—Å—è –∫—Ä–µ–¥–∏—Ç–∫–æ–π',
        '–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫—Ä–µ–¥–∏—Ç–∫–æ–π', '—Ç—Ä–∞—Ç–∏—Ç—å —Å –∫—Ä–µ–¥–∏—Ç–∫–∏', '—Ç—Ä–∞—á—É —Å –∫—Ä–µ–¥–∏—Ç–∫–∏', '—Ç—Ä–∞—Ç–∏—à—å —Å –∫—Ä–µ–¥–∏—Ç–∫–∏',
        '—Ç—Ä–∞—Ç–∏—Ç —Å –∫—Ä–µ–¥–∏—Ç–∫–∏', '–ø–æ–≥–∞—à–∞—Ç—å –∫—Ä–µ–¥–∏—Ç–∫—É', '–ø–æ–≥–∞—à–∞—é –∫—Ä–µ–¥–∏—Ç–∫—É', '–ø–æ–≥–∞—à–∞–µ—à—å –∫—Ä–µ–¥–∏—Ç–∫—É',
        '–ø–æ–≥–∞—à–∞–µ—Ç –∫—Ä–µ–¥–∏—Ç–∫—É', '–∫—Ä–µ–¥–∏—Ç–Ω—ã–π –ª–∏–º–∏—Ç', '–ª–∏–º–∏—Ç –ø–æ –∫–∞—Ä—Ç–µ', '–∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç—å –ø–æ –∫–∞—Ä—Ç–µ'
    ],

    '–∫—Ä–µ–¥–∏—Ç—ã': [
        '–∫—Ä–µ–¥–∏—Ç', '–∫—Ä–µ–¥–∏—Ç–∞', '–∫—Ä–µ–¥–∏—Ç—É', '–∫—Ä–µ–¥–∏—Ç–æ–º', '–∫—Ä–µ–¥–∏—Ç–µ', '–∫—Ä–µ–¥–∏—Ç—ã', '–∫—Ä–µ–¥–∏—Ç–æ–≤',
        '–∫—Ä–µ–¥–∏—Ç–∞–º', '–∫—Ä–µ–¥–∏—Ç–∞–º–∏', '–∫—Ä–µ–¥–∏—Ç–∞—Ö', '–∑–∞–µ–º', '–∑–∞–µ–º–∞', '–∑–∞–µ–º—É', '–∑–∞–µ–º–æ–º', '–∑–∞–µ–º–µ',
        '–∑–∞–π–º', '–∑–∞–π–º–∞', '–∑–∞–π–º—É', '–∑–∞–π–º–æ–º', '–∑–∞–π–º–µ', '—Å—Å—É–¥–∞', '—Å—Å—É–¥—ã', '—Å—Å—É–¥—É', '—Å—Å—É–¥–æ–π',
        '—Å—Å—É–¥–µ', '—Å—Å—É–¥', '—Å—Å—É–¥–∞–º', '—Å—Å—É–¥–∞–º–∏', '—Å—Å—É–¥–∞—Ö', '–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ', '–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏—è',
        '–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏—é', '–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ–º', '–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–∏',
        '–≤–∑—è—Ç—å –∫—Ä–µ–¥–∏—Ç', '–≤–∑—è–ª –∫—Ä–µ–¥–∏—Ç', '–≤–∑—è–ª–∞ –∫—Ä–µ–¥–∏—Ç', '–±—Ä–∞—Ç—å –∫—Ä–µ–¥–∏—Ç', '–±–µ—Ä—É –∫—Ä–µ–¥–∏—Ç',
        '–±–µ—Ä–µ—à—å –∫—Ä–µ–¥–∏—Ç', '–±–µ—Ä–µ—Ç –∫—Ä–µ–¥–∏—Ç', '–æ—Ñ–æ—Ä–º–∏—Ç—å –∫—Ä–µ–¥–∏—Ç', '–æ—Ñ–æ—Ä–º–∏–ª –∫—Ä–µ–¥–∏—Ç', '–æ—Ñ–æ—Ä–º–∏–ª–∞ –∫—Ä–µ–¥–∏—Ç',
        '–ø–æ–≥–∞—à–∞—Ç—å –∫—Ä–µ–¥–∏—Ç', '–ø–æ–≥–∞—à–∞—é –∫—Ä–µ–¥–∏—Ç', '–ø–æ–≥–∞—à–∞–µ—à—å –∫—Ä–µ–¥–∏—Ç', '–ø–æ–≥–∞—à–∞–µ—Ç –∫—Ä–µ–¥–∏—Ç',
        '–≤—ã–¥–∞–≤–∞—Ç—å –∫—Ä–µ–¥–∏—Ç', '–≤—ã–¥–∞—é –∫—Ä–µ–¥–∏—Ç', '–≤—ã–¥–∞–µ—à—å –∫—Ä–µ–¥–∏—Ç', '–≤—ã–¥–∞–µ—Ç –∫—Ä–µ–¥–∏—Ç', '–≤—ã–¥–∞—Ç—å –∫—Ä–µ–¥–∏—Ç',
        '–≤—ã–¥–∞–ª –∫—Ä–µ–¥–∏—Ç', '–≤—ã–¥–∞–ª–∞ –∫—Ä–µ–¥–∏—Ç', '–æ–¥–æ–±—Ä–∏—Ç—å –∫—Ä–µ–¥–∏—Ç', '–æ–¥–æ–±—Ä–∏–ª –∫—Ä–µ–¥–∏—Ç', '–æ–¥–æ–±—Ä–∏–ª–∞ –∫—Ä–µ–¥–∏—Ç',
        '–æ—Ç–∫–∞–∑–∞—Ç—å –≤ –∫—Ä–µ–¥–∏—Ç–µ', '–æ—Ç–∫–∞–∑–∞–ª –≤ –∫—Ä–µ–¥–∏—Ç–µ', '–æ—Ç–∫–∞–∑–∞–ª–∞ –≤ –∫—Ä–µ–¥–∏—Ç–µ', '–ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è —Å—Ç–∞–≤–∫–∞',
        '—Å—Ç–∞–≤–∫–∞ –ø–æ –∫—Ä–µ–¥–∏—Ç—É', '—É—Å–ª–æ–≤–∏—è –∫—Ä–µ–¥–∏—Ç–∞', '–∫—Ä–µ–¥–∏—Ç–Ω—ã–π –¥–æ–≥–æ–≤–æ—Ä'
    ],

    '–º–æ–±–∏–ª—å–Ω–∞—è —Å–≤—è–∑—å': [
        '–º–æ–±–∏–ª—å–Ω–∞—è —Å–≤—è–∑—å', '–º–æ–±–∏–ª—å–Ω–æ–π —Å–≤—è–∑–∏', '–º–æ–±–∏–ª—å–Ω—É—é —Å–≤—è–∑—å', '–º–æ–±–∏–ª—å–Ω–æ–π —Å–≤—è–∑—å—é',
        '—Å–∏–º –∫–∞—Ä—Ç–∞', '—Å–∏–º –∫–∞—Ä—Ç—ã', '—Å–∏–º –∫–∞—Ä—Ç—É', '—Å–∏–º –∫–∞—Ä—Ç–æ–π', '—Å–∏–º –∫–∞—Ä—Ç–µ',
        '—Å–∏–º-–∫–∞—Ä—Ç–∞', '—Å–∏–º-–∫–∞—Ä—Ç—ã', '—Å–∏–º-–∫–∞—Ä—Ç—É', '—Å–∏–º-–∫–∞—Ä—Ç–æ–π', '—Å–∏–º-–∫–∞—Ä—Ç–µ',
        '–≥–∞–∑–ø—Ä–æ–º –º–æ–±–∞–π–ª', '—Ç–∞—Ä–∏—Ñ —Å–≤—è–∑–∏', '–º–æ–±–∏–ª—å–Ω—ã–π —Ç–∞—Ä–∏—Ñ', '–º–æ–±–∏–ª—å–Ω–æ–≥–æ —Ç–∞—Ä–∏—Ñ–∞',
        '–º–æ–±–∏–ª—å–Ω–æ–º—É —Ç–∞—Ä–∏—Ñ—É', '–º–æ–±–∏–ª—å–Ω—ã–º —Ç–∞—Ä–∏—Ñ–æ–º',
        '–ø–æ–¥–∫–ª—é—á–∏—Ç—å —Å–∏–º-–∫–∞—Ä—Ç—É', '–ø–æ–¥–∫–ª—é—á–∏–ª —Å–∏–º-–∫–∞—Ä—Ç—É', '–ø–æ–¥–∫–ª—é—á–∏–ª–∞ —Å–∏–º-–∫–∞—Ä—Ç—É',
        '–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Å–≤—è–∑—å—é', '–ø–æ–ª—å–∑—É—é—Å—å —Å–≤—è–∑—å—é', '–ø–æ–ª—å–∑—É–µ—à—å—Å—è —Å–≤—è–∑—å—é', '–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–≤—è–∑—å—é',
        '—Å–º–µ–Ω–∏—Ç—å —Ç–∞—Ä–∏—Ñ', '—Å–º–µ–Ω–∏–ª —Ç–∞—Ä–∏—Ñ', '—Å–º–µ–Ω–∏–ª–∞ —Ç–∞—Ä–∏—Ñ', '–ø–æ–¥–∫–ª—é—á–∏—Ç—å —Ç–∞—Ä–∏—Ñ',
        '–ø–æ–¥–∫–ª—é—á–∏–ª —Ç–∞—Ä–∏—Ñ', '–ø–æ–¥–∫–ª—é—á–∏–ª–∞ —Ç–∞—Ä–∏—Ñ', '–º–æ–±–∏–ª—å–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä', '–æ–ø–µ—Ä–∞—Ç–æ—Ä —Å–≤—è–∑–∏'
    ],

    '–º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ': [
        '–º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–º–æ–±–∏–ª—å–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è', '–º–æ–±–∏–ª—å–Ω–æ–º—É –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—é',
        '–º–æ–±–∏–ª—å–Ω—ã–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º', '–º–æ–±–∏–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏', '–º–æ–±–∏–ª—å–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è',
        '–º–æ–±–∏–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π', '–º–æ–±–∏–ª—å–Ω—ã–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º', '–º–æ–±–∏–ª—å–Ω—ã–º–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º–∏',
        '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –±–∞–Ω–∫–∞', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –±–∞–Ω–∫–∞', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—é –±–∞–Ω–∫–∞', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º –±–∞–Ω–∫–∞',
        '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≥–ø–±', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≥–ø–±', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—é –≥–ø–±', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º –≥–ø–±',
        '–º–æ–±–∏–ª—å–Ω—ã–π –±–∞–Ω–∫', '–º–æ–±–∏–ª—å–Ω–æ–≥–æ –±–∞–Ω–∫–∞', '–º–æ–±–∏–ª—å–Ω–æ–º—É –±–∞–Ω–∫—É', '–º–æ–±–∏–ª—å–Ω—ã–º –±–∞–Ω–∫–æ–º',
        '—Å–∫–∞—á–∞—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—Å–∫–∞—á–∞–ª –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—Å–∫–∞—á–∞–ª–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ',
        '—É—Å—Ç–∞–Ω–æ–≤–∏–ª –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—É—Å—Ç–∞–Ω–æ–≤–∏–ª–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º',
        '–ø–æ–ª—å–∑—É—é—Å—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º', '–ø–æ–ª—å–∑—É–µ—à—å—Å—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º', '–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º',
        '–∑–∞–π—Ç–∏ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–∑–∞—Ö–æ–∂—É –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–∑–∞—Ö–æ–¥–∏—à—å –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–∑–∞—Ö–æ–¥–∏—Ç –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ',
        '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∞–µ—Ç', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Ç–æ—Ä–º–æ–∑–∏—Ç', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç',
        '–æ–±–Ω–æ–≤–∏—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–æ–±–Ω–æ–≤–∏–ª –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–æ–±–Ω–æ–≤–∏–ª–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è'
    ],

    '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫': [
        '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫–∞', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫—É', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫–æ–º',
        '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫–µ', '–æ–Ω–ª–∞–π–Ω –±–∞–Ω–∫', '–æ–Ω–ª–∞–π–Ω –±–∞–Ω–∫–∞', '–æ–Ω–ª–∞–π–Ω –±–∞–Ω–∫—É', '–æ–Ω–ª–∞–π–Ω –±–∞–Ω–∫–æ–º',
        '–ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç', '–ª–∏—á–Ω–æ–≥–æ –∫–∞–±–∏–Ω–µ—Ç–∞', '–ª–∏—á–Ω–æ–º—É –∫–∞–±–∏–Ω–µ—Ç—É', '–ª–∏—á–Ω—ã–º –∫–∞–±–∏–Ω–µ—Ç–æ–º',
        '–∫–∞–±–∏–Ω–µ—Ç –≥–ø–±', '–∫–∞–±–∏–Ω–µ—Ç–∞ –≥–ø–±', '–∫–∞–±–∏–Ω–µ—Ç—É –≥–ø–±', '–∫–∞–±–∏–Ω–µ—Ç–æ–º –≥–ø–±',
        '–≤–æ–π—Ç–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫', '–≤—Ö–æ–∂—É –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫', '–≤—Ö–æ–¥–∏—à—å –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫',
        '–≤—Ö–æ–¥–∏—Ç –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫', '–∑–∞–π—Ç–∏ –≤ –ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç', '–∑–∞—Ö–æ–∂—É –≤ –ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç',
        '–∑–∞—Ö–æ–¥–∏—à—å –≤ –ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç', '–∑–∞—Ö–æ–¥–∏—Ç –≤ –ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç', '—Ä–∞–±–æ—Ç–∞—Ç—å —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫',
        '—Ä–∞–±–æ—Ç–∞—é —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫', '—Ä–∞–±–æ—Ç–∞–µ—à—å —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫', '—Ä–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫',
        '–æ–Ω–ª–∞–π–Ω —É—Å–ª—É–≥–∏', '–¥–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ', '—É–¥–∞–ª–µ–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ —Å—á–µ—Ç—É'
    ],

    '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏': [
        '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è–º', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è–º–∏', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è—Ö',
        '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–π —Å—á–µ—Ç', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ —Å—á–µ—Ç–∞', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–æ–º—É —Å—á–µ—Ç—É',
        '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–º —Å—á–µ—Ç–æ–º', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–æ–º —Å—á–µ—Ç–µ', '–∏–∏—Å', '–∞–∫—Ü–∏–∏', '–∞–∫—Ü–∏–π',
        '–∞–∫—Ü–∏—è–º', '–∞–∫—Ü–∏—è–º–∏', '–∞–∫—Ü–∏—è—Ö', '–æ–±–ª–∏–≥–∞—Ü–∏–∏', '–æ–±–ª–∏–≥–∞—Ü–∏–π', '–æ–±–ª–∏–≥–∞—Ü–∏—è–º',
        '–æ–±–ª–∏–≥–∞—Ü–∏—è–º–∏', '–æ–±–ª–∏–≥–∞—Ü–∏—è—Ö', '—Ü–µ–Ω–Ω—ã–µ –±—É–º–∞–≥–∏', '—Ü–µ–Ω–Ω—ã—Ö –±—É–º–∞–≥', '—Ü–µ–Ω–Ω—ã–º –±—É–º–∞–≥–∞–º',
        '–∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–µ–Ω—å–≥–∏', '–∏–Ω–≤–µ—Å—Ç–∏—Ä—É—é –¥–µ–Ω—å–≥–∏', '–∏–Ω–≤–µ—Å—Ç–∏—Ä—É–µ—à—å –¥–µ–Ω—å–≥–∏', '–∏–Ω–≤–µ—Å—Ç–∏—Ä—É–µ—Ç –¥–µ–Ω—å–≥–∏',
        '–≤–ª–æ–∂–∏—Ç—å –¥–µ–Ω—å–≥–∏', '–≤–ª–æ–∂–∏–ª –¥–µ–Ω—å–≥–∏', '–≤–ª–æ–∂–∏–ª–∞ –¥–µ–Ω—å–≥–∏', '–≤–∫–ª–∞–¥—ã–≤–∞—Ç—å –¥–µ–Ω—å–≥–∏',
        '–≤–∫–ª–∞–¥—ã–≤–∞—é –¥–µ–Ω—å–≥–∏', '–≤–∫–ª–∞–¥—ã–≤–∞–µ—à—å –¥–µ–Ω—å–≥–∏', '–≤–∫–ª–∞–¥—ã–≤–∞–µ—Ç –¥–µ–Ω—å–≥–∏', '–ø–æ–∫—É–ø–∞—Ç—å –∞–∫—Ü–∏–∏',
        '–ø–æ–∫—É–ø–∞—é –∞–∫—Ü–∏–∏', '–ø–æ–∫—É–ø–∞–µ—à—å –∞–∫—Ü–∏–∏', '–ø–æ–∫—É–ø–∞–µ—Ç –∞–∫—Ü–∏–∏', '–∫—É–ø–∏—Ç—å –∞–∫—Ü–∏–∏',
        '–∫—É–ø–∏–ª –∞–∫—Ü–∏–∏', '–∫—É–ø–∏–ª–∞ –∞–∫—Ü–∏–∏', '–ø—Ä–æ–¥–∞–≤–∞—Ç—å –∞–∫—Ü–∏–∏', '–ø—Ä–æ–¥–∞—é –∞–∫—Ü–∏–∏', '–ø—Ä–æ–¥–∞–µ—à—å –∞–∫—Ü–∏–∏',
        '–ø—Ä–æ–¥–∞–µ—Ç –∞–∫—Ü–∏–∏', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–π –ø–æ—Ä—Ç—Ñ–µ–ª—å', '–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π', '—Ä–∏—Å–∫–∏ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π'
    ],

    '–∏–ø–æ—Ç–µ–∫–∞': [
        '–∏–ø–æ—Ç–µ–∫–∞', '–∏–ø–æ—Ç–µ–∫–∏', '–∏–ø–æ—Ç–µ–∫–µ', '–∏–ø–æ—Ç–µ–∫—É', '–∏–ø–æ—Ç–µ–∫–æ–π', '–∏–ø–æ—Ç–µ–∫–æ—é',
        '–∏–ø–æ—Ç–µ—á–Ω—ã–π –∫—Ä–µ–¥–∏—Ç', '–∏–ø–æ—Ç–µ—á–Ω–æ–≥–æ –∫—Ä–µ–¥–∏—Ç–∞', '–∏–ø–æ—Ç–µ—á–Ω–æ–º—É –∫—Ä–µ–¥–∏—Ç—É',
        '–∏–ø–æ—Ç–µ—á–Ω—ã–º –∫—Ä–µ–¥–∏—Ç–æ–º', '–∫—Ä–µ–¥–∏—Ç –Ω–∞ –∂–∏–ª—å–µ', '–∫—Ä–µ–¥–∏—Ç–∞ –Ω–∞ –∂–∏–ª—å–µ', '–∫—Ä–µ–¥–∏—Ç—É –Ω–∞ –∂–∏–ª—å–µ',
        '–∫—Ä–µ–¥–∏—Ç–æ–º –Ω–∞ –∂–∏–ª—å–µ', '–∏–ø–æ—Ç–µ—á–Ω–æ–µ –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ', '–∏–ø–æ—Ç–µ—á–Ω–æ–≥–æ –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏—è',
        '–≤–∑—è—Ç—å –∏–ø–æ—Ç–µ–∫—É', '–≤–∑—è–ª –∏–ø–æ—Ç–µ–∫—É', '–≤–∑—è–ª–∞ –∏–ø–æ—Ç–µ–∫—É', '–æ—Ñ–æ—Ä–º–∏—Ç—å –∏–ø–æ—Ç–µ–∫—É',
        '–æ—Ñ–æ—Ä–º–∏–ª –∏–ø–æ—Ç–µ–∫—É', '–æ—Ñ–æ—Ä–º–∏–ª–∞ –∏–ø–æ—Ç–µ–∫—É', '–ø–æ–≥–∞—à–∞—Ç—å –∏–ø–æ—Ç–µ–∫—É', '–ø–æ–≥–∞—à–∞—é –∏–ø–æ—Ç–µ–∫—É',
        '–ø–æ–≥–∞—à–∞–µ—à—å –∏–ø–æ—Ç–µ–∫—É', '–ø–æ–≥–∞—à–∞–µ—Ç –∏–ø–æ—Ç–µ–∫—É', '–∏–ø–æ—Ç–µ—á–Ω–∞—è —Å—Ç–∞–≤–∫–∞', '—Å—Ç–∞–≤–∫–∞ –ø–æ –∏–ø–æ—Ç–µ–∫–µ',
        '–∏–ø–æ—Ç–µ—á–Ω–∞—è –∫–≤–∞—Ä—Ç–∏—Ä–∞', '–∏–ø–æ—Ç–µ—á–Ω–æ–µ –∂–∏–ª—å–µ', '–ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—ã–π –≤–∑–Ω–æ—Å', '–∏–ø–æ—Ç–µ—á–Ω—ã–π –±—Ä–æ–∫–µ—Ä'
    ],

    '–≤–∫–ª–∞–¥—ã': [
        '–≤–∫–ª–∞–¥', '–≤–∫–ª–∞–¥–∞', '–≤–∫–ª–∞–¥—É', '–≤–∫–ª–∞–¥–æ–º', '–≤–∫–ª–∞–¥–µ', '–≤–∫–ª–∞–¥—ã', '–≤–∫–ª–∞–¥–æ–≤',
        '–≤–∫–ª–∞–¥–∞–º', '–≤–∫–ª–∞–¥–∞–º–∏', '–≤–∫–ª–∞–¥–∞—Ö', '–¥–µ–ø–æ–∑–∏—Ç', '–¥–µ–ø–æ–∑–∏—Ç–∞', '–¥–µ–ø–æ–∑–∏—Ç—É',
        '–¥–µ–ø–æ–∑–∏—Ç–æ–º', '–¥–µ–ø–æ–∑–∏—Ç–µ', '–¥–µ–ø–æ–∑–∏—Ç—ã', '–¥–µ–ø–æ–∑–∏—Ç–æ–≤', '–¥–µ–ø–æ–∑–∏—Ç–∞–º', '–¥–µ–ø–æ–∑–∏—Ç–∞–º–∏',
        '—Å—Ä–æ—á–Ω—ã–π –≤–∫–ª–∞–¥', '—Å—Ä–æ—á–Ω–æ–≥–æ –≤–∫–ª–∞–¥–∞', '—Å—Ä–æ—á–Ω–æ–º—É –≤–∫–ª–∞–¥—É', '—Å—Ä–æ—á–Ω—ã–º –≤–∫–ª–∞–¥–æ–º',
        '—Å–±–µ—Ä–µ–≥–∞—Ç–µ–ª—å–Ω—ã–π –≤–∫–ª–∞–¥', '—Å–±–µ—Ä–µ–≥–∞—Ç–µ–ª—å–Ω–æ–≥–æ –≤–∫–ª–∞–¥–∞', '—Å–±–µ—Ä–µ–≥–∞—Ç–µ–ª—å–Ω–æ–º—É –≤–∫–ª–∞–¥—É',
        '—Å–±–µ—Ä–µ–≥–∞—Ç–µ–ª—å–Ω—ã–º –≤–∫–ª–∞–¥–æ–º',
        '–æ—Ç–∫—Ä—ã—Ç—å –≤–∫–ª–∞–¥', '–æ—Ç–∫—Ä—ã–ª –≤–∫–ª–∞–¥', '–æ—Ç–∫—Ä—ã–ª–∞ –≤–∫–ª–∞–¥', '–æ—Ñ–æ—Ä–º–∏—Ç—å –¥–µ–ø–æ–∑–∏—Ç',
        '–æ—Ñ–æ—Ä–º–∏–ª –¥–µ–ø–æ–∑–∏—Ç', '–æ—Ñ–æ—Ä–º–∏–ª–∞ –¥–µ–ø–æ–∑–∏—Ç', '–ø–æ–ª–æ–∂–∏—Ç—å –¥–µ–Ω—å–≥–∏ –Ω–∞ –≤–∫–ª–∞–¥',
        '–ø–æ–ª–æ–∂–∏–ª –¥–µ–Ω—å–≥–∏ –Ω–∞ –≤–∫–ª–∞–¥', '–ø–æ–ª–æ–∂–∏–ª–∞ –¥–µ–Ω—å–≥–∏ –Ω–∞ –≤–∫–ª–∞–¥', '—Å–Ω—è—Ç—å —Å –≤–∫–ª–∞–¥–∞',
        '—Å–Ω—è–ª —Å –≤–∫–ª–∞–¥–∞', '—Å–Ω—è–ª–∞ —Å –≤–∫–ª–∞–¥–∞', '–ø—Ä–æ—Ü–µ–Ω—Ç—ã –ø–æ –≤–∫–ª–∞–¥—É', '–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –≤–∫–ª–∞–¥–∞',
        '—Å—Ç–∞–≤–∫–∞ –ø–æ –≤–∫–ª–∞–¥—É', '—Å—Ä–æ–∫ –≤–∫–ª–∞–¥–∞', '–ø–æ–ø–æ–ª–Ω—è–µ–º—ã–π –≤–∫–ª–∞–¥', '—Å–±–µ—Ä–µ–≥–∞—Ç–µ–ª—å–Ω—ã–π —Å—á–µ—Ç'
    ],

    '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω—ã–µ —É—Å–ª—É–≥–∏': [
        '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω—ã–µ —É—Å–ª—É–≥–∏', '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω—ã—Ö —É—Å–ª—É–≥', '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω—ã–º —É—Å–ª—É–≥–∞–º',
        '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω—ã–º–∏ —É—Å–ª—É–≥–∞–º–∏', '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω—ã—Ö —É—Å–ª—É–≥–∞—Ö', '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω–∞—è —É—Å–ª—É–≥–∞',
        '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω–æ–π —É—Å–ª—É–≥–∏', '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω—É—é —É—Å–ª—É–≥—É', '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω–æ–π —É—Å–ª—É–≥–æ–π',
        '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–∏–π', '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–∏—è', '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–∏—é', '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–∏–µ–º', '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–∏–∏',
        '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ', '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è', '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω–æ–º—É —Ö—Ä–∞–Ω–µ–Ω–∏—é',
        '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω—ã–º —Ö—Ä–∞–Ω–µ–Ω–∏–µ–º', '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω–æ–º —Ö—Ä–∞–Ω–µ–Ω–∏–∏',
        '—Ö—Ä–∞–Ω–∏—Ç—å —Ü–µ–Ω–Ω—ã–µ –±—É–º–∞–≥–∏', '—Ö—Ä–∞–Ω—é —Ü–µ–Ω–Ω—ã–µ –±—É–º–∞–≥–∏', '—Ö—Ä–∞–Ω–∏—à—å —Ü–µ–Ω–Ω—ã–µ –±—É–º–∞–≥–∏',
        '—Ö—Ä–∞–Ω–∏—Ç —Ü–µ–Ω–Ω—ã–µ –±—É–º–∞–≥–∏', '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ', '—É—á–µ—Ç —Ü–µ–Ω–Ω—ã—Ö –±—É–º–∞–≥',
        '—Ä–µ–µ—Å—Ç—Ä –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤', '–¥–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω–∞—è —Ä–∞—Å–ø–∏—Å–∫–∞', '–∫–∞—Å—Ç–æ–¥–∏–∞–ª—å–Ω—ã–µ —É—Å–ª—É–≥–∏'
    ],

    '–¥–µ–Ω–µ–∂–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã': [
        '–¥–µ–Ω–µ–∂–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã', '–¥–µ–Ω–µ–∂–Ω—ã—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤', '–¥–µ–Ω–µ–∂–Ω—ã–º –ø–µ—Ä–µ–≤–æ–¥–∞–º',
        '–¥–µ–Ω–µ–∂–Ω—ã–º–∏ –ø–µ—Ä–µ–≤–æ–¥–∞–º–∏', '–¥–µ–Ω–µ–∂–Ω—ã—Ö –ø–µ—Ä–µ–≤–æ–¥–∞—Ö', '–¥–µ–Ω–µ–∂–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥',
        '–¥–µ–Ω–µ–∂–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞', '–¥–µ–Ω–µ–∂–Ω–æ–º—É –ø–µ—Ä–µ–≤–æ–¥—É', '–¥–µ–Ω–µ–∂–Ω—ã–º –ø–µ—Ä–µ–≤–æ–¥–æ–º',
        '–ø–µ—Ä–µ–≤–æ–¥ –¥–µ–Ω–µ–≥', '–ø–µ—Ä–µ–≤–æ–¥–∞ –¥–µ–Ω–µ–≥', '–ø–µ—Ä–µ–≤–æ–¥—É –¥–µ–Ω–µ–≥', '–ø–µ—Ä–µ–≤–æ–¥–æ–º –¥–µ–Ω–µ–≥',
        '–ø–µ—Ä–µ–≤–æ–¥ —Å—Ä–µ–¥—Å—Ç–≤', '–ø–µ—Ä–µ–≤–æ–¥–∞ —Å—Ä–µ–¥—Å—Ç–≤', '–ø–µ—Ä–µ–≤–æ–¥—É —Å—Ä–µ–¥—Å—Ç–≤', '–ø–µ—Ä–µ–≤–æ–¥–æ–º —Å—Ä–µ–¥—Å—Ç–≤',
        '–æ—Ç–ø—Ä–∞–≤–∫–∞ –¥–µ–Ω–µ–≥', '–æ—Ç–ø—Ä–∞–≤–∫–∏ –¥–µ–Ω–µ–≥', '–æ—Ç–ø—Ä–∞–≤–∫–µ –¥–µ–Ω–µ–≥', '–æ—Ç–ø—Ä–∞–≤–∫–æ–π –¥–µ–Ω–µ–≥',
        '–ø–æ–ª—É—á–µ–Ω–∏–µ –¥–µ–Ω–µ–≥', '–ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ–Ω–µ–≥', '–ø–æ–ª—É—á–µ–Ω–∏—é –¥–µ–Ω–µ–≥', '–ø–æ–ª—É—á–µ–Ω–∏–µ–º –¥–µ–Ω–µ–≥',
        '–ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ–≤–æ–∂—É –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ–≤–æ–¥–∏—à—å –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ–≤–æ–¥–∏—Ç –¥–µ–Ω—å–≥–∏',
        '–ø–µ—Ä–µ–≤–æ–¥–∏–º –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ–≤–æ–¥–∏—Ç–µ –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ–≤–æ–¥—è—Ç –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –¥–µ–Ω—å–≥–∏',
        '–ø–µ—Ä–µ–≤—ë–ª –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ–≤–µ–ª–∞ –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ–≤–µ–ª–∏ –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ–≤–æ–¥—è –¥–µ–Ω—å–≥–∏',
        '–æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –¥–µ–Ω—å–≥–∏', '–æ—Ç–ø—Ä–∞–≤–ª—è—é –¥–µ–Ω—å–≥–∏', '–æ—Ç–ø—Ä–∞–≤–ª—è–µ—à—å –¥–µ–Ω—å–≥–∏', '–æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –¥–µ–Ω—å–≥–∏',
        '–æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–µ–Ω—å–≥–∏', '–æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç–µ –¥–µ–Ω—å–≥–∏', '–æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç –¥–µ–Ω—å–≥–∏', '–æ—Ç–ø—Ä–∞–≤–∏—Ç—å –¥–µ–Ω—å–≥–∏',
        '–æ—Ç–ø—Ä–∞–≤–∏–ª –¥–µ–Ω—å–≥–∏', '–æ—Ç–ø—Ä–∞–≤–∏–ª–∞ –¥–µ–Ω—å–≥–∏', '–æ—Ç–ø—Ä–∞–≤–∏–ª–∏ –¥–µ–Ω—å–≥–∏', '–æ—Ç–ø—Ä–∞–≤–ª—è—è –¥–µ–Ω—å–≥–∏',
        '–ø–µ—Ä–µ—á–∏—Å–ª—è—Ç—å –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ—á–∏—Å–ª—è—é –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ—á–∏—Å–ª—è–µ—à—å –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ—á–∏—Å–ª—è–µ—Ç –¥–µ–Ω—å–≥–∏',
        '–ø–µ—Ä–µ—á–∏—Å–ª—è–µ–º –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ—á–∏—Å–ª—è–µ—Ç–µ –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ—á–∏—Å–ª—è—é—Ç –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ—á–∏—Å–ª–∏—Ç—å –¥–µ–Ω—å–≥–∏',
        '–ø–µ—Ä–µ—á–∏—Å–ª–∏–ª –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ—á–∏—Å–ª–∏–ª–∞ –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ—á–∏—Å–ª–∏–ª–∏ –¥–µ–Ω—å–≥–∏', '–ø–µ—Ä–µ—á–∏—Å–ª—è—è –¥–µ–Ω—å–≥–∏',
        '–ø–µ—Ä–µ–≤–æ–¥ –º–µ–∂–¥—É –∫–∞—Ä—Ç–∞–º–∏', '–ø–µ—Ä–µ–≤–æ–¥—ã –º–µ–∂–¥—É –∫–∞—Ä—Ç–∞–º–∏', '–ø–µ—Ä–µ–≤–æ–¥—É –º–µ–∂–¥—É –∫–∞—Ä—Ç–∞–º–∏',
        '–º–µ–∂–∫–∞—Ä—Ç–æ–≤—ã–π –ø–µ—Ä–µ–≤–æ–¥', '–º–µ–∂–∫–∞—Ä—Ç–æ–≤–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞', '–º–µ–∂–∫–∞—Ä—Ç–æ–≤–æ–º—É –ø–µ—Ä–µ–≤–æ–¥—É',
        '–ø–µ—Ä–µ–≤–æ–¥ —Å –∫–∞—Ä—Ç—ã –Ω–∞ –∫–∞—Ä—Ç—É', '–ø–µ—Ä–µ–≤–æ–¥–∞ —Å –∫–∞—Ä—Ç—ã –Ω–∞ –∫–∞—Ä—Ç—É', '–ø–µ—Ä–µ–≤–æ–¥—É —Å –∫–∞—Ä—Ç—ã –Ω–∞ –∫–∞—Ä—Ç—É',
        '–±—ã—Å—Ç—Ä—ã–π –ø–µ—Ä–µ–≤–æ–¥', '–±—ã—Å—Ç—Ä–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞', '–±—ã—Å—Ç—Ä–æ–º—É –ø–µ—Ä–µ–≤–æ–¥—É', '–±—ã—Å—Ç—Ä—ã–º –ø–µ—Ä–µ–≤–æ–¥–æ–º',
        '–æ–Ω–ª–∞–π–Ω –ø–µ—Ä–µ–≤–æ–¥', '–æ–Ω–ª–∞–π–Ω –ø–µ—Ä–µ–≤–æ–¥–∞', '–æ–Ω–ª–∞–π–Ω –ø–µ—Ä–µ–≤–æ–¥—É', '–æ–Ω–ª–∞–π–Ω –ø–µ—Ä–µ–≤–æ–¥–æ–º',
        '—Å–∏—Å—Ç–µ–º–∞ –±—ã—Å—Ç—Ä—ã—Ö –ø–ª–∞—Ç–µ–∂–µ–π', '—Å–±–ø', '–ø–µ—Ä–µ–≤–æ–¥ –ø–æ –Ω–æ–º–µ—Ä—É —Ç–µ–ª–µ—Ñ–æ–Ω–∞'
    ],

    '–∑–∞—Ä–ø–ª–∞—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã': [
        '–∑–∞—Ä–ø–ª–∞—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞', '–∑–∞—Ä–ø–ª–∞—Ç–Ω–æ–π –∫–∞—Ä—Ç—ã', '–∑–∞—Ä–ø–ª–∞—Ç–Ω—É—é –∫–∞—Ä—Ç—É', '–∑–∞—Ä–ø–ª–∞—Ç–Ω–æ–π –∫–∞—Ä—Ç–æ–π',
        '–∑–∞—Ä–ø–ª–∞—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã', '–∑–∞—Ä–ø–ª–∞—Ç–Ω—ã—Ö –∫–∞—Ä—Ç', '–∑–∞—Ä–ø–ª–∞—Ç–Ω—ã–º –∫–∞—Ä—Ç–∞–º', '–∑–∞—Ä–ø–ª–∞—Ç–Ω—ã–º–∏ –∫–∞—Ä—Ç–∞–º–∏',
        '–∑–∞—Ä–ø–ª–∞—Ç–Ω—ã–π –ø—Ä–æ–µ–∫—Ç', '–∑–∞—Ä–ø–ª–∞—Ç–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞', '–∑–∞—Ä–ø–ª–∞—Ç–Ω–æ–º—É –ø—Ä–æ–µ–∫—Ç—É', '–∑–∞—Ä–ø–ª–∞—Ç–Ω—ã–º –ø—Ä–æ–µ–∫—Ç–æ–º',
        '–∫–∞—Ä—Ç–∞ –¥–ª—è –∑–∞—Ä–ø–ª–∞—Ç—ã', '–∫–∞—Ä—Ç—ã –¥–ª—è –∑–∞—Ä–ø–ª–∞—Ç—ã', '–∫–∞—Ä—Ç—É –¥–ª—è –∑–∞—Ä–ø–ª–∞—Ç—ã', '–∫–∞—Ä—Ç–æ–π –¥–ª—è –∑–∞—Ä–ø–ª–∞—Ç—ã',
        '–∑–∞—Ä–ø–ª–∞—Ç–Ω—ã–π —Å—á–µ—Ç', '–∑–∞—Ä–ø–ª–∞—Ç–Ω–æ–≥–æ —Å—á–µ—Ç–∞', '–∑–∞—Ä–ø–ª–∞—Ç–Ω–æ–º—É —Å—á–µ—Ç—É', '–∑–∞—Ä–ø–ª–∞—Ç–Ω—ã–º —Å—á–µ—Ç–æ–º',
        '–ø–æ–ª—É—á–∞—Ç—å –∑–∞—Ä–ø–ª–∞—Ç—É –Ω–∞ –∫–∞—Ä—Ç—É', '–ø–æ–ª—É—á–∞—é –∑–∞—Ä–ø–ª–∞—Ç—É –Ω–∞ –∫–∞—Ä—Ç—É', '–ø–æ–ª—É—á–∞–µ—à—å –∑–∞—Ä–ø–ª–∞—Ç—É –Ω–∞ –∫–∞—Ä—Ç—É',
        '–ø–æ–ª—É—á–∞–µ—Ç –∑–∞—Ä–ø–ª–∞—Ç—É –Ω–∞ –∫–∞—Ä—Ç—É', '–∑–∞—Ä–ø–ª–∞—Ç–Ω—ã–π –ø—Ä–æ–µ–∫—Ç', '–∫–∞—Ä—Ç–∞ –¥–ª—è –∑–∞—Ä–ø–ª–∞—Ç—ã',
        '–∑–∞—Ä–ø–ª–∞—Ç–∞ –Ω–∞ –∫–∞—Ä—Ç—É', '–Ω–∞—á–∏—Å–ª–µ–Ω–∏–µ –∑–∞—Ä–ø–ª–∞—Ç—ã', '–ø–æ–ª—É—á–µ–Ω–∏–µ –∑–∞—Ä–ø–ª–∞—Ç—ã', '–∑–∞—Ä–ø–ª–∞—Ç–Ω–∞—è –∫–∞—Ä—Ç–æ—á–∫–∞'
    ],

    '–ø—Ä–µ–º–∏–∞–ª—å–Ω—ã–µ –∫–∞—Ä—Ç—ã': [
        '–ø—Ä–µ–º–∏–∞–ª—å–Ω–∞—è –∫–∞—Ä—Ç–∞', '–ø—Ä–µ–º–∏–∞–ª—å–Ω–æ–π –∫–∞—Ä—Ç—ã', '–ø—Ä–µ–º–∏–∞–ª—å–Ω—É—é –∫–∞—Ä—Ç—É', '–ø—Ä–µ–º–∏–∞–ª—å–Ω–æ–π –∫–∞—Ä—Ç–æ–π',
        '–ø—Ä–µ–º–∏–∞–ª—å–Ω—ã–µ –∫–∞—Ä—Ç—ã', '–ø—Ä–µ–º–∏–∞–ª—å–Ω—ã—Ö –∫–∞—Ä—Ç', '–ø—Ä–µ–º–∏–∞–ª—å–Ω—ã–º –∫–∞—Ä—Ç–∞–º', '–ø—Ä–µ–º–∏–∞–ª—å–Ω—ã–º–∏ –∫–∞—Ä—Ç–∞–º–∏',
        '–∑–æ–ª–æ—Ç–∞—è –∫–∞—Ä—Ç–∞', '–∑–æ–ª–æ—Ç–æ–π –∫–∞—Ä—Ç—ã', '–∑–æ–ª–æ—Ç—É—é –∫–∞—Ä—Ç—É', '–∑–æ–ª–æ—Ç–æ–π –∫–∞—Ä—Ç–æ–π',
        '–ø–ª–∞—Ç–∏–Ω–æ–≤–∞—è –∫–∞—Ä—Ç–∞', '–ø–ª–∞—Ç–∏–Ω–æ–≤–æ–π –∫–∞—Ä—Ç—ã', '–ø–ª–∞—Ç–∏–Ω–æ–≤—É—é –∫–∞—Ä—Ç—É', '–ø–ª–∞—Ç–∏–Ω–æ–≤–æ–π –∫–∞—Ä—Ç–æ–π',
        '–ø—Ä–µ–º–∏—É–º –∫–∞—Ä—Ç–∞', '–ø—Ä–µ–º–∏—É–º –∫–∞—Ä—Ç—ã', '–ø—Ä–µ–º–∏—É–º –∫–∞—Ä—Ç—É', '–ø—Ä–µ–º–∏—É–º –∫–∞—Ä—Ç–æ–π',
        '–≤–∏–ø –∫–∞—Ä—Ç–∞', '–≤–∏–ø –∫–∞—Ä—Ç—ã', '–≤–∏–ø –∫–∞—Ä—Ç—É', '–≤–∏–ø –∫–∞—Ä—Ç–æ–π', '–ø—Ä–µ–º–∏–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å',
        '–æ—Ñ–æ—Ä–º–∏—Ç—å –ø—Ä–µ–º–∏–∞–ª—å–Ω—É—é –∫–∞—Ä—Ç—É', '–æ—Ñ–æ—Ä–º–∏–ª –ø—Ä–µ–º–∏–∞–ª—å–Ω—É—é –∫–∞—Ä—Ç—É', '–æ—Ñ–æ—Ä–º–∏–ª–∞ –ø—Ä–µ–º–∏–∞–ª—å–Ω—É—é –∫–∞—Ä—Ç—É',
        '–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–µ–º–∏–∞–ª—å–Ω–æ–π –∫–∞—Ä—Ç–æ–π', '–ø–æ–ª—å–∑—É—é—Å—å –ø—Ä–µ–º–∏–∞–ª—å–Ω–æ–π –∫–∞—Ä—Ç–æ–π', '–ø–æ–ª—å–∑—É–µ—à—å—Å—è –ø—Ä–µ–º–∏–∞–ª—å–Ω–æ–π –∫–∞—Ä—Ç–æ–π',
        '–ø—Ä–µ–º–∏–∞–ª—å–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ', '–≤–∏–ø –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ', '–ø—Ä–∏–≤–∏–ª–µ–≥–∏–∏ –∫–∞—Ä—Ç—ã', '–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Å–ª—É–≥–∏'
    ],

    '–∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ': [
        '–∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç', '–∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç–∞', '–∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç—É', '–∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç–æ–º', '–∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç–µ',
        '–∫—Ä–µ–¥–∏—Ç –Ω–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—å', '–∫—Ä–µ–¥–∏—Ç–∞ –Ω–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—å', '–∫—Ä–µ–¥–∏—Ç—É –Ω–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—å',
        '–∫—Ä–µ–¥–∏—Ç–æ–º –Ω–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—å', '–∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ', '–∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏—è',
        '–∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏—é', '–∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ–º',
        '–≤–∑—è—Ç—å –∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç', '–≤–∑—è–ª –∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç', '–≤–∑—è–ª–∞ –∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç', '–æ—Ñ–æ—Ä–º–∏—Ç—å –∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç',
        '–æ—Ñ–æ—Ä–º–∏–ª –∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç', '–æ—Ñ–æ—Ä–º–∏–ª–∞ –∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç', '–∫—Ä–µ–¥–∏—Ç –Ω–∞ –º–∞—à–∏–Ω—É', '–∫—Ä–µ–¥–∏—Ç –Ω–∞ –∞–≤—Ç–æ',
        '–ø–æ–∫—É–ø–∫–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—è –≤ –∫—Ä–µ–¥–∏—Ç', '–∞–≤—Ç–æ –≤ –∫—Ä–µ–¥–∏—Ç', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω–æ–µ –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ'
    ],

    '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –∫—Ä–µ–¥–∏—Ç–æ–≤': [
        '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è', '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—é', '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ–º',
        '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–∏', '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –∫—Ä–µ–¥–∏—Ç–∞', '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–∞',
        '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—é –∫—Ä–µ–¥–∏—Ç–∞', '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∫—Ä–µ–¥–∏—Ç–∞', '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–π–º–∞',
        '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–π–º–∞', '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—é –∑–∞–π–º–∞', '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∑–∞–π–º–∞',
        '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–µ–¥–∏—Ç', '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä—É—é –∫—Ä–µ–¥–∏—Ç', '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä—É–µ—à—å –∫—Ä–µ–¥–∏—Ç',
        '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä—É–µ—Ç –∫—Ä–µ–¥–∏—Ç', '–ø–µ—Ä–µ–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ', '–æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∫—Ä–µ–¥–∏—Ç—ã', '–æ–±—ä–µ–¥–∏–Ω—è—é –∫—Ä–µ–¥–∏—Ç—ã',
        '–æ–±—ä–µ–¥–∏–Ω—è–µ—à—å –∫—Ä–µ–¥–∏—Ç—ã', '–æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫—Ä–µ–¥–∏—Ç—ã', '—Å–Ω–∏–∑–∏—Ç—å –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—É—é —Å—Ç–∞–≤–∫—É',
        '—Å–Ω–∏–∂–∞—é –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—É—é —Å—Ç–∞–≤–∫—É', '—Å–Ω–∏–∂–∞–µ—à—å –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—É—é —Å—Ç–∞–≤–∫—É', '—Å–Ω–∏–∂–∞–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—É—é —Å—Ç–∞–≤–∫—É'
    ],

    '—Å—Ç—Ä–∞—Ö–æ–≤—ã–µ –∏ —Å–µ—Ä–≤–∏—Å–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã': [
        '—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ', '—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏—è', '—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏—é', '—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ–º', '—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–∏',
        '—Å—Ç—Ä–∞—Ö–æ–≤–∫–∞', '—Å—Ç—Ä–∞—Ö–æ–≤–∫–∏', '—Å—Ç—Ä–∞—Ö–æ–≤–∫—É', '—Å—Ç—Ä–∞—Ö–æ–≤–∫–æ–π', '—Å—Ç—Ä–∞—Ö–æ–≤–∫–µ',
        '—Å—Ç—Ä–∞—Ö–æ–≤–æ–π –ø–æ–ª–∏—Å', '—Å—Ç—Ä–∞—Ö–æ–≤–æ–≥–æ –ø–æ–ª–∏—Å–∞', '—Å—Ç—Ä–∞—Ö–æ–≤–æ–º—É –ø–æ–ª–∏—Å—É', '—Å—Ç—Ä–∞—Ö–æ–≤—ã–º –ø–æ–ª–∏—Å–æ–º',
        '—Å—Ç—Ä–∞—Ö–æ–≤—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã', '—Å—Ç—Ä–∞—Ö–æ–≤—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤', '—Å—Ç—Ä–∞—Ö–æ–≤—ã–º –ø—Ä–æ–¥—É–∫—Ç–∞–º',
        '—Å–µ—Ä–≤–∏—Å–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã', '—Å–µ—Ä–≤–∏—Å–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤', '—Å–µ—Ä–≤–∏—Å–Ω—ã–º –ø—Ä–æ–¥—É–∫—Ç–∞–º',
        '–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Å–ª—É–≥–∏', '–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ª—É–≥', '–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º —É—Å–ª—É–≥–∞–º',
        '–æ—Ñ–æ—Ä–º–∏—Ç—å —Å—Ç—Ä–∞—Ö–æ–≤–∫—É', '–æ—Ñ–æ—Ä–º–∏–ª —Å—Ç—Ä–∞—Ö–æ–≤–∫—É', '–æ—Ñ–æ—Ä–º–∏–ª–∞ —Å—Ç—Ä–∞—Ö–æ–≤–∫—É', '–∫—É–ø–∏—Ç—å —Å—Ç—Ä–∞—Ö–æ–≤–∫—É',
        '–∫—É–ø–∏–ª —Å—Ç—Ä–∞—Ö–æ–≤–∫—É', '–∫—É–ø–∏–ª–∞ —Å—Ç—Ä–∞—Ö–æ–≤–∫—É', '–∑–∞—Å—Ç—Ä–∞—Ö–æ–≤–∞—Ç—å', '–∑–∞—Å—Ç—Ä–∞—Ö–æ–≤–∞–ª', '–∑–∞—Å—Ç—Ä–∞—Ö–æ–≤–∞–ª–∞',
        '—Å—Ç—Ä–∞—Ö–æ–≤–∞—è –∑–∞—â–∏—Ç–∞', '—Å—Ç—Ä–∞—Ö–æ–≤–æ–π —Å–ª—É—á–∞–π', '—Å—Ç—Ä–∞—Ö–æ–≤—ã–µ –≤—ã–ø–ª–∞—Ç—ã', '—Å–µ—Ä–≤–∏—Å–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ',
        '–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —É—Å–ª—É–≥–∞', '–ø–æ–¥–∫–ª—é—á–∏—Ç—å —É—Å–ª—É–≥—É', '–ø–æ–¥–∫–ª—é—á–∏–ª —É—Å–ª—É–≥—É', '–ø–æ–¥–∫–ª—é—á–∏–ª–∞ —É—Å–ª—É–≥—É'
    ]
}

SERVICE_CONTEXT_KEYWORDS = [
    # –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ - –ø–æ–ª–Ω—ã–π –æ—Ö–≤–∞—Ç
    '–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ', '–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è', '–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—é', '–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ–º', '–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–∏',
    '–æ–±—Å–ª—É–∂–∏–≤–∞–Ω—å–µ', '–æ–±—Å–ª—É–∂–∏–≤–∞–Ω—å—è', '–æ–±—Å–ª—É–∂–∏–≤–∞–Ω—å—é', '–æ–±—Å–ª—É–∂–∏–≤–∞–Ω—å–µ–º', '–æ–±—Å–ª—É–∂–∏–≤–∞–Ω—å–∏',
    '–æ–±—Å–ª—É–∂–∏–≤–∞—é—â–∏–π', '–æ–±—Å–ª—É–∂–∏–≤–∞—é—â–µ–≥–æ', '–æ–±—Å–ª—É–∂–∏–≤–∞—é—â–µ–º—É', '–æ–±—Å–ª—É–∂–∏–≤–∞—é—â–∏–º', '–æ–±—Å–ª—É–∂–∏–≤–∞—é—â–µ–º',
    '–æ–±—Å–ª—É–∂–∏–≤–∞—é—â–∞—è', '–æ–±—Å–ª—É–∂–∏–≤–∞—é—â–µ–π', '–æ–±—Å–ª—É–∂–∏–≤–∞—é—â—É—é', '–æ–±—Å–ª—É–∂–∏–≤–∞—é—â–µ—é', '–æ–±—Å–ª—É–∂–∏–≤–∞—é—â–µ–µ',
    '–æ–±—Å–ª—É–∂–∏–≤–∞—é—â–∏–µ', '–æ–±—Å–ª—É–∂–∏–≤–∞—é—â–∏—Ö', '–æ–±—Å–ª—É–∂–∏–≤–∞—é—â–∏–º', '–æ–±—Å–ª—É–∂–∏–≤–∞—é—â–∏–º–∏',
    '–æ–±—Å–ª—É–∂–∏–≤–∞—Ç—å', '–æ–±—Å–ª—É–∂–∏–≤–∞—é', '–æ–±—Å–ª—É–∂–∏–≤–∞–µ—à—å', '–æ–±—Å–ª—É–∂–∏–≤–∞–µ—Ç', '–æ–±—Å–ª—É–∂–∏–≤–∞–µ–º',
    '–æ–±—Å–ª—É–∂–∏–≤–∞–µ—Ç–µ', '–æ–±—Å–ª—É–∂–∏–≤–∞—é—Ç', '–æ–±—Å–ª—É–∂–∏–≤–∞–ª', '–æ–±—Å–ª—É–∂–∏–≤–∞–ª–∞', '–æ–±—Å–ª—É–∂–∏–≤–∞–ª–æ', '–æ–±—Å–ª—É–∂–∏–≤–∞–ª–∏',
    '–æ–±—Å–ª—É–∂–∏–≤–∞–π', '–æ–±—Å–ª—É–∂–∏–≤–∞–π—Ç–µ', '–æ–±—Å–ª—É–∂–∏–≤–∞—è', '–æ–±—Å–ª—É–∂–µ–Ω–Ω—ã–π', '–æ–±—Å–ª—É–∂–µ–Ω–Ω–æ–≥–æ', '–æ–±—Å–ª—É–∂–µ–Ω–Ω–æ–º—É',
    '–æ–±—Å–ª—É–∂–µ–Ω–Ω—ã–º', '–æ–±—Å–ª—É–∂–µ–Ω–Ω–æ–º', '–æ–±—Å–ª—É–∂–µ–Ω–Ω–∞—è', '–æ–±—Å–ª—É–∂–µ–Ω–Ω–æ–π', '–æ–±—Å–ª—É–∂–µ–Ω–Ω—É—é', '–æ–±—Å–ª—É–∂–µ–Ω–Ω–æ—é',
    '–æ–±—Å–ª—É–∂–µ–Ω–Ω–æ–µ', '–æ–±—Å–ª—É–∂–µ–Ω–Ω—ã–µ', '–æ–±—Å–ª—É–∂–µ–Ω–Ω—ã—Ö', '–æ–±—Å–ª—É–∂–µ–Ω–Ω—ã–º', '–æ–±—Å–ª—É–∂–µ–Ω–Ω—ã–º–∏',
    '–æ–±—Å–ª—É–∂–∏', '–æ–±—Å–ª—É–∂–∏–ª', '–æ–±—Å–ª—É–∂–∏–ª–∞', '–æ–±—Å–ª—É–∂–∏–ª–æ', '–æ–±—Å–ª—É–∂–∏–ª–∏', '–æ–±—Å–ª—É–∂—É', '–æ–±—Å–ª—É–∂–∏—à—å',

    # —Å–µ—Ä–≤–∏—Å - –ø–æ–ª–Ω—ã–π –æ—Ö–≤–∞—Ç
    '—Å–µ—Ä–≤–∏—Å', '—Å–µ—Ä–≤–∏—Å–∞', '—Å–µ—Ä–≤–∏—Å—É', '—Å–µ—Ä–≤–∏—Å–æ–º', '—Å–µ—Ä–≤–∏—Å–µ', '—Å–µ—Ä–≤–∏—Å—ã', '—Å–µ—Ä–≤–∏—Å–æ–≤',
    '—Å–µ—Ä–≤–∏—Å–∞–º', '—Å–µ—Ä–≤–∏—Å–∞–º–∏', '—Å–µ—Ä–≤–∏—Å–∞—Ö', '—Å–µ—Ä–≤–∏—Å–Ω—ã–π', '—Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ', '—Å–µ—Ä–≤–∏—Å–Ω–æ–º—É',
    '—Å–µ—Ä–≤–∏—Å–Ω—ã–º', '—Å–µ—Ä–≤–∏—Å–Ω–æ–º', '—Å–µ—Ä–≤–∏—Å–Ω–∞—è', '—Å–µ—Ä–≤–∏—Å–Ω–æ–π', '—Å–µ—Ä–≤–∏—Å–Ω—É—é', '—Å–µ—Ä–≤–∏—Å–Ω–æ—é',
    '—Å–µ—Ä–≤–∏—Å–Ω–æ–µ', '—Å–µ—Ä–≤–∏—Å–Ω—ã–µ', '—Å–µ—Ä–≤–∏—Å–Ω—ã—Ö', '—Å–µ—Ä–≤–∏—Å–Ω—ã–º', '—Å–µ—Ä–≤–∏—Å–Ω—ã–º–∏',

    # —Å–æ—Ç—Ä—É–¥–Ω–∏–∫ - –ø–æ–ª–Ω—ã–π –æ—Ö–≤–∞—Ç
    '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫—É', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–º', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–µ',
    '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∏', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞—Ö',
    '—Å–æ—Ç—Ä—É–¥–Ω–∏—Ü–∞', '—Å–æ—Ç—Ä—É–¥–Ω–∏—Ü—ã', '—Å–æ—Ç—Ä—É–¥–Ω–∏—Ü–µ', '—Å–æ—Ç—Ä—É–¥–Ω–∏—Ü—É', '—Å–æ—Ç—Ä—É–¥–Ω–∏—Ü–µ–π',
    '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞—Ç—å', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞—é', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ—à—å', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ—Ç', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ–º',
    '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ—Ç–µ', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞—é—Ç', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞–ª', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞–ª–∞', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞–ª–æ',
    '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞–ª–∏', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞—è', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞—é—â–∏–π', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞—é—â–µ–≥–æ', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞—é—â–µ–º—É',

    # –º–µ–Ω–µ–¥–∂–µ—Ä - –ø–æ–ª–Ω—ã–π –æ—Ö–≤–∞—Ç
    '–º–µ–Ω–µ–¥–∂–µ—Ä', '–º–µ–Ω–µ–¥–∂–µ—Ä–∞', '–º–µ–Ω–µ–¥–∂–µ—Ä—É', '–º–µ–Ω–µ–¥–∂–µ—Ä–æ–º', '–º–µ–Ω–µ–¥–∂–µ—Ä–µ',
    '–º–µ–Ω–µ–¥–∂–µ—Ä—ã', '–º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤', '–º–µ–Ω–µ–¥–∂–µ—Ä–∞–º', '–º–µ–Ω–µ–¥–∂–µ—Ä–∞–º–∏', '–º–µ–Ω–µ–¥–∂–µ—Ä–∞—Ö',
    '–º–µ–Ω–µ–¥–∂–µ—Ä—Å–∫–∏–π', '–º–µ–Ω–µ–¥–∂–µ—Ä—Å–∫–æ–≥–æ', '–º–µ–Ω–µ–¥–∂–µ—Ä—Å–∫–æ–º—É', '–º–µ–Ω–µ–¥–∂–µ—Ä—Å–∫–∏–º', '–º–µ–Ω–µ–¥–∂–µ—Ä—Å–∫–æ–º',
    '–º–µ–Ω–µ–¥–∂–µ—Ä—Å–∫–∞—è', '–º–µ–Ω–µ–¥–∂–µ—Ä—Å–∫–æ–π', '–º–µ–Ω–µ–¥–∂–µ—Ä—Å–∫—É—é', '–º–µ–Ω–µ–¥–∂–µ—Ä—Å–∫–æ—é', '–º–µ–Ω–µ–¥–∂–µ—Ä—Å–∫–æ–µ',
    '–º–µ–Ω–µ–¥–∂–µ—Ä—Å–∫–∏–µ', '–º–µ–Ω–µ–¥–∂–µ—Ä—Å–∫–∏—Ö', '–º–µ–Ω–µ–¥–∂–µ—Ä—Å–∫–∏–º', '–º–µ–Ω–µ–¥–∂–µ—Ä—Å–∫–∏–º–∏',

    # –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç - –ø–æ–ª–Ω—ã–π –æ—Ö–≤–∞—Ç
    '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç', '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–∞', '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç—É', '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–æ–º', '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–µ',
    '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç—ã', '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–æ–≤', '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–∞–º', '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–∞–º–∏', '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–∞—Ö',
    '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–µ–π', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–µ—é',
    '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è–º', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è–º–∏', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è—Ö',
    '–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞—Ç—å', '–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É—é', '–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–µ—à—å', '–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–µ—Ç', '–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–µ–º',
    '–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–µ—Ç–µ', '–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É—é—Ç', '–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–ª', '–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–ª–∞', '–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–ª–æ',
    '–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–ª–∏', '–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–π', '–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–π—Ç–µ', '–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É—è',
    '–ø—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞—Ç—å', '–ø—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–ª', '–ø—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–ª–∞', '–ø—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–ª–æ',

    # –≤–µ–∂–ª–∏–≤—ã–π - –ø–æ–ª–Ω—ã–π –æ—Ö–≤–∞—Ç —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
    '–≤–µ–∂–ª–∏–≤—ã–π', '–≤–µ–∂–ª–∏–≤–æ–≥–æ', '–≤–µ–∂–ª–∏–≤–æ–º—É', '–≤–µ–∂–ª–∏–≤—ã–º', '–≤–µ–∂–ª–∏–≤–æ–º',
    '–≤–µ–∂–ª–∏–≤–∞—è', '–≤–µ–∂–ª–∏–≤–æ–π', '–≤–µ–∂–ª–∏–≤—É—é', '–≤–µ–∂–ª–∏–≤–æ—é', '–≤–µ–∂–ª–∏–≤–æ–µ',
    '–≤–µ–∂–ª–∏–≤—ã–µ', '–≤–µ–∂–ª–∏–≤—ã—Ö', '–≤–µ–∂–ª–∏–≤—ã–º', '–≤–µ–∂–ª–∏–≤—ã–º–∏', '–≤–µ–∂–ª–∏–≤–æ',
    '–≤–µ–∂–ª–∏–≤–æ—Å—Ç—å', '–≤–µ–∂–ª–∏–≤–æ—Å—Ç–∏', '–≤–µ–∂–ª–∏–≤–æ—Å—Ç—å—é', '–≤–µ–∂–ª–∏–≤–æ—Å—Ç—è–º', '–≤–µ–∂–ª–∏–≤–æ—Å—Ç—è–º–∏',
    '—É—á—Ç–∏–≤—ã–π', '—É—á—Ç–∏–≤–æ–≥–æ', '—É—á—Ç–∏–≤–æ–º—É', '—É—á—Ç–∏–≤—ã–º', '—É—á—Ç–∏–≤–æ–º', '—É—á—Ç–∏–≤–∞—è', '—É—á—Ç–∏–≤–æ–π',
    '—É—á—Ç–∏–≤—É—é', '—É—á—Ç–∏–≤–æ—é', '—É—á—Ç–∏–≤–æ–µ', '—É—á—Ç–∏–≤—ã–µ', '—É—á—Ç–∏–≤—ã—Ö', '—É—á—Ç–∏–≤—ã–º', '—É—á—Ç–∏–≤—ã–º–∏',
    '–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π', '–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ', '–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–º—É', '–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º', '–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–º',
    '–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è', '–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π', '–∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é', '–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—é', '–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ',
    '–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ', '–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö', '–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º', '–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏', '–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ',
    '–æ–±—Ö–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π', '–æ–±—Ö–æ–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ', '–æ–±—Ö–æ–¥–∏—Ç–µ–ª—å–Ω–æ–º—É', '–æ–±—Ö–æ–¥–∏—Ç–µ–ª—å–Ω—ã–º',

    # –≥—Ä—É–±—ã–π - –ø–æ–ª–Ω—ã–π –æ—Ö–≤–∞—Ç —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
    '–≥—Ä—É–±—ã–π', '–≥—Ä—É–±–æ–≥–æ', '–≥—Ä—É–±–æ–º—É', '–≥—Ä—É–±—ã–º', '–≥—Ä—É–±–æ–º',
    '–≥—Ä—É–±–∞—è', '–≥—Ä—É–±–æ–π', '–≥—Ä—É–±—É—é', '–≥—Ä—É–±–æ—é', '–≥—Ä—É–±–æ–µ',
    '–≥—Ä—É–±—ã–µ', '–≥—Ä—É–±—ã—Ö', '–≥—Ä—É–±—ã–º', '–≥—Ä—É–±—ã–º–∏', '–≥—Ä—É–±–æ',
    '–≥—Ä—É–±–æ—Å—Ç—å', '–≥—Ä—É–±–æ—Å—Ç–∏', '–≥—Ä—É–±–æ—Å—Ç—å—é', '–≥—Ä—É–±–æ—Å—Ç—è–º', '–≥—Ä—É–±–æ—Å—Ç—è–º–∏',
    '–Ω–µ–≤–µ–∂–ª–∏–≤—ã–π', '–Ω–µ–≤–µ–∂–ª–∏–≤–æ–≥–æ', '–Ω–µ–≤–µ–∂–ª–∏–≤–æ–º—É', '–Ω–µ–≤–µ–∂–ª–∏–≤—ã–º', '–Ω–µ–≤–µ–∂–ª–∏–≤–æ–º',
    '–Ω–µ–≤–µ–∂–ª–∏–≤–∞—è', '–Ω–µ–≤–µ–∂–ª–∏–≤–æ–π', '–Ω–µ–≤–µ–∂–ª–∏–≤—É—é', '–Ω–µ–≤–µ–∂–ª–∏–≤–æ—é', '–Ω–µ–≤–µ–∂–ª–∏–≤–æ–µ',
    '–Ω–µ–≤–µ–∂–ª–∏–≤—ã–µ', '–Ω–µ–≤–µ–∂–ª–∏–≤—ã—Ö', '–Ω–µ–≤–µ–∂–ª–∏–≤—ã–º', '–Ω–µ–≤–µ–∂–ª–∏–≤—ã–º–∏',
    '—Ö–∞–º—Å–∫–∏–π', '—Ö–∞–º—Å–∫–æ–≥–æ', '—Ö–∞–º—Å–∫–æ–º—É', '—Ö–∞–º—Å–∫–∏–º', '—Ö–∞–º—Å–∫–æ–º',
    '—Ö–∞–º—Å–∫–∞—è', '—Ö–∞–º—Å–∫–æ–π', '—Ö–∞–º—Å–∫—É—é', '—Ö–∞–º—Å–∫–æ—é', '—Ö–∞–º—Å–∫–æ–µ',
    '—Ö–∞–º—Å–∫–∏–µ', '—Ö–∞–º—Å–∫–∏—Ö', '—Ö–∞–º—Å–∫–∏–º', '—Ö–∞–º—Å–∫–∏–º–∏', '—Ö–∞–º—Å–∫–∏',
    '—Ö–∞–º—Å—Ç–≤–æ', '—Ö–∞–º—Å—Ç–≤–∞', '—Ö–∞–º—Å—Ç–≤—É', '—Ö–∞–º—Å—Ç–≤–æ–º', '—Ö–∞–º—Å—Ç–≤–µ',
    '–Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π', '–Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ', '–Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–º—É', '–Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º',

    # –ø–æ–º–æ—â—å - –ø–æ–ª–Ω—ã–π –æ—Ö–≤–∞—Ç —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
    '–ø–æ–º–æ—â—å', '–ø–æ–º–æ—â–∏', '–ø–æ–º–æ—â—å—é', '–ø–æ–º–æ—â—å—é', '–ø–æ–º–æ—â–∞–º', '–ø–æ–º–æ—â–∞–º–∏',
    '–ø–æ–º–æ–≥–∞—Ç—å', '–ø–æ–º–æ–≥–∞—é', '–ø–æ–º–æ–≥–∞–µ—à—å', '–ø–æ–º–æ–≥–∞–µ—Ç', '–ø–æ–º–æ–≥–∞–µ–º', '–ø–æ–º–æ–≥–∞–µ—Ç–µ', '–ø–æ–º–æ–≥–∞—é—Ç',
    '–ø–æ–º–æ–≥', '–ø–æ–º–æ–≥–ª–∞', '–ø–æ–º–æ–≥–ª–æ', '–ø–æ–º–æ–≥–ª–∏', '–ø–æ–º–æ–≥–∏', '–ø–æ–º–æ–≥–∏—Ç–µ', '–ø–æ–º–æ–≥–∞—è',
    '–ø–æ–º–æ—â–Ω–∏–∫', '–ø–æ–º–æ—â–Ω–∏–∫–∞', '–ø–æ–º–æ—â–Ω–∏–∫—É', '–ø–æ–º–æ—â–Ω–∏–∫–æ–º', '–ø–æ–º–æ—â–Ω–∏–∫–µ',
    '–ø–æ–º–æ—â–Ω–∏—Ü–∞', '–ø–æ–º–æ—â–Ω–∏—Ü—ã', '–ø–æ–º–æ—â–Ω–∏—Ü–µ', '–ø–æ–º–æ—â–Ω–∏—Ü—É', '–ø–æ–º–æ—â–Ω–∏—Ü–µ–π',
    '–ø–æ–¥–º–æ–≥–∞', '–ø–æ–¥–º–æ–≥–∏', '–ø–æ–¥–º–æ–≥–µ', '–ø–æ–¥–º–æ–≥—É', '–ø–æ–¥–º–æ–≥–æ–π', '–ø–æ–¥–º–æ–≥–æ—é',
    '—Å–æ–¥–µ–π—Å—Ç–≤–∏–µ', '—Å–æ–¥–µ–π—Å—Ç–≤–∏—è', '—Å–æ–¥–µ–π—Å—Ç–≤–∏—é', '—Å–æ–¥–µ–π—Å—Ç–≤–∏–µ–º', '—Å–æ–¥–µ–π—Å—Ç–≤–∏–∏',
    '–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø–æ–º–æ—â—å', '–æ–∫–∞–∑—ã–≤–∞—é –ø–æ–º–æ—â—å', '–æ–∫–∞–∑—ã–≤–∞–µ—à—å –ø–æ–º–æ—â—å', '–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ–º–æ—â—å',
    '–æ–∫–∞–∑–∞—Ç—å –ø–æ–º–æ—â—å', '–æ–∫–∞–∑–∞–ª –ø–æ–º–æ—â—å', '–æ–∫–∞–∑–∞–ª–∞ –ø–æ–º–æ—â—å', '–æ–∫–∞–∑–∞–ª–æ –ø–æ–º–æ—â—å',

    # –ø–æ–¥–¥–µ—Ä–∂–∫–∞ - –ø–æ–ª–Ω—ã–π –æ—Ö–≤–∞—Ç —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
    '–ø–æ–¥–¥–µ—Ä–∂–∫–∞', '–ø–æ–¥–¥–µ—Ä–∂–∫–∏', '–ø–æ–¥–¥–µ—Ä–∂–∫—É', '–ø–æ–¥–¥–µ—Ä–∂–∫–æ–π', '–ø–æ–¥–¥–µ—Ä–∂–∫–æ—é', '–ø–æ–¥–¥–µ—Ä–∂–∫–µ',
    '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—à—å', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º',
    '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç–µ', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–ª', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–ª–∞', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–ª–æ',
    '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–ª–∏', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π—Ç–µ', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—è',
    '–ø–æ–¥–¥–µ—Ä–∂–∞—Ç—å', '–ø–æ–¥–¥–µ—Ä–∂–∞–ª', '–ø–æ–¥–¥–µ—Ä–∂–∞–ª–∞', '–ø–æ–¥–¥–µ—Ä–∂–∞–ª–æ', '–ø–æ–¥–¥–µ—Ä–∂–∞–ª–∏',
    '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–µ–≥–æ', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–µ–º—É', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–º',

    # –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π - –ø–æ–ª–Ω—ã–π –æ—Ö–≤–∞—Ç —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
    '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π', '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ–≥–æ', '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ–º—É', '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–º', '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ–º',
    '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–∞—è', '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ–π', '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—É—é', '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ—é', '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ–µ',
    '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–µ', '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã—Ö', '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–º', '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–º–∏', '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ',
    '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å', '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏', '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é', '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º',
    '–∑–∞–±–æ—Ç–ª–∏–≤—ã–π', '–∑–∞–±–æ—Ç–ª–∏–≤–æ–≥–æ', '–∑–∞–±–æ—Ç–ª–∏–≤–æ–º—É', '–∑–∞–±–æ—Ç–ª–∏–≤—ã–º', '–∑–∞–±–æ—Ç–ª–∏–≤–æ–º',
    '–∑–∞–±–æ—Ç–ª–∏–≤–∞—è', '–∑–∞–±–æ—Ç–ª–∏–≤–æ–π', '–∑–∞–±–æ—Ç–ª–∏–≤—É—é', '–∑–∞–±–æ—Ç–ª–∏–≤–æ—é', '–∑–∞–±–æ—Ç–ª–∏–≤–æ–µ',
    '—á—É—Ç–∫–∏–π', '—á—É—Ç–∫–æ–≥–æ', '—á—É—Ç–∫–æ–º—É', '—á—É—Ç–∫–∏–º', '—á—É—Ç–∫–æ–º', '—á—É—Ç–∫–∞—è', '—á—É—Ç–∫–æ–π',
    '—á—É—Ç–∫—É—é', '—á—É—Ç–∫–æ—é', '—á—É—Ç–∫–æ–µ', '—á—É—Ç–∫–∏–µ', '—á—É—Ç–∫–∏—Ö', '—á—É—Ç–∫–∏–º', '—á—É—Ç–∫–∏–º–∏',

    # –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º - –ø–æ–ª–Ω—ã–π –æ—Ö–≤–∞—Ç —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
    '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º–∞', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º—É', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º–æ–º',
    '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º–µ', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∞', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—É', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–æ–º',
    '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–µ', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—ã', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–æ–≤', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∞–º', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∞–º–∏',
    '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–º—É', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º',
    '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ—é',
    '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º',
    '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ', '–∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è', '–∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏',
    '–∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—é', '–∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–µ–π', '–∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–µ—é', '–∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π',
    '–∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ', '–∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É', '–∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º',
    '–∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω—ã–π', '–∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ–≥–æ', '–∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ–º—É', '–∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω—ã–º',

    # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è
    '—Ä–∞–±–æ—Ç–∞', '—Ä–∞–±–æ—Ç—ã', '—Ä–∞–±–æ—Ç–µ', '—Ä–∞–±–æ—Ç—É', '—Ä–∞–±–æ—Ç–æ–π', '—Ä–∞–±–æ—Ç–æ—é', '—Ä–∞–±–æ—Ç',
    '—Ä–∞–±–æ—Ç–∞–º', '—Ä–∞–±–æ—Ç–∞–º–∏', '—Ä–∞–±–æ—Ç–∞—Ö', '—Ä–∞–±–æ—Ç–∞—Ç—å', '—Ä–∞–±–æ—Ç–∞—é', '—Ä–∞–±–æ—Ç–∞–µ—à—å',
    '—Ä–∞–±–æ—Ç–∞–µ—Ç', '—Ä–∞–±–æ—Ç–∞–µ–º', '—Ä–∞–±–æ—Ç–∞–µ—Ç–µ', '—Ä–∞–±–æ—Ç–∞—é—Ç', '—Ä–∞–±–æ—Ç–∞–ª', '—Ä–∞–±–æ—Ç–∞–ª–∞',

    '–∫–∞—á–µ—Å—Ç–≤–æ', '–∫–∞—á–µ—Å—Ç–≤–∞', '–∫–∞—á–µ—Å—Ç–≤—É', '–∫–∞—á–µ—Å—Ç–≤–æ–º', '–∫–∞—á–µ—Å—Ç–≤–µ', '–∫–∞—á–µ—Å—Ç–≤',
    '–∫–∞—á–µ—Å—Ç–≤–∞–º', '–∫–∞—á–µ—Å—Ç–≤–∞–º–∏', '–∫–∞—á–µ—Å—Ç–≤–∞—Ö', '–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π', '–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ',

    '–æ—Ç–Ω–æ—à–µ–Ω–∏–µ', '–æ—Ç–Ω–æ—à–µ–Ω–∏—è', '–æ—Ç–Ω–æ—à–µ–Ω–∏—é', '–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º', '–æ—Ç–Ω–æ—à–µ–Ω–∏–∏', '–æ—Ç–Ω–æ—à–µ–Ω–∏–π',
    '–æ—Ç–Ω–æ—à–µ–Ω–∏—è–º', '–æ—Ç–Ω–æ—à–µ–Ω–∏—è–º–∏', '–æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö', '–æ—Ç–Ω–æ—Å–∏—Ç—å—Å—è', '–æ—Ç–Ω–æ—à—É—Å—å',
    '–æ—Ç–Ω–æ—Å–∏—à—å—Å—è', '–æ—Ç–Ω–æ—Å–∏—Ç—Å—è', '–æ—Ç–Ω–æ—Å–∏–º—Å—è', '–æ—Ç–Ω–æ—Å–∏—Ç–µ—Å—å', '–æ—Ç–Ω–æ—Å—è—Ç—Å—è',

    '–æ–±—â–µ–Ω–∏–µ', '–æ–±—â–µ–Ω–∏—è', '–æ–±—â–µ–Ω–∏—é', '–æ–±—â–µ–Ω–∏–µ–º', '–æ–±—â–µ–Ω–∏–∏', '–æ–±—â–µ–Ω–∏–π',
    '–æ–±—â–µ–Ω–∏—è–º', '–æ–±—â–µ–Ω–∏—è–º–∏', '–æ–±—â–µ–Ω–∏—è—Ö', '–æ–±—â–∞—Ç—å—Å—è', '–æ–±—â–∞—é—Å—å', '–æ–±—â–∞–µ—à—å—Å—è',

    '—Ä–µ—à–µ–Ω–∏–µ', '—Ä–µ—à–µ–Ω–∏—è', '—Ä–µ—à–µ–Ω–∏—é', '—Ä–µ—à–µ–Ω–∏–µ–º', '—Ä–µ—à–µ–Ω–∏–∏', '—Ä–µ—à–µ–Ω–∏–π',
    '—Ä–µ—à–µ–Ω–∏—è–º', '—Ä–µ—à–µ–Ω–∏—è–º–∏', '—Ä–µ—à–µ–Ω–∏—è—Ö', '—Ä–µ—à–∞—Ç—å', '—Ä–µ—à–∞—é', '—Ä–µ—à–∞–µ—à—å',

    '–æ–±—Ä–∞—â–µ–Ω–∏–µ', '–æ–±—Ä–∞—â–µ–Ω–∏—è', '–æ–±—Ä–∞—â–µ–Ω–∏—é', '–æ–±—Ä–∞—â–µ–Ω–∏–µ–º', '–æ–±—Ä–∞—â–µ–Ω–∏–∏', '–æ–±—Ä–∞—â–µ–Ω–∏–π',
    '–æ–±—Ä–∞—â–µ–Ω–∏—è–º', '–æ–±—Ä–∞—â–µ–Ω–∏—è–º–∏', '–æ–±—Ä–∞—â–µ–Ω–∏—è—Ö', '–æ–±—Ä–∞—â–∞—Ç—å—Å—è', '–æ–±—Ä–∞—â–∞—é—Å—å',

    '–ø—Ä–∏–µ–º', '–ø—Ä–∏–µ–º–∞', '–ø—Ä–∏–µ–º—É', '–ø—Ä–∏–µ–º–æ–º', '–ø—Ä–∏–µ–º–µ', '–ø—Ä–∏–µ–º—ã', '–ø—Ä–∏–µ–º–æ–≤',
    '–ø—Ä–∏–µ–º–∞–º', '–ø—Ä–∏–µ–º–∞–º–∏', '–ø—Ä–∏–µ–º–∞—Ö', '–ø—Ä–∏–Ω–∏–º–∞—Ç—å', '–ø—Ä–∏–Ω–∏–º–∞—é', '–ø—Ä–∏–Ω–∏–º–∞–µ—à—å',

    '–æ—Ç–¥–µ–ª–µ–Ω–∏–µ', '–æ—Ç–¥–µ–ª–µ–Ω–∏—è', '–æ—Ç–¥–µ–ª–µ–Ω–∏—é', '–æ—Ç–¥–µ–ª–µ–Ω–∏–µ–º', '–æ—Ç–¥–µ–ª–µ–Ω–∏–∏', '–æ—Ç–¥–µ–ª–µ–Ω–∏–π',
    '–æ—Ç–¥–µ–ª–µ–Ω–∏—è–º', '–æ—Ç–¥–µ–ª–µ–Ω–∏—è–º–∏', '–æ—Ç–¥–µ–ª–µ–Ω–∏—è—Ö', '–æ—Ñ–∏—Å', '–æ—Ñ–∏—Å–∞', '–æ—Ñ–∏—Å—É',

    '—Ñ–∏–ª–∏–∞–ª', '—Ñ–∏–ª–∏–∞–ª–∞', '—Ñ–∏–ª–∏–∞–ª—É', '—Ñ–∏–ª–∏–∞–ª–æ–º', '—Ñ–∏–ª–∏–∞–ª–µ', '—Ñ–∏–ª–∏–∞–ª—ã', '—Ñ–∏–ª–∏–∞–ª–æ–≤',
    '—Ñ–∏–ª–∏–∞–ª–∞–º', '—Ñ–∏–ª–∏–∞–ª–∞–º–∏', '—Ñ–∏–ª–∏–∞–ª–∞—Ö', '–±–∞–Ω–∫', '–±–∞–Ω–∫–∞', '–±–∞–Ω–∫—É', '–±–∞–Ω–∫–æ–º',

    '–∫–ª–∏–µ–Ω—Ç', '–∫–ª–∏–µ–Ω—Ç–∞', '–∫–ª–∏–µ–Ω—Ç—É', '–∫–ª–∏–µ–Ω—Ç–æ–º', '–∫–ª–∏–µ–Ω—Ç–µ', '–∫–ª–∏–µ–Ω—Ç—ã', '–∫–ª–∏–µ–Ω—Ç–æ–≤',
    '–∫–ª–∏–µ–Ω—Ç–∞–º', '–∫–ª–∏–µ–Ω—Ç–∞–º–∏', '–∫–ª–∏–µ–Ω—Ç–∞—Ö', '–∫–ª–∏–µ–Ω—Ç—Å–∫–∏–π', '–∫–ª–∏–µ–Ω—Ç—Å–∫–æ–≥–æ',

    '–ø—Ä–æ–±–ª–µ–º–∞', '–ø—Ä–æ–±–ª–µ–º—ã', '–ø—Ä–æ–±–ª–µ–º–µ', '–ø—Ä–æ–±–ª–µ–º—É', '–ø—Ä–æ–±–ª–µ–º–æ–π', '–ø—Ä–æ–±–ª–µ–º–æ—é',
    '–ø—Ä–æ–±–ª–µ–º', '–ø—Ä–æ–±–ª–µ–º–∞–º', '–ø—Ä–æ–±–ª–µ–º–∞–º–∏', '–ø—Ä–æ–±–ª–µ–º–∞—Ö', '—Ä–µ—à–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—É',

    '–≤–æ–ø—Ä–æ—Å', '–≤–æ–ø—Ä–æ—Å–∞', '–≤–æ–ø—Ä–æ—Å—É', '–≤–æ–ø—Ä–æ—Å–æ–º', '–≤–æ–ø—Ä–æ—Å–µ', '–≤–æ–ø—Ä–æ—Å—ã', '–≤–æ–ø—Ä–æ—Å–æ–≤',
    '–≤–æ–ø—Ä–æ—Å–∞–º', '–≤–æ–ø—Ä–æ—Å–∞–º–∏', '–≤–æ–ø—Ä–æ—Å–∞—Ö', '–∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å', '–∑–∞–¥–∞—é –≤–æ–ø—Ä–æ—Å',

    '–∂–∞–ª–æ–±–∞', '–∂–∞–ª–æ–±—ã', '–∂–∞–ª–æ–±–µ', '–∂–∞–ª–æ–±—É', '–∂–∞–ª–æ–±–æ–π', '–∂–∞–ª–æ–±–æ—é', '–∂–∞–ª–æ–±',
    '–∂–∞–ª–æ–±–∞–º', '–∂–∞–ª–æ–±–∞–º–∏', '–∂–∞–ª–æ–±–∞—Ö', '–∂–∞–ª–æ–≤–∞—Ç—å—Å—è', '–∂–∞–ª—É—é—Å—å', '–∂–∞–ª—É–µ—à—å—Å—è',

    '–ø—Ä–µ—Ç–µ–Ω–∑–∏—è', '–ø—Ä–µ—Ç–µ–Ω–∑–∏–∏', '–ø—Ä–µ—Ç–µ–Ω–∑–∏—é', '–ø—Ä–µ—Ç–µ–Ω–∑–∏–µ–π', '–ø—Ä–µ—Ç–µ–Ω–∑–∏–µ—é', '–ø—Ä–µ—Ç–µ–Ω–∑–∏–π',
    '–ø—Ä–µ—Ç–µ–Ω–∑–∏—è–º', '–ø—Ä–µ—Ç–µ–Ω–∑–∏—è–º–∏', '–ø—Ä–µ—Ç–µ–Ω–∑–∏—è—Ö', '–ø—Ä–µ–¥—ä—è–≤–∏—Ç—å –ø—Ä–µ—Ç–µ–Ω–∑–∏—é',

    '—Ä–µ–∫–ª–∞–º–∞—Ü–∏—è', '—Ä–µ–∫–ª–∞–º–∞—Ü–∏–∏', '—Ä–µ–∫–ª–∞–º–∞—Ü–∏—é', '—Ä–µ–∫–ª–∞–º–∞—Ü–∏–µ–π', '—Ä–µ–∫–ª–∞–º–∞—Ü–∏–µ—é',
    '—Ä–µ–∫–ª–∞–º–∞—Ü–∏–π', '—Ä–µ–∫–ª–∞–º–∞—Ü–∏—è–º', '—Ä–µ–∫–ª–∞–º–∞—Ü–∏—è–º–∏', '—Ä–µ–∫–ª–∞–º–∞—Ü–∏—è—Ö',

    '–æ—Ç–∑—ã–≤', '–æ—Ç–∑—ã–≤–∞', '–æ—Ç–∑—ã–≤—É', '–æ—Ç–∑—ã–≤–æ–º', '–æ—Ç–∑—ã–≤–µ', '–æ—Ç–∑—ã–≤—ã', '–æ—Ç–∑—ã–≤–æ–≤',
    '–æ—Ç–∑—ã–≤–∞–º', '–æ—Ç–∑—ã–≤–∞–º–∏', '–æ—Ç–∑—ã–≤–∞—Ö', '–æ—Å—Ç–∞–≤–∏—Ç—å –æ—Ç–∑—ã–≤', '–æ—Å—Ç–∞–≤–ª—è—é –æ—Ç–∑—ã–≤',

    # –≥–ª–∞–≥–æ–ª—ã –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è
    '–æ–±—Å–ª—É–∂–∏–ª', '–æ–±—Å–ª—É–∂–∏–ª–∞', '–æ–±—Å–ª—É–∂–∏–ª–∏', '–æ–±—Å–ª—É–∂—É', '–æ–±—Å–ª—É–∂–∏—à—å', '–æ–±—Å–ª—É–∂–∏—Ç',
    '–æ–±—Å–ª—É–∂–∏–º', '–æ–±—Å–ª—É–∂–∏—Ç–µ', '–æ–±—Å–ª—É–∂–∞—Ç', '–æ–±—Å–ª—É–∂–∏–≤–∞–ª', '–æ–±—Å–ª—É–∂–∏–≤–∞–ª–∞', '–æ–±—Å–ª—É–∂–∏–≤–∞–ª–æ',
    '–æ–±—Å–ª—É–∂–∏–≤–∞–ª–∏', '–æ–±—Å–ª—É–∂–µ–Ω–æ', '–æ–±—Å–ª—É–∂–µ–Ω', '–æ–±—Å–ª—É–∂–µ–Ω–∞', '–æ–±—Å–ª—É–∂–µ–Ω—ã',

    '–ø–æ–º–æ–≥', '–ø–æ–º–æ–≥–ª–∞', '–ø–æ–º–æ–≥–ª–∏', '–ø–æ–º–æ–≥–ª–æ', '–ø–æ–º–æ–≥—É', '–ø–æ–º–æ–∂–µ—à—å', '–ø–æ–º–æ–∂–µ—Ç',
    '–ø–æ–º–æ–∂–µ–º', '–ø–æ–º–æ–∂–µ—Ç–µ', '–ø–æ–º–æ–≥—É—Ç', '–ø–æ–º–æ–≥–∞–ª', '–ø–æ–º–æ–≥–∞–ª–∞', '–ø–æ–º–æ–≥–∞–ª–æ', '–ø–æ–º–æ–≥–∞–ª–∏',

    '–ø–æ–¥–¥–µ—Ä–∂–∞–ª', '–ø–æ–¥–¥–µ—Ä–∂–∞–ª–∞', '–ø–æ–¥–¥–µ—Ä–∂–∞–ª–∏', '–ø–æ–¥–¥–µ—Ä–∂–∞–ª–æ', '–ø–æ–¥–¥–µ—Ä–∂—É', '–ø–æ–¥–¥–µ—Ä–∂–∏—à—å',
    '–ø–æ–¥–¥–µ—Ä–∂–∏—Ç', '–ø–æ–¥–¥–µ—Ä–∂–∏–º', '–ø–æ–¥–¥–µ—Ä–∂–∏—Ç–µ', '–ø–æ–¥–¥–µ—Ä–∂–∞—Ç', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–ª', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–ª–∞',

    '–ø—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–ª', '–ø—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–ª–∞', '–ø—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–ª–∏', '–ø—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É—é',
    '–ø—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–µ—à—å', '–ø—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–µ—Ç', '–ø—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–µ–º', '–ø—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–µ—Ç–µ',

    '—Ä–µ—à–∏–ª', '—Ä–µ—à–∏–ª–∞', '—Ä–µ—à–∏–ª–∏', '—Ä–µ—à–∏–ª–æ', '—Ä–µ—à—É', '—Ä–µ—à–∏—à—å', '—Ä–µ—à–∏—Ç', '—Ä–µ—à–∏–º',
    '—Ä–µ—à–∏—Ç–µ', '—Ä–µ—à–∞—Ç', '—Ä–µ—à–∞–ª', '—Ä–µ—à–∞–ª–∞', '—Ä–µ—à–∞–ª–æ', '—Ä–µ—à–∞–ª–∏', '—Ä–µ—à–∞—è',

    '–æ—Ç–≤–µ—Ç–∏–ª', '–æ—Ç–≤–µ—Ç–∏–ª–∞', '–æ—Ç–≤–µ—Ç–∏–ª–∏', '–æ—Ç–≤–µ—Ç–∏–ª–æ', '–æ—Ç–≤–µ—á—É', '–æ—Ç–≤–µ—Ç–∏—à—å', '–æ—Ç–≤–µ—Ç–∏—Ç',
    '–æ—Ç–≤–µ—Ç–∏–º', '–æ—Ç–≤–µ—Ç–∏—Ç–µ', '–æ—Ç–≤–µ—Ç—è—Ç', '–æ—Ç–≤–µ—á–∞–ª', '–æ—Ç–≤–µ—á–∞–ª–∞', '–æ—Ç–≤–µ—á–∞–ª–æ', '–æ—Ç–≤–µ—á–∞–ª–∏'
]



def detect_gender_from_text(text):
    text_lower = text.lower()
    female_patterns = [r'\b(?:–ø—Ä–∏—à–ª–∞|–ø–æ–ª—É—á–∏–ª–∞|–æ—Ñ–æ—Ä–º–∏–ª–∞|–ø–æ–∑–≤–æ–Ω–∏–ª–∞|–Ω–∞–ø–∏—Å–∞–ª–∞|—Å–∫–∞–∑–∞–ª–∞|—Å–¥–µ–ª–∞–ª–∞|—É–≤–∏–¥–µ–ª–∞|—É–∑–Ω–∞–ª–∞|—Ä–µ—à–∏–ª–∞)\b']
    male_patterns = [r'\b(?:–ø—Ä–∏—à–µ–ª|–ø–æ–ª—É—á–∏–ª|–æ—Ñ–æ—Ä–º–∏–ª|–ø–æ–∑–≤–æ–Ω–∏–ª|–Ω–∞–ø–∏—Å–∞–ª|—Å–∫–∞–∑–∞–ª|—Å–¥–µ–ª–∞–ª|—É–≤–∏–¥–µ–ª|—É–∑–Ω–∞–ª|—Ä–µ—à–∏–ª)\b']
    female_matches = sum(len(re.findall(pattern, text_lower)) for pattern in female_patterns)
    male_matches = sum(len(re.findall(pattern, text_lower)) for pattern in male_patterns)
    if female_matches > male_matches:
        return '–∂–µ–Ω—â–∏–Ω–∞'
    elif male_matches > female_matches:
        return '–º—É–∂—á–∏–Ω–∞'
    else:
        return '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω'


def load_optimized_bert_model():
    """üî• –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –∑–∞–≥—Ä—É–∑–∫–∞ BERT –º–æ–¥–µ–ª–∏"""
    try:
        # üî• –ü–†–ò–û–†–ò–¢–ï–¢: —Å–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å
        try:
            #model = SentenceTransformer('sentence-transformers/paraphrase-albert-small-v2',
            model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2',
                                        device='cuda' if torch.cuda.is_available() else 'cpu')
            print_success("–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø BERT –º–æ–¥–µ–ª—å (Albert Small)")
            return model
        except:
            # üî• –†–ï–ó–ï–†–í: –≤—Ç–æ—Ä–∞—è –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å
            try:
                model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2',
                                            device='cuda' if torch.cuda.is_available() else 'cpu')
                print_success("–ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –ª–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å (MiniLM-L6)")
                return model
            except:
                # üî• –§–û–õ–ë–≠–ö: –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
                return load_bert_model()

    except Exception as e:
        print_error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π BERT –º–æ–¥–µ–ª–∏: {e}")
        return load_bert_model()


def ultra_fast_bert_segmentation(text, bert_model, verbose=False):
    """üî• –£–õ–¨–¢–†–ê-–ë–´–°–¢–†–ê–Ø BERT —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è"""
    # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
    if len(text) < 150:
        entities = detect_entities_with_positions(text)
        primary_entity = entities[0]['entity'] if entities else '–æ–±—â–µ–µ_–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ'
        return [{
            'text': text,
            'entity_type': primary_entity,
            'service_channel': None,
            'combined_entity_name': primary_entity,
            'entity_category': 'fast_track',
            'is_entity_specific': bool(entities),
            'has_service_context': contains_service_context(text),
            'confidence': 'high',
            'segmentation_method': 'ultra_fast',
            'semantic_coherence': 1.0,
            'detection_method': 'keyword_only'
        }]

    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    if len(text) > 4000:
        text = text[:4000] + "..."

    return advanced_bert_segmentation(text, bert_model, verbose)

#def load_bert_model():
#    try:
#        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
#                                    device='cuda' if torch.cuda.is_available() else 'cpu')
#        print_success("–ó–∞–≥—Ä—É–∂–µ–Ω–∞ BERT –º–æ–¥–µ–ª—å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
#        return model
#    except Exception as e:
#        print_error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ BERT –º–æ–¥–µ–ª–∏: {e}")
#        return None

def load_bert_model():
    try:
        # üî• –ó–ê–ú–ï–ù–ê –Ω–∞ —É–ª—å—Ç—Ä–∞-–ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å (–≤ 4 —Ä–∞–∑–∞ –±—ã—Å—Ç—Ä–µ–µ!)
        #model = SentenceTransformer('sentence-transformers/paraphrase-albert-small-v2',
        model=SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2',
                                    device='cuda' if torch.cuda.is_available() else 'cpu')
        print_success("–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø BERT –º–æ–¥–µ–ª—å (Albert Small)")
        return model
    except Exception as e:
        # Fallback –∫ –µ—â–µ –±–æ–ª–µ–µ –ª–µ–≥–∫–æ–π –º–æ–¥–µ–ª–∏
        try:
            model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2',
                                        device='cuda' if torch.cuda.is_available() else 'cpu')
            print_success("–ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –ª–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å (MiniLM-L6)")
            return model
        except Exception as e2:
            print_error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ BERT –º–æ–¥–µ–ª–∏: {e2}")
            return None

def enhanced_sentence_splitting(text):
    try:
        sentences = nltk.tokenize.sent_tokenize(text, language='russian')
    except:
        sentences = re.split(r'(?<=[.!?])\s+', text)
    return [s.strip() for s in sentences if len(s.strip()) > 0]


def detect_entities_with_positions(text):
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –í–°–ï–• —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    entities_found = []
    text_lower = text.lower()

    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã
    for product, keywords in PRODUCT_ENTITIES.items():
        for keyword in keywords:
            pattern = r'\b' + re.escape(keyword) + r'\b'
            matches = list(re.finditer(pattern, text_lower))
            for match in matches:
                entities_found.append({
                    'entity': product, 'keyword': keyword,
                    'position': match.start(), 'length': len(keyword)
                })

    # –í–∞–∂–Ω–æ: –¥–æ–±–∞–≤–ª—è–µ–º –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è
    service_indicators = ['–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ', '—Å–µ—Ä–≤–∏—Å', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫', '–º–µ–Ω–µ–¥–∂–µ—Ä']
    for indicator in service_indicators:
        matches = list(re.finditer(r'\b' + indicator + r'\w*\b', text_lower))
        for match in matches:
            entities_found.append({
                'entity': '–æ–±—â–µ–µ_–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ',
                'keyword': match.group(),
                'position': match.start(),
                'length': len(match.group())
            })

    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    unique_entities = []
    seen = set()
    for entity in sorted(entities_found, key=lambda x: x['position']):
        key = (entity['position'], entity['entity'])
        if key not in seen:
            unique_entities.append(entity)
            seen.add(key)

    return unique_entities


def find_natural_boundaries(text, entities):
    boundaries = []
    if len(entities) < 2: return boundaries
    for i in range(len(entities) - 1):
        current_entity = entities[i]
        next_entity = entities[i + 1]
        gap_start = current_entity['position'] + current_entity['length']
        gap_end = next_entity['position']
        gap_text = text[gap_start:gap_end]
        best_boundary = find_best_split_point(gap_text, gap_start)
        if best_boundary: boundaries.append(best_boundary)
    return sorted(boundaries)


def find_best_split_point(gap_text, gap_start):
    gap_lower = gap_text.lower()
    split_points = []
    semicolon_pos = gap_text.find(';')
    if semicolon_pos != -1: split_points.append((gap_start + semicolon_pos + 1, 10))
    comma_positions = [m.start() for m in re.finditer(',', gap_text)]
    for pos in comma_positions:
        if pos + 1 < len(gap_text) and gap_text[pos + 1] == ' ':
            split_points.append((gap_start + pos + 1, 8))
    conjunctions = [' –∞ ', ' –Ω–æ ', ' –¥–∞ –∏ ', ' –∏ ']
    for conj in conjunctions:
        conj_pos = gap_lower.find(conj)
        if conj_pos != -1: split_points.append((gap_start + conj_pos, 5))
    if split_points:
        split_points.sort(key=lambda x: (-x[1], x[0]))
        return split_points[0][0]
    if len(gap_text) > 5: return gap_start + len(gap_text) // 2
    return None


def contains_service_context(text):
    if not text: return False
    text_lower = text.lower()
    service_indicators = 0
    for keyword in SERVICE_CONTEXT_KEYWORDS:
        pattern = r'\b' + re.escape(keyword) + r'\w*\b'
        matches = re.findall(pattern, text_lower)
        service_indicators += len(matches)
    word_count = len(text.split())
    threshold = max(1, word_count // 10)
    return service_indicators >= threshold


def bert_semantic_analysis(text, bert_model):
    """BERT –ê–ù–ê–õ–ò–ó —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    if bert_model is None:
        return {'segments': [{'text': text, 'semantic_label': 'unknown'}]}

    sentences = enhanced_sentence_splitting(text)
    if len(sentences) <= 1:
        return {'segments': [{'text': text, 'semantic_label': 'single_sentence'}]}

    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentence_embeddings = []
        sentences_to_process = []
        sentence_indices = []

        for i, sentence in enumerate(sentences):
            sentence_hash = get_text_hash(sentence)
            if sentence_hash in bert_embedding_cache:
                cached_embedding = bert_embedding_cache[sentence_hash]
                if cached_embedding.device != bert_model.device:
                    cached_embedding = cached_embedding.to(bert_model.device)
                sentence_embeddings.append(cached_embedding)
            else:
                sentences_to_process.append(sentence)
                sentence_indices.append(i)
                sentence_embeddings.append(None)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        if sentences_to_process:
            new_embeddings = bert_model.encode(
                sentences_to_process,
                convert_to_tensor=True,
                show_progress_bar=False,
                batch_size=min(32, len(sentences_to_process)),
                normalize_embeddings=True
            )

            for idx, (sentence, embedding) in enumerate(zip(sentences_to_process, new_embeddings)):
                sentence_hash = get_text_hash(sentence)
                bert_embedding_cache[sentence_hash] = embedding.cpu()
                sentence_embeddings[sentence_indices[idx]] = embedding

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        final_embeddings = []
        for embedding in sentence_embeddings:
            if embedding is not None:
                final_embeddings.append(embedding)

        if not final_embeddings:
            return {'segments': [{'text': text, 'semantic_label': 'error'}]}

        sentence_embeddings_tensor = torch.stack(final_embeddings)

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarity_matrix = util.pytorch_cos_sim(sentence_embeddings_tensor, sentence_embeddings_tensor)

        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥
        all_similarities = []
        for i in range(len(sentences)):
            for j in range(i + 1, len(sentences)):
                all_similarities.append(similarity_matrix[i][j].item())

        if all_similarities:
            avg_similarity = np.mean(all_similarities)
            std_similarity = np.std(all_similarities)
            dynamic_threshold = max(0.2, avg_similarity - std_similarity)
        else:
            dynamic_threshold = 0.3

        # –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è
        segments = []
        current_segment = [sentences[0]]
        current_embedding = sentence_embeddings_tensor[0].unsqueeze(0)

        for i in range(1, len(sentences)):
            similarity = util.pytorch_cos_sim(sentence_embeddings_tensor[i].unsqueeze(0),
                                              current_embedding.mean(dim=0))
            similarity_score = similarity.item()

            if similarity_score > dynamic_threshold:
                current_segment.append(sentences[i])
                current_embedding = torch.cat([current_embedding, sentence_embeddings_tensor[i].unsqueeze(0)])
            else:
                segment_text = ' '.join(current_segment)
                segments.append({
                    'text': segment_text,
                    'sentences': current_segment.copy(),
                    'semantic_coherence': similarity_score,
                    'segment_id': len(segments)
                })
                current_segment = [sentences[i]]
                current_embedding = sentence_embeddings_tensor[i].unsqueeze(0)

        if current_segment:
            segment_text = ' '.join(current_segment)
            segments.append({
                'text': segment_text,
                'sentences': current_segment,
                'semantic_coherence': 1.0,
                'segment_id': len(segments)
            })

        return {'segments': segments, 'similarity_matrix': similarity_matrix, 'threshold': dynamic_threshold}

    except Exception as e:
        print_warning(f"–û—à–∏–±–∫–∞ BERT –∞–Ω–∞–ª–∏–∑–∞: {e}")
        return {'segments': [{'text': text, 'semantic_label': 'error'}]}


def improved_bert_segmentation(text, bert_model, verbose=False):
    """üî• –£–õ–£–ß–®–ï–ù–ù–ê–Ø —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º BERT –∞–Ω–∞–ª–∏–∑–æ–º"""

    # 1. –£–õ–£–ß–®–ï–ù–ù–û–ï –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å –ø–æ–∑–∏—Ü–∏—è–º–∏
    entities = detect_entities_with_positions(text)

    if verbose:
        print(f"   üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Å—É—â–Ω–æ—Å—Ç–µ–π: {len(entities)}")
        for entity in entities:
            print(f"   ‚îú‚îÄ‚îÄ '{entity['keyword']}' ({entity['entity']}) –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {entity['position']}")

    # 2. –ï–°–õ–ò –Ω–∞—à–ª–∏ –†–ê–ó–ù–´–ï —Å—É—â–Ω–æ—Å—Ç–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ü–†–ê–í–ò–õ–ê –≤–º–µ—Å—Ç–æ BERT (–ë–´–°–¢–†–ï–ï)
    if len(entities) >= 2:
        entities_sorted = sorted(entities, key=lambda x: x['position'])

        # –ë–´–°–¢–†–´–ô –ø–æ–∏—Å–∫ –≥—Ä–∞–Ω–∏—Ü –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª
        boundaries = []
        for i in range(len(entities_sorted) - 1):
            current = entities_sorted[i]
            next_ent = entities_sorted[i + 1]

            gap_start = current['position'] + current['length']
            gap_end = next_ent['position']
            gap_text = text[gap_start:gap_end]

            # –ë–´–°–¢–†–´–ô –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ–∏—Å–∫–∞ —Ç–æ—á–∫–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
            boundary = find_best_split_point_fast(gap_text, gap_start)
            if boundary:
                boundaries.append(boundary)

        if boundaries:
            segments = []
            start_pos = 0
            boundaries = sorted(boundaries)

            for boundary in boundaries:
                segment_text = text[start_pos:boundary].strip()
                if len(segment_text) > 3:
                    # –ë–´–°–¢–†–û–ï –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ —Å–µ–≥–º–µ–Ω—Ç–µ
                    seg_entities = set()
                    for entity in entities_sorted:
                        if start_pos <= entity['position'] < boundary:
                            seg_entities.add(entity['entity'])

                    segments.append({
                        'text': segment_text,
                        'sentences': [segment_text],
                        'semantic_coherence': 1.0,
                        'segment_id': len(segments),
                        'entities': list(seg_entities)
                    })
                start_pos = boundary

            # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
            if start_pos < len(text):
                segment_text = text[start_pos:].strip()
                if len(segment_text) > 3:
                    seg_entities = set()
                    for entity in entities_sorted:
                        if entity['position'] >= start_pos:
                            seg_entities.add(entity['entity'])

                    segments.append({
                        'text': segment_text,
                        'sentences': [segment_text],
                        'semantic_coherence': 1.0,
                        'segment_id': len(segments),
                        'entities': list(seg_entities)
                    })

            if verbose:
                print(f"   üìä –ë—ã—Å—Ç—Ä–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

            return {'segments': segments, 'threshold': 0.5}

    # 3. –ï—Å–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏ –ø—Ä–∞–≤–∏–ª–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ë–ï–ó–û–ü–ê–°–ù–´–ô BERT
    return bert_semantic_analysis(text, bert_model)


def find_best_split_point_fast(gap_text, gap_start):
    """–ë–´–°–¢–†–ê–Ø –≤–µ—Ä—Å–∏—è –ø–æ–∏—Å–∫–∞ —Ç–æ—á–∫–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è"""
    if len(gap_text) < 5:
        return None

    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã: ; , —Å–æ—é–∑—ã
    if ';' in gap_text:
        pos = gap_text.find(';')
        return gap_start + pos + 1  # –ü–æ—Å–ª–µ —Ç–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç–æ–π

    # –ó–∞–ø—è—Ç—ã–µ —Å –ø—Ä–æ–±–µ–ª–æ–º –ø–æ—Å–ª–µ
    comma_match = re.search(r',\s+', gap_text)
    if comma_match:
        return gap_start + comma_match.end()

    # –°–æ—é–∑—ã
    for conj in [' –∞ ', ' –Ω–æ ', ' –¥–∞ –∏ ']:
        if conj in gap_text.lower():
            pos = gap_text.lower().find(conj)
            return gap_start + pos

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å–µ—Ä–µ–¥–∏–Ω–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–æ–º–µ–∂—É—Ç–∫–æ–≤
    if len(gap_text) > 15:
        return gap_start + len(gap_text) // 2

    return None

@lru_cache(maxsize=1000)
def cached_detect_entities(segment_text):
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    if not segment_text: return []
    segment_lower = segment_text.lower()
    entities_found = []
    for product, keywords in PRODUCT_ENTITIES.items():
        for keyword in keywords:
            pattern = r'\b' + re.escape(keyword) + r'\b'
            if re.search(pattern, segment_lower):
                entities_found.append(product)
                break
    return entities_found


def detect_entities_with_bert(segment_text, bert_model):
    if not segment_text or len(segment_text.strip()) < 3:
        return [{'entity_type': '–æ–±—â–µ–µ', 'combined_name': '–æ–±—â–µ–µ', 'entity_category': 'general',
                 'has_service_context': False, 'confidence': 'low', 'detection_method': 'fallback'}]

    # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ –∫—ç—à
    cached_entities = cached_detect_entities(segment_text)
    if cached_entities:
        return [{
            'entity_type': cached_entities[0],
            'combined_name': cached_entities[0],
            'entity_category': 'product_only',
            'has_service_context': contains_service_context(segment_text),
            'confidence': 'high',
            'detection_method': 'cached_keyword'
        }]

    # –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
    segment_lower = segment_text.lower()
    entities_found = []
    for product, keywords in PRODUCT_ENTITIES.items():
        for keyword in keywords:
            pattern = r'\b' + re.escape(keyword) + r'\b'
            if re.search(pattern, segment_lower):
                entities_found.append(product)
                break

    if not entities_found:
        entities_found.append('–æ–±—â–µ–µ')

    result_entities = []
    for entity in set(entities_found):
        has_service_context = contains_service_context(segment_text)
        if entity == "–æ–±—â–µ–µ_–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ":
            entity_category = 'general_service'
            confidence = 'high' if has_service_context else 'medium'
        elif entity in PRODUCT_ENTITIES:
            entity_category = 'product_with_service' if has_service_context else 'product_only'
            confidence = 'high'
        else:
            entity_category = 'general'
            confidence = 'low'

        result_entities.append({
            'entity_type': entity,
            'combined_name': entity,
            'entity_category': entity_category,
            'has_service_context': has_service_context,
            'confidence': confidence,
            'detection_method': 'keyword_based'
        })

    return result_entities


def advanced_bert_segmentation(review_text, bert_model, segmenter=None, verbose=False):
    """
    –£–õ–£–ß–®–ï–ù–ù–ê–Ø —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä–∞
    """
    # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
    if segmenter:
        return professional_bert_segmentation(review_text, segmenter, verbose)

    # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
    if len(review_text) < 100:
        entities = detect_entities_with_positions(review_text)
        if len(entities) == 1:
            return [{
                'text': review_text,
                'entity_type': entities[0]['entity'],
                'service_channel': None,
                'combined_entity_name': entities[0]['entity'],
                'entity_category': 'product_only',
                'is_entity_specific': True,
                'has_service_context': contains_service_context(review_text),
                'confidence': 'high',
                'segmentation_method': 'fast_track',
                'semantic_coherence': 1.0,
                'detection_method': 'fast_keyword'
            }]

    # –£–õ–£–ß–®–ï–ù–ù–ê–Ø —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è (–≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥)
    analysis_result = improved_bert_segmentation(review_text, bert_model, verbose)
    segments = analysis_result['segments']

    # –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
    segments = post_process_segmentation(segments, review_text)

    split_reviews = []

    for segment in segments:
        segment_text = segment['text']
        entities = detect_entities_with_bert(segment_text, bert_model)

        # –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ª–æ–≥–∏–∫–∞ –≤—ã–±–æ—Ä–∞ —Å—É—â–Ω–æ—Å—Ç–∏
        if len(entities) > 1:
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø—Ä–æ–¥—É–∫—Ç–∞–º –Ω–∞–¥ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ–º
            product_entities = [e for e in entities
                                if e['entity_type'] in PRODUCT_ENTITIES]
            service_entities = [e for e in entities
                                if e['entity_type'] == '–æ–±—â–µ–µ_–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ']

            if product_entities:
                entities = [product_entities[0]]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ø—Ä–æ–¥—É–∫—Ç
            elif service_entities:
                entities = [service_entities[0]]
            else:
                entities = [entities[0]]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Å—É—â–Ω–æ—Å—Ç—å

        for entity_info in entities:
            split_review = {
                'text': segment_text,
                'entity_type': entity_info['entity_type'],
                'service_channel': None,
                'combined_entity_name': entity_info['combined_name'],
                'entity_category': entity_info['entity_category'],
                'is_entity_specific': entity_info['entity_type'] in PRODUCT_ENTITIES,
                'has_service_context': entity_info['has_service_context'],
                'confidence': entity_info['confidence'],
                'segmentation_method': 'enhanced_bert_semantic',
                'semantic_coherence': segment.get('semantic_coherence', 1.0),
                'detection_method': entity_info.get('detection_method', 'unknown')
            }
            split_reviews.append(split_review)

    return split_reviews


def post_process_segmentation(segments, original_text):
    """–ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    if not segments:
        return segments

    # 1. –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—É—â–Ω–æ—Å—Ç–∏ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
    all_entities = set([e['entity'] for e in detect_entities_with_positions(original_text)])

    # 2. –ù–∞—Ö–æ–¥–∏–º —Å—É—â–Ω–æ—Å—Ç–∏ –≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ö
    segments_entities = set()
    for segment in segments:
        segment_entities = detect_entities_with_bert(segment['text'], None)
        segment_entity_types = [e['entity_type'] for e in segment_entities]
        segments_entities.update(segment_entity_types)

    # 3. –ï—Å–ª–∏ –∫–∞–∫–∏–µ-—Ç–æ —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ—Ç–µ—Ä—è–ª–∏—Å—å
    missing_entities = all_entities - segments_entities
    if missing_entities:
        segments = redistribute_missing_entities(segments, missing_entities, original_text)

    return segments


def redistribute_missing_entities(segments, missing_entities, original_text):
    """–ü–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –ø–æ—Ç–µ—Ä—è–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    if not missing_entities:
        return segments

    new_segments = []
    i = 0
    while i < len(segments):
        current_segment = segments[i]
        current_text = current_segment['text']
        current_entities = set([e['entity_type'] for e in detect_entities_with_bert(current_text, None)])

        should_merge = False
        if i < len(segments) - 1:
            next_segment = segments[i + 1]
            next_entities = set([e['entity_type'] for e in detect_entities_with_bert(next_segment['text'], None)])

            if (current_entities.intersection(missing_entities) and
                    next_entities.intersection(missing_entities)):
                should_merge = True

        if should_merge:
            merged_text = current_text + ' ' + next_segment['text']
            new_segment = {
                'text': merged_text,
                'sentences': current_segment.get('sentences', [current_text]) +
                             next_segment.get('sentences', [next_segment['text']]),
                'semantic_coherence': min(current_segment.get('semantic_coherence', 0.5),
                                          next_segment.get('semantic_coherence', 0.5)),
                'segment_id': len(new_segments)
            }
            new_segments.append(new_segment)
            i += 2
        else:
            new_segments.append(current_segment)
            i += 1

    return new_segments


def validate_split_results(split_reviews, original_text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è"""
    if not split_reviews:
        return False

    reconstructed = ' '.join([r['text'] for r in split_reviews])
    similarity = fuzz.ratio(original_text.lower(), reconstructed.lower())

    return similarity >= 80


def find_text_column(df):
    """–ù–∞—Ö–æ–¥–∏—Ç —Å—Ç–æ–ª–±–µ—Ü —Å —Ç–µ–∫—Å—Ç–æ–º –æ—Ç–∑—ã–≤–æ–≤"""
    text_keywords = ['—Ç–µ–∫—Å—Ç', 'text', '–æ—Ç–∑—ã–≤', 'review', 'comment', '–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π']
    for col in df.columns:
        col_lower = col.lower()
        if any(keyword in col_lower for keyword in text_keywords):
            return col

    for col in df.columns:
        if df[col].dtype == 'object' and df[col].str.len().mean() > 20:
            return col

    return df.columns[0] if len(df.columns) > 0 else None


def process_batch_parallel(batch_data, bert_model, segmenter=None, max_workers=None):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –æ—Ç–∑—ã–≤–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä–∞
    """
    if max_workers is None:
        max_workers = min(multiprocessing.cpu_count(), 8)

    results = []

    print_info(f"üîÑ –ó–∞–ø—É—Å–∫ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å {max_workers} –ø–æ—Ç–æ–∫–∞–º–∏")

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç–∑—ã–≤–∞
    tasks = [(idx, row, text_column, bert_model, segmenter) for idx, row, text_column in batch_data]

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º ThreadPoolExecutor –¥–ª—è I/O bound –æ–ø–µ—Ä–∞—Ü–∏–π
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
        future_to_task = {executor.submit(process_single_review, task): task for task in tasks}

        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        for future in tqdm(as_completed(future_to_task), total=len(tasks), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞"):
            try:
                batch_results = future.result()
                results.extend(batch_results)
            except Exception as e:
                print_error(f"–û—à–∏–±–∫–∞ –≤ –ø–æ—Ç–æ–∫–µ: {e}")
                continue

    return results


def process_dataset_parallel(input_file, output_file, bert_model, segmenter=None, batch_size=50, max_workers=None,
                             show_examples=False):
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä–∞"""
    print_step("–ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–¢–ê–°–ï–¢–ê")

    if segmenter:
        print("üéØ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä")
    else:
        print("ü§ñ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è BERT —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è")

    try:
        df = pd.read_csv(input_file, sep=';', encoding='windows-1251', on_bad_lines='warn')
        text_column = find_text_column(df)
        print_success(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –æ—Ç–∑—ã–≤–æ–≤")
    except Exception as e:
        print_error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        return False

    if show_examples:
        if segmenter:
            demonstrate_professional_analysis(segmenter, bert_model)
        else:
            demonstrate_bert_analysis(bert_model)

    all_results = []
    original_columns = [col for col in df.columns if col != text_column]

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    batches = []
    current_batch = []

    for idx, row in df.iterrows():
        current_batch.append((idx, row, text_column))
        if len(current_batch) >= batch_size:
            batches.append(current_batch)
            current_batch = []

    if current_batch:
        batches.append(current_batch)

    print_info(f"üì¶ –†–∞–∑–¥–µ–ª–µ–Ω–æ –Ω–∞ {len(batches)} –±–∞—Ç—á–µ–π –ø–æ {batch_size} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ (—á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å –ø–∞–º—è—Ç—å)
    for i, batch_data in enumerate(batches):
        print_info(f"üîß –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i + 1}/{len(batches)}")

        start_time = time.time()
        batch_results = process_batch_parallel(batch_data, bert_model, segmenter, max_workers)
        batch_time = time.time() - start_time

        all_results.extend(batch_results)

        print_success(f"‚úÖ –ë–∞—Ç—á {i + 1} –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {batch_time:.2f} —Å–µ–∫. ({len(batch_results)} –∑–∞–ø–∏—Å–µ–π)")

    # –°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π DataFrame
    result_df = pd.DataFrame(all_results)

    try:
        result_df.to_csv(output_file, sep=';', index=False, encoding='windows-1251')
        print_success(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")

        print("\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò:")
        print(f"   ‚Ä¢ –ò—Å—Ö–æ–¥–Ω—ã–µ –æ—Ç–∑—ã–≤—ã: {len(df)}")
        print(f"   ‚Ä¢ –†–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏: {len(result_df)}")
        print(f"   ‚Ä¢ –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è: {len(result_df) / len(df):.2f}x")
        print(f"   ‚Ä¢ –†–µ–∂–∏–º: {'–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è' if segmenter else '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è BERT —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è'}")

        print(f"\nüë• –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –û–ü–†–ï–î–ï–õ–ï–ù–ù–û–ì–û –ü–û–õ–ê:")
        gender_stats = result_df['detected_gender'].value_counts()
        for gender, count in gender_stats.items():
            pct = count / len(result_df) * 100
            print(f"   ‚Ä¢ {gender}: {count} ({pct:.1f}%)")

        print(f"\nüè¶ –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –°–£–©–ù–û–°–¢–ï–ô:")
        entity_stats = result_df['entity_type'].value_counts().head(10)
        for entity, count in entity_stats.items():
            pct = count / len(result_df) * 100
            print(f"   ‚Ä¢ {entity}: {count} ({pct:.1f}%)")

        return True

    except Exception as e:
        print_error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        return False


def demonstrate_bert_analysis(bert_model):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –£–õ–£–ß–®–ï–ù–ù–û–ì–û BERT –∞–Ω–∞–ª–∏–∑–∞"""
    print_step("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –£–õ–£–ß–®–ï–ù–ù–û–ì–û BERT –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê")

    test_cases = [
        "–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –≤ –æ—Ñ–∏—Å–µ –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ, –∞ –≤–æ—Ç –≤–∫–ª–∞–¥ —à–∏–∫–∞—Ä–µ–Ω –¥–∞ –∏ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ —Ç–æ–∂–µ —Ö–æ—Ä–æ—à–µ–µ",
        "–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –≤ –æ—Ñ–∏—Å–µ –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ, –≤–∫–ª–∞–¥ —à–∏–∫–∞—Ä–µ–Ω –¥–∞ –∏ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ —Ç–æ–∂–µ —Ö–æ—Ä–æ—à–µ–µ",
        "–∫—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞ –æ—Ç–ª–∏—á–Ω–∞—è, —Å—Ç–∞–≤–∫–∞ –º–∞–ª–µ–Ω—å–∫–∞—è, –∞ –º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ–µ",
        "–ú–µ–Ω–µ–¥–∂–µ—Ä –ø–æ–º–æ–≥ –æ—Ñ–æ—Ä–º–∏—Ç—å –¥–µ–±–µ—Ç–æ–≤—É—é –∫–∞—Ä—Ç—É, –≤—Å–µ –æ–±—ä—è—Å–Ω–∏–ª, –Ω–æ –ø–æ—Ç–æ–º –∫–æ–º–∏—Å—Å–∏—è –æ–∫–∞–∑–∞–ª–∞—Å—å –≤—ã—Å–æ–∫–æ–π"
    ]

    for i, text in enumerate(test_cases, 1):
        print(f"\nüß™ –¢–ï–°–¢ {i}:")
        print(f"   üìù –¢–µ–∫—Å—Ç: {text}")

        entities = detect_entities_with_positions(text)
        print(f"   üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ —Å –ø–æ–∑–∏—Ü–∏—è–º–∏:")
        for entity in entities:
            print(f"   ‚îú‚îÄ‚îÄ {entity['entity']} (–ø–æ–∑–∏—Ü–∏—è: {entity['position']})")

        points = get_optimal_segmentation_points(text, entities)
        print(f"   üìç –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {points}")

        analysis = improved_bert_segmentation(text, bert_model)
        print(f"   üîç –£–ª—É—á—à–µ–Ω–Ω—ã–π BERT –Ω–∞—à–µ–ª {len(analysis['segments'])} —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤:")

        for j, segment in enumerate(analysis['segments'], 1):
            print(f"   ‚îú‚îÄ‚îÄ –°–µ–≥–º–µ–Ω—Ç {j}: {segment['text'][:60]}...")
            print(f"   ‚îÇ   –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å: {segment.get('semantic_coherence', 0):.2f}")
            if 'entities' in segment:
                print(f"   ‚îÇ   –°—É—â–Ω–æ—Å—Ç–∏ –≤ —Å–µ–≥–º–µ–Ω—Ç–µ: {segment['entities']}")

        result = advanced_bert_segmentation(text, bert_model)
        print(f"   üéØ –ò—Ç–æ–≥–æ–≤–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {len(result)} —á–∞—Å—Ç–µ–π")
        for r in result:
            print(f"   ‚îú‚îÄ‚îÄ [{r['entity_type']}] {r['text'][:50]}...")


def interactive_bert_analysis(bert_model, segmenter=None):
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –≤—ã–±–æ—Ä–æ–º —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä–∞"""
    print_step("–ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –ê–ù–ê–õ–ò–ó –û–¢–ó–´–í–û–í")

    if segmenter:
        print("‚úÖ –î–æ—Å—Ç—É–ø–µ–Ω –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä")
        use_professional = input("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä? (y/n): ").strip().lower()
        use_professional = use_professional in ['y', 'yes', '–¥–∞', '–¥']
    else:
        use_professional = False
        print("‚ö†Ô∏è –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π")

    while True:
        print("\n" + "=" * 70)
        text = input("üìù –í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–∏–ª–∏ '–≤—ã—Ö–æ–¥'): ").strip()

        if text.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit']:
            break

        if not text:
            continue

        try:
            print("\nüî¨ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç...")

            if use_professional and segmenter:
                result = professional_bert_segmentation(text, segmenter, verbose=True)
                method = "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è"
            else:
                # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ø–µ—Ä–µ–¥–∞–µ–º segmenter –∫–∞–∫ None –≤ advanced_bert_segmentation
                result = advanced_bert_segmentation(text, bert_model, segmenter=None)
                method = "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è BERT —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è"

            print(f"‚úÖ {method}: {len(result)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

            print_entity_result(text, result)

            print(f"\nüîç –î–µ—Ç–∞–ª–∏ –∞–Ω–∞–ª–∏–∑–∞ ({method}):")
            for i, item in enumerate(result, 1):
                method_icon = "üéØ" if use_professional else "ü§ñ"
                coherence = item.get('semantic_coherence', 0)
                confidence_icon = "üî¥" if item['confidence'] == 'low' else "üü°" if item[
                                                                                     'confidence'] == 'medium' else "üü¢"
                print(
                    f"   {i}. {method_icon} {confidence_icon} [{item['entity_type']}] (—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å: {coherence:.2f})")
                print(f"      {item['text'][:80]}...")

        except Exception as e:
            print_error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")

        cont = input("\n‚ñ∂Ô∏è  –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∞–Ω–∞–ª–∏–∑? (y/n): ").strip().lower()
        if cont not in ['y', 'yes', '–¥–∞', '–¥']:
            break


def print_entity_result(text, entities):
    print(f"\nüìã –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç–∑—ã–≤–∞:")
    print(f"   üìù –¢–µ–∫—Å—Ç: \"{text[:200]}{'...' if len(text) > 200 else ''}\"")
    if entities:
        print("   üè∑Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏:")
        for entity in entities:
            confidence_icon = "üî¥" if entity['confidence'] == 'low' else "üü°" if entity['confidence'] == 'medium' else "üü¢"
            service_icon = "üë•" if entity['has_service_context'] else "üì¶"
            print(f"      {confidence_icon} {service_icon} {entity['entity_type']} ({entity['entity_category']})")
    else:
        print("   ‚ùå –°—É—â–Ω–æ—Å—Ç–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")
    print("-" * 80)


def get_optimal_segmentation_points(text, entities):
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    segmentation_points = []

    if not entities:
        return segmentation_points

    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–∫–∏ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏
    for i in range(1, len(entities)):
        current_entity = entities[i]
        prev_entity = entities[i - 1]

        # –ï—Å–ª–∏ —Å—É—â–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã–µ –∏ –º–µ–∂–¥—É –Ω–∏–º–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
        if (current_entity['entity'] != prev_entity['entity'] and
                current_entity['position'] - (prev_entity['position'] + prev_entity['length']) > 10):

            # –ò—â–µ–º –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—É—é –≥—Ä–∞–Ω–∏—Ü—É –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç–∏
            gap_start = prev_entity['position'] + prev_entity['length']
            gap_end = current_entity['position']
            gap_text = text[gap_start:gap_end].lower()

            # –ò—â–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏–ª–∏ —Å–æ—é–∑—ã –∫–∞–∫ —Ç–æ—á–∫–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
            boundary_chars = [',', ';', '–∞', '–Ω–æ', '–¥–∞ –∏', '–∏']
            boundary_pos = -1

            for boundary in boundary_chars:
                pos = gap_text.find(boundary)
                if pos != -1 and (boundary_pos == -1 or pos < boundary_pos):
                    boundary_pos = pos

            if boundary_pos != -1:
                segmentation_points.append(gap_start + boundary_pos)

    return segmentation_points


def process_single_review(args):
    idx, row, text_column, bert_model, segmenter = args  # –î–æ–±–∞–≤–ª—è–µ–º segmenter
    try:
        text = str(row[text_column])
        if not text or text.lower() in ['nan', 'none', '']:
            return []
        detected_gender = detect_gender_from_text(text)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if segmenter:
            split_reviews = professional_bert_segmentation(text, segmenter)
        else:
            with bert_lock:
                split_reviews = advanced_bert_segmentation(text, bert_model, segmenter=None)

        results = []
        for review in split_reviews:
            new_row = {
                'original_review_id': idx,
                'text': review['text'],
                'entity_type': review['entity_type'],
                'service_channel': review['service_channel'],
                'combined_entity_name': review['combined_entity_name'],
                'entity_category': review['entity_category'],
                'is_entity_specific': review['is_entity_specific'],
                'has_service_context': review['has_service_context'],
                'confidence': review['confidence'],
                'segmentation_method': review['segmentation_method'],
                'semantic_coherence': review.get('semantic_coherence', 0),
                'detection_method': review.get('detection_method', 'unknown'),
                'detected_gender': detected_gender
            }
            for col in row.index:
                if col != text_column:
                    new_row[col] = row[col]
            results.append(new_row)
        return results
    except Exception as e:
        print_warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏ {idx}: {e}")
        return []



def main():
    print("=" * 80)
    print("\033[1;35müéØ –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–´–ô –°–ï–ì–ú–ï–ù–¢–ê–¢–û–† –ë–ê–ù–ö–û–í–°–ö–ò–• –û–¢–ó–´–í–û–í\033[0m")
    print("=" * 80)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä
    try:
        segmenter = ProfessionalSegmenter()
        print_success("‚úÖ –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
        professional_mode = True
    except Exception as e:
        print_warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä: {e}")
        print_warning("–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è BERT —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è")
        segmenter = None
        professional_mode = False

    # –ó–∞–≥—Ä—É–∂–∞–µ–º BERT –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
    bert_model = load_bert_model()
    if bert_model is None:
        print_error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å BERT –º–æ–¥–µ–ª—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É sentence-transformers.")
        return

    print_success("‚úÖ –ú–æ–¥–µ–ª–∏ –≥–æ—Ç–æ–≤—ã –∫ —Ä–∞–±–æ—Ç–µ")

    # –°–ø—Ä–æ—Å–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —á—Ç–æ –æ–Ω —Ö–æ—á–µ—Ç —Å–¥–µ–ª–∞—Ç—å
    print("\nüîç –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã:")
    print("1 - –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–æ–≤")
    print("2 - –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ CSV —Ñ–∞–π–ª–∞")
    print("3 - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö")
    print("4 - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä–æ–≤")
    print("5 - –í—ã—Ö–æ–¥")

    try:
        choice = input("–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä –≤–∞—Ä–∏–∞–Ω—Ç–∞ (1-5): ").strip()

        if choice == "1":
            print("\nüéØ –†–µ–∂–∏–º: –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –ê–ù–ê–õ–ò–ó")
            if professional_mode:
                print("   ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä")
            else:
                print("   ‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è BERT —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è")
            interactive_bert_analysis(bert_model, segmenter)

        elif choice == "2":
            print("\nüéØ –†–µ–∂–∏–º: –ü–ê–ö–ï–¢–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê")
            if professional_mode:
                print("   ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä")
            else:
                print("   ‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è BERT —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è")

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏
            print("\n‚ö° –ù–ê–°–¢–†–û–ô–ö–ò –ú–ù–û–ì–û–ü–û–¢–û–ß–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–ò:")
            print(f"   ‚Ä¢ –î–æ—Å—Ç—É–ø–Ω–æ —è–¥–µ—Ä CPU: {multiprocessing.cpu_count()}")
            if professional_mode:
                print("   ‚Ä¢ –†–µ–∂–∏–º: –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è")

            try:
                max_workers = int(input("   –í–≤–µ–¥–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 4-8): ") or "4")
                batch_size = int(input("   –í–≤–µ–¥–∏—Ç–µ —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 20-100): ") or "50")
            except:
                max_workers = 4
                batch_size = 50
                print_warning("–ò—Å–ø–æ–ª—å–∑—É—é –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 4 –ø–æ—Ç–æ–∫–∞, –±–∞—Ç—á 50")

            input_file = input("–í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É: ").strip() or 'GaspromBank_dataset.csv'
            output_file = input("–í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: ").strip()

            if professional_mode:
                output_file = output_file or 'GaspromBank_professional_dataset.csv'
            else:
                output_file = output_file or 'GaspromBank_bert_parallel_dataset.csv'

            # –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            start_time = time.time()

            success = process_dataset_parallel(
                input_file,
                output_file,
                bert_model,
                segmenter=segmenter,  # –ü–µ—Ä–µ–¥–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä
                batch_size=batch_size,
                max_workers=max_workers,
                show_examples=False
            )

            total_time = time.time() - start_time

            if success:
                print("\n" + "=" * 80)
                print(f"\033[1;32m‚úÖ –ü–ê–ö–ï–¢–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–ê\033[0m")
                print(f"   ‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_time:.2f} —Å–µ–∫—É–Ω–¥")
                print(
                    f"   üéØ –†–µ–∂–∏–º: {'–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è' if professional_mode else '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è BERT —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è'}")
                try:
                    df = pd.read_csv(input_file, sep=';', encoding='windows-1251', on_bad_lines='warn')
                    print(f"   üöÄ –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(df) / total_time:.2f} –æ—Ç–∑—ã–≤–æ–≤/—Å–µ–∫")
                except:
                    pass
                print("=" * 80)
            else:
                print_error("‚ùå –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –° –û–®–ò–ë–ö–ê–ú–ò")

        elif choice == "3":
            print("\nüéØ –†–µ–∂–∏–º: –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –ü–†–ò–ú–ï–†–ê–•")
            if professional_mode:
                print("   ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä")
                demonstrate_professional_analysis(segmenter, bert_model)
            else:
                print("   ‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è BERT —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è")
                demonstrate_bert_analysis(bert_model)

        elif choice == "4":
            print("\nüéØ –†–µ–∂–∏–º: –°–†–ê–í–ù–ï–ù–ò–ï –°–ï–ì–ú–ï–ù–¢–ê–¢–û–†–û–í")
            if professional_mode:
                compare_segmentators(segmenter, bert_model)
            else:
                print("‚ùå –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
                print("–ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö (–æ–ø—Ü–∏—è 3)")

        elif choice == "5":
            print("üëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
        else:
            print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")

    except KeyboardInterrupt:
        print("\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è...")
    except Exception as e:
        print_error(f"–û—à–∏–±–∫–∞: {e}")


def interactive_bert_analysis(bert_model, segmenter=None):
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –≤—ã–±–æ—Ä–æ–º —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä–∞"""
    print_step("–ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –ê–ù–ê–õ–ò–ó –û–¢–ó–´–í–û–í")

    if segmenter:
        print("‚úÖ –î–æ—Å—Ç—É–ø–µ–Ω –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä")
        use_professional = input("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä? (y/n): ").strip().lower()
        use_professional = use_professional in ['y', 'yes', '–¥–∞', '–¥']
    else:
        use_professional = False
        print("‚ö†Ô∏è –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π")

    while True:
        print("\n" + "=" * 70)
        text = input("üìù –í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–∏–ª–∏ '–≤—ã—Ö–æ–¥'): ").strip()

        if text.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit']:
            break

        if not text:
            continue

        try:
            print("\nüî¨ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç...")

            if use_professional and segmenter:
                result = professional_bert_segmentation(text, segmenter, verbose=True)
                method = "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è"
            else:
                result = advanced_bert_segmentation(text, bert_model)
                method = "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è BERT —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è"

            print(f"‚úÖ {method}: {len(result)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

            print_entity_result(text, result)

            print(f"\nüîç –î–µ—Ç–∞–ª–∏ –∞–Ω–∞–ª–∏–∑–∞ ({method}):")
            for i, item in enumerate(result, 1):
                method_icon = "üéØ" if use_professional else "ü§ñ"
                coherence = item.get('semantic_coherence', 0)
                confidence_icon = "üî¥" if item['confidence'] == 'low' else "üü°" if item[
                                                                                     'confidence'] == 'medium' else "üü¢"
                print(
                    f"   {i}. {method_icon} {confidence_icon} [{item['entity_type']}] (—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å: {coherence:.2f})")
                print(f"      {item['text'][:80]}...")

        except Exception as e:
            print_error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")

        cont = input("\n‚ñ∂Ô∏è  –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∞–Ω–∞–ª–∏–∑? (y/n): ").strip().lower()
        if cont not in ['y', 'yes', '–¥–∞', '–¥']:
            break


def demonstrate_professional_analysis(segmenter, bert_model):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä–∞"""
    print_step("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–û–ô –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–ò")

    test_cases = [
        "–°–æ–±–∏—Ä–∞—é—Å—å –≤ –ê—Ä–º–µ–Ω–∏—é, –µ—Å—Ç—å –∫–∞—Ä—Ç–∞ –º–∏—Ä, —Ö–æ—Ç–µ–ª–∞ —É–∑–Ω–∞—Ç—å —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–∏ –æ–Ω–∞ —Ç–∞–º, –Ω–∞–ø–∏—Å–∞–ª–∞ –í–ö 18.07 –≤ 16:14, –∫–∞–∫ –æ–±—ã—á–Ω–æ, –æ—Ç–≤–µ—Ç–∏–ª–∏ –±—ã—Å—Ç—Ä–æ, —á–µ—Ä–µ–∑ 2 –º–∏–Ω—É—Ç—ã. –°–ø–µ—Ä–≤–∞ —É—Ç–æ—á–Ω–∏–ª–∞ —Ä–∞–±–æ—Ç–∞—é—Ç –ª–∏ –∫–∞—Ä—Ç—ã UP, –æ—Ç–≤–µ—Ç–∏–ª–∏, —á—Ç–æ –∑–∞ –≥—Ä–∞–Ω–∏—Ü–µ–π –Ω–µ—Ç, —Ç–æ–ª—å–∫–æ –≤ –†–§. –î–∞–ª–µ–µ —Å–ø—Ä–æ—Å–∏–ª–∞ –≤ –∫–∞–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∞—Ö –ø—Ä–∏–Ω–∏–º–∞—é—Ç –∫–∞—Ä—Ç—ã –º–∏—Ä, –∏ –ø—Ä–∏–Ω–∏–º–∞—é—Ç –ª–∏ –≤ –ê—Ä–º–µ–Ω–∏–∏, –æ—Ç–≤–µ—Ç–∏–ª–∏ –ø—Ä–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –ø—Ä–æ —Ç–æ, —á—Ç–æ –≤ –ê—Ä–º–µ–Ω–∏–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞—Ä—Ç—É —Ç–æ–ª—å–∫–æ –≤ —Ç–µ—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö, –≥–¥–µ —ç–∫–≤–∞–π–µ—Ä –í–¢–ë. –ò —É—Ç–æ—á–Ω–∏–ª–∞ –µ—â—ë –ø—Ä–æ –≤–∏–∑—É, –Ω–∞–ø–∏—Å–∞–ª–∏, —á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –†–§. –ü–µ—Ä–µ–ø–∏—Å–∫—É –ø—Ä–∏–∫—Ä–µ–ø–ª—è—é. –ë–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ —á—ë—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã",
        "–û—Ñ–æ—Ä–º–ª—è–ª –∫—Ä–µ–¥–∏—Ç–Ω—É—é –∫–∞—Ä—Ç—É, –º–µ–Ω–µ–¥–∂–µ—Ä –±—ã–ª –æ—á–µ–Ω—å –≤–µ–∂–ª–∏–≤, –≤—Å–µ –æ–±—ä—è—Å–Ω–∏–ª. –ü–æ—Ç–æ–º —Ä–µ—à–∏–ª –æ—Ç–∫—Ä—ã—Ç—å –≤–∫–ª–∞–¥, –Ω–æ —Ç–∞–º —É–∂–µ –¥—Ä—É–≥–æ–π —Å–æ—Ç—Ä—É–¥–Ω–∏–∫ –ø–æ–º–æ–≥–∞–ª, —Ç–æ–∂–µ –≤—Å–µ —á–µ—Ç–∫–æ —Ä–∞—Å—Å–∫–∞–∑–∞–ª –ø—Ä–æ –ø—Ä–æ—Ü–µ–Ω—Ç—ã",
        "–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –≥–ª—é—á–∏—Ç, –Ω–µ –º–æ–≥—É –≤–æ–π—Ç–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫. –ó–≤–æ–Ω–∏–ª –≤ –ø–æ–¥–¥–µ—Ä–∂–∫—É, –¥–æ–ª–≥–æ –∂–¥–∞–ª –æ—Ç–≤–µ—Ç–∞, –≤ –∏—Ç–æ–≥–µ —Å–∫–∞–∑–∞–ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, –Ω–æ –Ω–µ –ø–æ–º–æ–≥–ª–æ"
    ]

    for i, text in enumerate(test_cases, 1):
        print(f"\nüß™ –¢–ï–°–¢ {i}:")
        print(f"   üìù –¢–µ–∫—Å—Ç: {text[:100]}...")

        # –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è
        professional_result = professional_bert_segmentation(text, segmenter)
        print(f"   üéØ –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è: {len(professional_result)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

        for j, seg in enumerate(professional_result, 1):
            print(f"   ‚îú‚îÄ‚îÄ {j}. [{seg['entity_type']}] {seg['text'][:60]}...")

        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        standard_result = advanced_bert_segmentation(text, bert_model)
        print(f"   ü§ñ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è BERT —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è: {len(standard_result)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

        for j, seg in enumerate(standard_result, 1):
            print(f"   ‚îú‚îÄ‚îÄ {j}. [{seg['entity_type']}] {seg['text'][:60]}...")


def compare_segmentators(segmenter, bert_model):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä–æ–≤"""
    print_step("–°–†–ê–í–ù–ï–ù–ò–ï –°–ï–ì–ú–ï–ù–¢–ê–¢–û–†–û–í")

    test_text = "–°–æ–±–∏—Ä–∞—é—Å—å –≤ –ê—Ä–º–µ–Ω–∏—é, –µ—Å—Ç—å –∫–∞—Ä—Ç–∞ –º–∏—Ä, —Ö–æ—Ç–µ–ª–∞ —É–∑–Ω–∞—Ç—å —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–∏ –æ–Ω–∞ —Ç–∞–º, –Ω–∞–ø–∏—Å–∞–ª–∞ –í–ö 18.07 –≤ 16:14, –∫–∞–∫ –æ–±—ã—á–Ω–æ, –æ—Ç–≤–µ—Ç–∏–ª–∏ –±—ã—Å—Ç—Ä–æ, —á–µ—Ä–µ–∑ 2 –º–∏–Ω—É—Ç—ã. –°–ø–µ—Ä–≤–∞ —É—Ç–æ—á–Ω–∏–ª–∞ —Ä–∞–±–æ—Ç–∞—é—Ç –ª–∏ –∫–∞—Ä—Ç—ã UP, –æ—Ç–≤–µ—Ç–∏–ª–∏, —á—Ç–æ –∑–∞ –≥—Ä–∞–Ω–∏—Ü–µ–π –Ω–µ—Ç, —Ç–æ–ª—å–∫–æ –≤ –†–§. –î–∞–ª–µ–µ —Å–ø—Ä–æ—Å–∏–ª–∞ –≤ –∫–∞–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∞—Ö –ø—Ä–∏–Ω–∏–º–∞—é—Ç –∫–∞—Ä—Ç—ã –º–∏—Ä, –∏ –ø—Ä–∏–Ω–∏–º–∞—é—Ç –ª–∏ –≤ –ê—Ä–º–µ–Ω–∏–∏, –æ—Ç–≤–µ—Ç–∏–ª–∏ –ø—Ä–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –ø—Ä–æ —Ç–æ, —á—Ç–æ –≤ –ê—Ä–º–µ–Ω–∏–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞—Ä—Ç—É —Ç–æ–ª—å–∫–æ –≤ —Ç–µ—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö, –≥–¥–µ —ç–∫–≤–∞–π–µ—Ä –í–¢–ë. –ò —É—Ç–æ—á–Ω–∏–ª–∞ –µ—â—ë –ø—Ä–æ –≤–∏–∑—É, –Ω–∞–ø–∏—Å–∞–ª–∏, —á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –†–§. –ü–µ—Ä–µ–ø–∏—Å–∫—É –ø—Ä–∏–∫—Ä–µ–ø–ª—è—é. –ë–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ —á—ë—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã"

    print("üìä –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –Ω–∞ –ø—Ä–æ–±–ª–µ–º–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ:")
    print(f"üìù –¢–µ–∫—Å—Ç: {test_text[:150]}...\n")

    # –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏
    start_time = time.time()
    professional_result = professional_bert_segmentation(test_text, segmenter)
    professional_time = time.time() - start_time

    start_time = time.time()
    standard_result = advanced_bert_segmentation(test_text, bert_model)
    standard_time = time.time() - start_time

    print(f"üéØ –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–´–ô –°–ï–ì–ú–ï–ù–¢–ê–¢–û–†:")
    print(f"   ‚Ä¢ –°–µ–≥–º–µ–Ω—Ç–æ–≤: {len(professional_result)}")
    print(f"   ‚Ä¢ –í—Ä–µ–º—è: {professional_time:.3f} —Å–µ–∫")
    for i, seg in enumerate(professional_result, 1):
        print(f"   {i}. [{seg['entity_type']}] {seg['text'][:70]}...")

    print(f"\nü§ñ –°–¢–ê–ù–î–ê–†–¢–ù–´–ô BERT –°–ï–ì–ú–ï–ù–¢–ê–¢–û–†:")
    print(f"   ‚Ä¢ –°–µ–≥–º–µ–Ω—Ç–æ–≤: {len(standard_result)}")
    print(f"   ‚Ä¢ –í—Ä–µ–º—è: {standard_time:.3f} —Å–µ–∫")
    for i, seg in enumerate(standard_result, 1):
        print(f"   {i}. [{seg['entity_type']}] {seg['text'][:70]}...")

    print(
        f"\nüìà –í–´–í–û–î: –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä {'–±—ã—Å—Ç—Ä–µ–µ' if professional_time < standard_time else '–º–µ–¥–ª–µ–Ω–Ω–µ–µ'} –∏ –¥–∞–µ—Ç {'–ª—É—á—à–∏–µ' if len(professional_result) < len(standard_result) else '—Å—Ä–∞–≤–Ω–∏–º—ã–µ'} —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")


# –û–±–Ω–æ–≤–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä–∞
def process_single_review(args):
    idx, row, text_column, bert_model, segmenter = args  # –î–æ–±–∞–≤–ª—è–µ–º segmenter
    try:
        text = str(row[text_column])
        if not text or text.lower() in ['nan', 'none', '']: return []
        detected_gender = detect_gender_from_text(text)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if segmenter:
            split_reviews = professional_bert_segmentation(text, segmenter)
        else:
            with bert_lock:
                split_reviews = advanced_bert_segmentation(text, bert_model)

        results = []
        for review in split_reviews:
            new_row = {
                'original_review_id': idx,
                'text': review['text'],
                'entity_type': review['entity_type'],
                'service_channel': review['service_channel'],
                'combined_entity_name': review['combined_entity_name'],
                'entity_category': review['entity_category'],
                'is_entity_specific': review['is_entity_specific'],
                'has_service_context': review['has_service_context'],
                'confidence': review['confidence'],
                'segmentation_method': review['segmentation_method'],
                'semantic_coherence': review.get('semantic_coherence', 0),
                'detection_method': review.get('detection_method', 'unknown'),
                'detected_gender': detected_gender
            }
            for col in row.index:
                if col != text_column:
                    new_row[col] = row[col]
            results.append(new_row)
        return results
    except Exception as e:
        print_warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏ {idx}: {e}")
        return []


if __name__ == "__main__":
    main()