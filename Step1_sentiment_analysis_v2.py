# Step1_sentiment_analysis_radical_with_fuzzy.py
import pandas as pd
import numpy as np
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import torch
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import sys
import os
import re
import json
from typing import List, Dict, Tuple, Optional, Any
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, \
    classification_report
from collections import defaultdict, Counter
from sklearn.utils import resample
import matplotlib.pyplot as plt
from transformers import TrainerCallback

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CUDA –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
os.environ["CUDA_LAUNCH_BLOCKING"] = "0"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import warnings

warnings.filterwarnings("ignore")


def print_step(message):
    print(f"\n\033[1;36m>>> {message}\033[0m")


def print_success(message):
    print(f"\033[1;32m‚úì {message}\033[0m")


def print_warning(message):
    print(f"\033[1;33m‚ö† {message}\033[0m")


def print_error(message):
    print(f"\033[1;31m‚úó {message}\033[0m")


print("=" * 80)
print("\033[1;35müöÄ –†–ê–î–ò–ö–ê–õ–¨–ù–û–ï –£–õ–£–ß–®–ï–ù–ò–ï + –õ–ï–ì–ö–ê–Ø FUZZY-–õ–û–ì–ò–ö–ê\033[0m")
print("=" * 80)


class LightFuzzyEnhancer:
    def __init__(self):
        self.strong_negative_indicators = [
            # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
            '–∑–∞–≤–∏—Å–∞–µ—Ç', '–≤—ã–ª–µ—Ç–∞–µ—Ç', '–≥–ª—é—á–∏—Ç', '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç', '—Ç–æ—Ä–º–æ–∑–∏—Ç', '–ª–∞–≥–∞–µ—Ç',
            '–æ—à–∏–±–∫–∞', '—Å–±–æ–π', '–±–∞–≥', '–ø–∞–¥–µ–Ω–∏–µ', '–∑–∞–∫—Ä—ã–ª–æ—Å—å', '–æ—Ç–∫–ª—é—á–∏–ª–æ—Å—å',

            # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
            '–æ–±–º–∞–Ω', '–æ–±–º–∞–Ω—É–ª', '–Ω–∞–≤—è–∑–∞–ª–∏', '–Ω–∞–≤—è–∑—ã–≤–∞—é—Ç', '—Å–∫—Ä—ã–ª–∏', '—Å–∫—Ä—ã–≤–∞—é—Ç',
            '–∫–æ–º–∏—Å—Å–∏—è', '–∫–æ–º–∏—Å—Å–∏–∏', '—Å–ø–∏—Å–∞–ª–∏', '—Å–Ω—è–ª–∏', '—Å–ø–∏—Å—ã–≤–∞—é—Ç', '—Å–Ω–∏–º–∞—é—Ç',
            '–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞', '–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª–∏', '–±–ª–æ–∫–∏—Ä—É—é—Ç', '–æ—Ç–∫–∞–∑–∞–ª–∏', '–æ—Ç–∫–∞–∑',
            '–¥–æ–ª–≥–æ', '–º–µ–¥–ª–µ–Ω–Ω–æ', '–æ–∂–∏–¥–∞–Ω–∏–µ', '–∂–¥–∞—Ç—å', '–æ—á–µ—Ä–µ–¥—å', '–æ—á–µ—Ä–µ–¥–∏',

            # –°–µ—Ä–≤–∏—Å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
            '–≥—Ä—É–±', '—Ö–∞–º', '—Ö–∞–º—Å—Ç–≤–æ', '–Ω–µ–≤–µ–∂–ª–∏–≤', '–Ω–µ–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω', '–∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç',
            '–Ω–µ–ø–æ–º–æ–≥', '–æ—Ç–∫–∞–∑–∞–ª–∏', '–Ω–µ—Ä–µ—à–∏–ª–∏', '–Ω–µ—Å–º–æ–≥–ª–∏',

            # –°–∏–ª—å–Ω—ã–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —ç–º–æ—Ü–∏–∏
            '—É–∂–∞—Å', '–∫–æ—à–º–∞—Ä', '–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ', '–±–µ—Å–∏—Ç', '—Ä–∞–∑–¥—Ä–∞–∂–∞–µ—Ç', '–Ω–µ—Ä–≤—ã',
            '–Ω–∏–∫–æ–≥–¥–∞ –±–æ–ª—å—à–µ', '—Ö—É–∂–µ –Ω–µ–∫—É–¥–∞', '–ø–æ–∑–æ—Ä', '—Å—Ç—ã–¥', '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω',

            # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ—Ç–µ—Ä–∏
            '–ø–æ—Ç–µ—Ä—è–ª', '–ø–æ—Ç–µ—Ä—è–ª–∏', '—É–∫—Ä–∞–ª–∏', '–º–æ—à–µ–Ω–Ω–∏–∫', '–º–æ—à–µ–Ω–Ω–∏–∫–∏', '–∞—Ñ–µ—Ä–∞',
            '–∫–∏–Ω—É–ª–∏', '–æ–±–≤–æ—Ä–æ–≤–∞–ª–∏', '–æ–±—Å—á–∏—Ç–∞–ª–∏',

            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
            '–Ω–µ–ª—å–∑—è –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è', '–Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ', '–Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω', '–Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è',
            '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç', '–Ω–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç'
        ]

        # –ë–û–õ–ï–ï –ö–û–ù–°–ï–†–í–ê–¢–ò–í–ù–´–ô —Å–ø–∏—Å–æ–∫ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        self.strong_positive_indicators = [
            # –£–º–µ—Ä–µ–Ω–Ω—ã–µ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
            '–¥–æ–≤–æ–ª–µ–Ω', '–¥–æ–≤–æ–ª—å–Ω–∞', '—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω', '—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∞',
            '–Ω–æ—Ä–º–∞–ª—å–Ω–æ', '–Ω–æ—Ä–º–∞–ª—å–Ω—ã–π', '—Ö–æ—Ä–æ—à–æ', '—Ö–æ—Ä–æ—à–∏–π',

            # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–æ—Ö–≤–∞–ª—ã
            '–≤–µ–∂–ª–∏–≤', '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª', '–∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω',
            '–±—ã—Å—Ç—Ä–æ', '–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ', '–ø–æ–º–æ–≥', '—Ä–µ—à–∏–ª', '–ø–æ–º–æ–≥–ª–∏', '—Ä–µ—à–∏–ª–∏',

            # –£–º–µ—Ä–µ–Ω–Ω—ã–µ —ç–º–æ—Ü–∏–∏
            '—Å–ø–∞—Å–∏–±–æ', '–±–ª–∞–≥–æ–¥–∞—Ä', '—Ä–µ–∫–æ–º–µ–Ω–¥—É—é',

            # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω—ã–µ —ç–º–æ—Ü–∏–∏ –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–¥–∫–æ –≤ –æ—Ç–∑—ã–≤–∞—Ö –æ –±–∞–Ω–∫–∞—Ö
            # '–≤–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω–æ', '—à–∏–∫–∞—Ä–Ω–æ', '–±–µ–∑—É–ø—Ä–µ—á–Ω–æ' - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è –æ—á–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
        ]

        # –î–û–ë–ê–í–õ–Ø–ï–ú –ù–ï–ô–¢–†–ê–õ–¨–ù–´–ï/–°–ú–ï–®–ê–ù–ù–´–ï –ò–ù–î–ò–ö–ê–¢–û–†–´
        self.neutral_indicators = [
            '–Ω–æ—Ä–º–∞–ª—å–Ω–æ', '–æ–±—ã—á–Ω–æ', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ', '—Ç–µ—Ä–ø–∏–º–æ', '—Å–æ–π–¥–µ—Ç', '—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ',
            '–ø—Ä–∏–µ–º–ª–µ–º–æ', '—Å—Ä–µ–¥–Ω–µ', '–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ'
        ]

        self.contrast_words = [' –Ω–æ ', ' –æ–¥–Ω–∞–∫–æ ', ' –∞ ', ' —Ö–æ—Ç—è ', ' —Ç–µ–º –Ω–µ –º–µ–Ω–µ–µ ', ' –∑–∞—Ç–æ ', ' –∞ –≤–æ—Ç ']

    def enhance_prediction(self, text: str, base_label: str, confidence: float):
        text_lower = text.lower()

        # –°–ß–ò–¢–ê–ï–ú –°–ò–õ–£ –ö–ê–ñ–î–û–ô –ö–ê–¢–ï–ì–û–†–ò–ò
        neg_strength = sum(3 if word in text_lower else 0 for word in self.strong_negative_indicators)
        pos_strength = sum(2 if word in text_lower else 0 for word in self.strong_positive_indicators)
        neutral_strength = sum(1 if word in text_lower else 0 for word in self.neutral_indicators)

        # 1. –ü–†–ò–û–†–ò–¢–ï–¢: –Ø–í–ù–´–ô –ù–ï–ì–ê–¢–ò–í (–±–∞–Ω–∫–æ–≤—Å–∫–∏–µ –æ—Ç–∑—ã–≤—ã —á–∞—â–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ)
        if neg_strength >= 2:  # –•–æ—Ç—è –±—ã 2 —Å–∏–ª—å–Ω—ã—Ö –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞
            if base_label != 'NEGATIVE':
                return 'NEGATIVE', min(0.95, confidence + 0.15), "—è–≤–Ω—ã–π_–Ω–µ–≥–∞—Ç–∏–≤_–±–∞–Ω–∫"

        # 2. –°–ú–ï–®–ê–ù–ù–´–ï/–ù–ï–ô–¢–†–ê–õ–¨–ù–´–ï –°–õ–£–ß–ê–ò
        if neutral_strength >= 2 and (pos_strength + neg_strength) < 3:
            return 'NEUTRAL', min(0.85, confidence + 0.1), "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π_–∫–æ–Ω—Ç–µ–∫—Å—Ç"

        # 3. –£–ú–ï–†–ï–ù–ù–´–ô –ü–û–ó–ò–¢–ò–í (—Ç—Ä–µ–±—É–µ–º –±–æ–ª—å—à–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤)
        if pos_strength >= 3 and neg_strength == 0:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–∞
            if base_label != 'POSITIVE':
                return 'POSITIVE', min(0.9, confidence + 0.1), "—É–º–µ—Ä–µ–Ω–Ω—ã–π_–ø–æ–∑–∏—Ç–∏–≤"

        # 4. –ö–û–ù–¢–†–ê–°–¢–ù–´–ï –°–õ–£–ß–ê–ò - —á–∞—â–µ –≤ —Å—Ç–æ—Ä–æ–Ω—É –Ω–µ–≥–∞—Ç–∏–≤–∞
        for contrast_word in self.contrast_words:
            if contrast_word in text_lower:
                parts = re.split(f"{contrast_word}\\s+", text_lower)
                if len(parts) >= 2:
                    # –í –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –æ—Ç–∑—ã–≤–∞—Ö –≤—Ç–æ—Ä–∞—è —á–∞—Å—Ç—å —á–∞—Å—Ç–æ –≤–∞–∂–Ω–µ–µ
                    second_part = parts[1]
                    second_neg = sum(1 for word in self.strong_negative_indicators if word in second_part)
                    second_pos = sum(1 for word in self.strong_positive_indicators if word in second_part)

                    if second_neg > second_pos:
                        return 'NEGATIVE', min(0.9, confidence + 0.1), "–∫–æ–Ω—Ç—Ä–∞—Å—Ç_–Ω–µ–≥–∞—Ç–∏–≤"

        return base_label, confidence, "–±–µ–∑_–∏–∑–º–µ–Ω–µ–Ω–∏–π"


class RadicalSentimentFineTuningDataset(Dataset):
    """–£–õ–£–ß–®–ï–ù–ù–´–ô –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏"""

    def __init__(self, texts, labels, tokenizer, max_length=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.label_map = {'–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ': 0, '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ': 1, '–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ': 2}

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.label_map.get(self.labels[idx], 1)

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }


class RadicalSentimentFineTuner:
    """–†–ê–î–ò–ö–ê–õ–¨–ù–´–ô –∫–ª–∞—Å—Å –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é"""

    def __init__(self):
        self.model_name = None
        self.tokenizer = None
        self.model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.fuzzy_enhancer = LightFuzzyEnhancer()

        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫
        self.train_losses = []
        self.eval_losses = []
        self.eval_accuracies = []

        # –°–ª–æ–≤–∞—Ä—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ç–µ–º
        self.topic_keywords = {
            "–º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ": ["–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "–º–æ–±–∏–ª—å–Ω—ã–π", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏", "–º–æ–±–∏–ª—å–Ω–æ–µ", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—é"],
            "–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –≤ –æ—Ñ–∏—Å–∞—Ö –∏ –æ—Ç–¥–µ–ª–µ–Ω–∏—è—Ö": ["–æ—Ñ–∏—Å", "–æ—Ç–¥–µ–ª–µ–Ω", "–æ–±—Å–ª—É–∂–∏–≤–∞–Ω", "–º–µ–Ω–µ–¥–∂–µ—Ä", "—Å–æ—Ç—Ä—É–¥–Ω–∏–∫",
                                                   "–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç", "—Ñ–∏–ª–∏–∞–ª"],
            "–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã": ["–∫—Ä–µ–¥–∏—Ç–Ω–∞—è", "–∫–∞—Ä—Ç–∞", "–∫—Ä–µ–¥–∏—Ç–Ω—É—é", "–∫–∞—Ä—Ç—É", "–∫—Ä–µ–¥–∏—Ç–Ω–æ–π", "–∫–∞—Ä—Ç–æ–π"],
            "–¥–µ–±–µ—Ç–æ–≤—ã–µ –∫–∞—Ä—Ç—ã": ["–¥–µ–±–µ—Ç–æ–≤–∞—è", "–∫–∞—Ä—Ç–∞", "–¥–µ–±–µ—Ç–æ–≤—É—é", "–∫–∞—Ä—Ç—É", "–¥–µ–±–µ—Ç–æ–≤–æ–π", "–∫–∞—Ä—Ç–æ–π"],
            "–∏–ø–æ—Ç–µ–∫–∞": ["–∏–ø–æ—Ç–µ–∫", "–∏–ø–æ—Ç–µ–∫—É", "–∏–ø–æ—Ç–µ–∫–∏", "–∏–ø–æ—Ç–µ—á–Ω–æ–π", "–∏–ø–æ—Ç–µ—á–Ω—ã–π"],
            "—Å—Ç—Ä–∞—Ö–æ–≤—ã–µ –∏ —Å–µ—Ä–≤–∏—Å–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã": ["—Å—Ç—Ä–∞—Ö–æ–≤–∫", "—Å—Ç—Ä–∞—Ö–æ–≤–∫—É", "—Å—Ç—Ä–∞—Ö–æ–≤–æ–π", "—Å–µ—Ä–≤–∏—Å", "—É—Å–ª—É–≥", "–ø—Ä–æ–¥—É–∫—Ç"],
            "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫": ["–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫", "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç –±–∞–Ω–∫", "–æ–Ω–ª–∞–π–Ω-–±–∞–Ω–∫", "–æ–Ω–ª–∞–π–Ω –±–∞–Ω–∫"],
            "–¥–µ–Ω–µ–∂–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã": ["–ø–µ—Ä–µ–≤–æ–¥", "–ø–µ—Ä–µ–≤–æ–¥—ã", "–ø–µ—Ä–µ–≤–æ–¥–∞", "–ø–µ—Ä–µ–≤–æ–¥–∞–º", "–¥–µ–Ω–µ–∂–Ω"],
            "–≤–∫–ª–∞–¥—ã": ["–≤–∫–ª–∞–¥", "–≤–∫–ª–∞–¥—ã", "–≤–∫–ª–∞–¥–∞", "–≤–∫–ª–∞–¥–∞–º", "–¥–µ–ø–æ–∑–∏—Ç"],
            "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏": ["–∏–Ω–≤–µ—Å—Ç–∏—Ü", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π", "–∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞"],
            "–∑–∞—Ä–ø–ª–∞—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã": ["–∑–∞—Ä–ø–ª–∞—Ç–Ω–∞—è", "–∫–∞—Ä—Ç–∞", "–∑–∞—Ä–ø–ª–∞—Ç–Ω—É—é", "–∫–∞—Ä—Ç—É", "–∑–∞—Ä–ø–ª–∞—Ç–Ω–æ–π"],
            "–ø—Ä–µ–º–∏–∞–ª—å–Ω—ã–µ –∫–∞—Ä—Ç—ã": ["–ø—Ä–µ–º–∏–∞–ª—å–Ω–∞—è", "–∫–∞—Ä—Ç–∞", "–ø—Ä–µ–º–∏–∞–ª—å–Ω—É—é", "–∫–∞—Ä—Ç—É", "–ø—Ä–µ–º–∏–∞–ª—å–Ω–æ–π"],
            "–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–µ —Å—á–µ—Ç–∞": ["–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–π", "—Å—á–µ—Ç", "–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω–æ–≥–æ", "—Å—á–µ—Ç–∞", "–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–º"],
            "–∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ": ["–∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç", "–∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç–∞", "–∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç—É", "–∞–≤—Ç–æ –∫—Ä–µ–¥–∏—Ç"],
            "—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –∫—Ä–µ–¥–∏—Ç–æ–≤": ["—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω", "—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤", "—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ"],
            "–±–æ–Ω—É—Å–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã": ["–±–æ–Ω—É—Å", "–±–æ–Ω—É—Å—ã", "–±–æ–Ω—É—Å–æ–≤", "–±–æ–Ω—É—Å–Ω–æ–π", "–ø—Ä–æ–≥—Ä–∞–º–º"]
        }

    def load_better_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –£–õ–£–ß–®–ï–ù–ù–û–ô –º–æ–¥–µ–ª–∏"""
        print_step("–ó–∞–≥—Ä—É–∑–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–¥–∏–∫–∞–ª—å–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è")

        model_options = [
            "blanchefort/rubert-base-cased-sentiment",  # –õ—É—á—à–∞—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ sentiment
            "seara/rubert-tiny2-russian-sentiment",  # –õ–µ–≥–∫–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è
            "cointegrated/rubert-tiny-sentiment-balanced",  # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è
        ]

        for model_name in model_options:
            try:
                print(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {model_name}")
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.model = AutoModelForSequenceClassification.from_pretrained(
                    model_name,
                    num_labels=3
                )
                self.model.to(self.device)
                self.model_name = model_name
                print_success(f"–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_name}")
                return True
            except Exception as e:
                print_warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {model_name}: {e}")
                continue

        print_error("–í—Å–µ —É–ª—É—á—à–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å")
        return False

    def extract_topic_segment(self, text: str, topic: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –†–ï–ê–õ–¨–ù–´–ô —Å–µ–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ç–µ–º–µ"""
        text_lower = text.lower()

        # –ü–æ–ª—É—á–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ç–µ–º—ã
        keywords = self.topic_keywords.get(topic, [topic.lower()])

        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', text)
        relevant_sentences = []

        for sentence in sentences:
            sentence_clean = sentence.strip()
            if not sentence_clean:
                continue

            sentence_lower = sentence_clean.lower()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
            if any(keyword in sentence_lower for keyword in keywords):
                relevant_sentences.append(sentence_clean)

        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Ö
        if relevant_sentences:
            segment = " ".join(relevant_sentences)
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            if len(segment) > 20:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ—Å–º—ã—Å–ª–µ–Ω–Ω–∞—è –¥–ª–∏–Ω–∞
                return segment

        return None

    def prepare_high_quality_dataset(self, reviews_file: str, results_file: str):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ö–ê–ß–ï–°–¢–í–ï–ù–ù–û–ì–û –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ –†–ï–ê–õ–¨–ù–´–• —Å–µ–≥–º–µ–Ω—Ç–æ–≤"""
        print_step("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ö–ê–ß–ï–°–¢–í–ï–ù–ù–û–ì–û –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            with open(reviews_file, 'r', encoding='utf-8') as f:
                reviews_data = json.load(f)
            with open(results_file, 'r', encoding='utf-8') as f:
                results_data = json.load(f)

            # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ id -> —Ç–µ–∫—Å—Ç
            id_to_text = {}
            for item in reviews_data['data']:
                id_to_text[item['id']] = item['text']

            training_texts = []
            training_labels = []
            segment_stats = defaultdict(int)

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –æ—Ç–∑—ã–≤
            for prediction in tqdm(results_data['predictions'], desc="–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤"):
                review_id = prediction['id']
                if review_id in id_to_text:
                    full_text = id_to_text[review_id]

                    # –î–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã –∏–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Å–µ–≥–º–µ–Ω—Ç
                    for topic, sentiment in zip(prediction['topics'], prediction['sentiments']):
                        segment = self.extract_topic_segment(full_text, topic)
                        if segment:
                            training_texts.append(segment)
                            training_labels.append(sentiment)
                            segment_stats[topic] += 1

            print_success(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(training_texts)} –†–ï–ê–õ–¨–ù–´–• —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤")
            print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ–º–∞–º:")
            for topic, count in segment_stats.items():
                print(f"   - {topic}: {count} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

            return training_texts, training_labels

        except Exception as e:
            print_error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            return [], []

    def balance_dataset(self, texts: List[str], labels: List[str]):
        """–ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ê –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏"""
        print_step("–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–∞")

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        label_counts = Counter(labels)
        print(f"üìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –î–û –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ò:")
        for label, count in label_counts.items():
            percentage = count / len(labels) * 100
            icon = "üëé" if label == '–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ' else "üëç" if label == '–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ' else "‚ûñ"
            print(f"   {icon} {label}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤ ({percentage:.1f}%)")

        # –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º oversampling –º–µ–Ω—å—à–µ–≥–æ –∫–ª–∞—Å—Å–∞
        max_count = max(label_counts.values())
        balanced_texts = []
        balanced_labels = []

        for label in ['–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ', '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ', '–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ']:
            label_texts = [t for t, l in zip(texts, labels) if l == label]

            if len(label_texts) < max_count:
                # Oversampling –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
                label_texts_resampled = resample(
                    label_texts,
                    n_samples=max_count,
                    random_state=42,
                    replace=True
                )
                balanced_texts.extend(label_texts_resampled)
                balanced_labels.extend([label] * len(label_texts_resampled))
                print(f"   üîÑ {label}: oversampling {len(label_texts)} ‚Üí {len(label_texts_resampled)}")
            else:
                balanced_texts.extend(label_texts)
                balanced_labels.extend([label] * len(label_texts))

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        balanced_counts = Counter(balanced_labels)
        print(f"üìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û–°–õ–ï –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ò:")
        for label, count in balanced_counts.items():
            percentage = count / len(balanced_labels) * 100
            icon = "üëé" if label == '–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ' else "üëç" if label == '–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ' else "‚ûñ"
            print(f"   {icon} {label}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤ ({percentage:.1f}%)")

        return balanced_texts, balanced_labels

    def plot_training_loss(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è"""
        try:
            if not self.train_losses or not self.eval_losses:
                print_warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ –æ—à–∏–±–∫–∏")
                return

            plt.figure(figsize=(12, 8))

            # –ì—Ä–∞—Ñ–∏–∫ –æ—à–∏–±–∫–∏
            plt.subplot(2, 1, 1)
            epochs_range = range(1, len(self.train_losses) + 1)
            eval_epochs = range(1, len(self.eval_losses) + 1)

            plt.plot(epochs_range, self.train_losses, 'b-', label='Train Loss', linewidth=2)
            plt.plot(eval_epochs, self.eval_losses, 'r-', label='Validation Loss', linewidth=2)
            plt.title('üìâ –ò–∑–º–µ–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è', fontsize=14, fontweight='bold')
            plt.xlabel('–®–∞–≥ –æ–±—É—á–µ–Ω–∏—è')
            plt.ylabel('–û—à–∏–±–∫–∞ (Loss)')
            plt.legend()
            plt.grid(True, alpha=0.3)

            # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
            if self.eval_accuracies:
                plt.subplot(2, 1, 2)
                accuracy_epochs = range(1, len(self.eval_accuracies) + 1)
                plt.plot(accuracy_epochs, self.eval_accuracies, 'g-', label='Validation Accuracy', linewidth=2)
                plt.title('üìà –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è', fontsize=14, fontweight='bold')
                plt.xlabel('–≠–ø–æ—Ö–∞')
                plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å (Accuracy)')
                plt.legend()
                plt.grid(True, alpha=0.3)

            plt.tight_layout()

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
            plot_filename = "training_metrics.png"
            plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
            plt.show()

            print_success(f"üìä –ì—Ä–∞—Ñ–∏–∫ –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {plot_filename}")

            # –ê–Ω–∞–ª–∏–∑ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            if len(self.eval_losses) >= 3:
                last_losses = self.eval_losses[-3:]
                if all(abs(last_losses[i] - last_losses[i - 1]) < 0.01 for i in range(1, len(last_losses))):
                    print_success("‚úÖ –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏")
                else:
                    print_warning("‚ö† –ú–æ–¥–µ–ª—å –µ—â–µ –Ω–µ —Å–æ—à–ª–∞—Å—å, –≤–æ–∑–º–æ–∂–Ω–æ —Å—Ç–æ–∏—Ç —É–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö")

        except Exception as e:
            print_warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ –æ—à–∏–±–∫–∏: {e}")

    def train_high_accuracy(self, training_texts: List[str], training_labels: List[str],
                            output_dir: str = "./radical_fine_tuned_model"):
        """–†–ê–î–ò–ö–ê–õ–¨–ù–û–ï –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è 85% —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å –≥—Ä–∞—Ñ–∏–∫–æ–º –æ—à–∏–±–∫–∏"""

        if not training_texts:
            print_error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return False

        print_step("–ó–∞–ø—É—Å–∫ –†–ê–î–ò–ö–ê–õ–¨–ù–û–ì–û –æ–±—É—á–µ–Ω–∏—è –¥–ª—è 85% —Ç–æ—á–Ω–æ—Å—Ç–∏")

        try:
            # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
            dataset = RadicalSentimentFineTuningDataset(
                texts=training_texts,
                labels=training_labels,
                tokenizer=self.tokenizer,
                max_length=256
            )

            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/validation (85/15 –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è)
            train_size = int(0.85 * len(dataset))
            val_size = len(dataset) - train_size
            train_dataset, val_dataset = torch.utils.data.random_split(
                dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)
            )

            print(f"üìä –£–õ–£–ß–®–ï–ù–ù–û–ï –†–ê–ó–ë–ò–ï–ù–ò–ï –î–ê–ù–ù–´–•:")
            print(f"  - –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {train_size} –ø—Ä–∏–º–µ—Ä–æ–≤")
            print(f"  - –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {val_size} –ø—Ä–∏–º–µ—Ä–æ–≤")
            print(f"  - –í—Å–µ–≥–æ: {len(dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")

            # –ö–∞—Å—Ç–æ–º–Ω—ã–π callback –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫
            class LossCallback(TrainerCallback):
                def __init__(self, outer):
                    self.outer = outer

                def on_log(self, args, state, control, logs=None, **kwargs):
                    if logs is not None:
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º train loss
                        if 'loss' in logs:
                            self.outer.train_losses.append(logs['loss'])
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º eval loss
                        if 'eval_loss' in logs:
                            self.outer.eval_losses.append(logs['eval_loss'])
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º accuracy
                        if 'eval_accuracy' in logs:
                            self.outer.eval_accuracies.append(logs['eval_accuracy'])

            # –†–ê–î–ò–ö–ê–õ–¨–ù–´–ï –∞—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
            training_args = TrainingArguments(
                output_dir=output_dir,
                num_train_epochs=20,
                per_device_train_batch_size=16,
                per_device_eval_batch_size=16,
                learning_rate=3e-5,
                warmup_steps=100,
                weight_decay=0.01,
                logging_dir='./radical_logs',
                logging_steps=20,
                eval_strategy="epoch",
                save_strategy="epoch",
                load_best_model_at_end=True,
                metric_for_best_model="accuracy",
                greater_is_better=True,
                report_to=None,
                save_total_limit=2,
                dataloader_pin_memory=False,
                gradient_accumulation_steps=2,
            )

            # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
            def compute_metrics(eval_pred):
                predictions, labels = eval_pred
                predictions = np.argmax(predictions, axis=1)

                accuracy = accuracy_score(labels, predictions)
                precision = precision_score(labels, predictions, average='weighted', zero_division=0)
                recall = recall_score(labels, predictions, average='weighted', zero_division=0)
                f1 = f1_score(labels, predictions, average='weighted', zero_division=0)

                return {
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1': f1
                }

            # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä —Å callback
            trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=val_dataset,
                tokenizer=self.tokenizer,
                compute_metrics=compute_metrics,
                callbacks=[LossCallback(self)]
            )

            # –ó–∞–ø—É—Å–∫–∞–µ–º –†–ê–î–ò–ö–ê–õ–¨–ù–û–ï –æ–±—É—á–µ–Ω–∏–µ
            print_step("üöÄ –ó–ê–ü–£–°–ö –†–ê–î–ò–ö–ê–õ–¨–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø...")
            training_result = trainer.train()

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            trainer.save_model(output_dir)
            self.tokenizer.save_pretrained(output_dir)

            # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫ –æ—à–∏–±–∫–∏
            self.plot_training_loss()

            # –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
            print_step("üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –†–ê–î–ò–ö–ê–õ–¨–ù–û–ì–û –î–û–û–ë–£–ß–ï–ù–ò–Ø")

            # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            eval_results = trainer.evaluate()
            final_accuracy = eval_results.get('eval_accuracy', 0)

            print(f"üéØ –ö–õ–Æ–ß–ï–í–´–ï –ú–ï–¢–†–ò–ö–ò:")
            print(f"  - Final Accuracy: {final_accuracy:.4f}")
            print(f"  - Final F1-score: {eval_results.get('eval_f1', 0):.4f}")
            print(f"  - Final Loss: {eval_results.get('eval_loss', 0):.4f}")

            # –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            test_predictions = trainer.predict(val_dataset)
            pred_labels = np.argmax(test_predictions.predictions, axis=1)
            true_labels = test_predictions.label_ids

            # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
            target_names = ['NEGATIVE', 'NEUTRAL', 'POSITIVE']
            report = classification_report(true_labels, pred_labels, target_names=target_names, digits=4)
            print(f"\nüìä –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò:")
            print(report)

            # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
            accuracy_percent = final_accuracy * 100
            print(f"\nüìà –ò–¢–û–ì–û–í–ê–Ø –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê:")
            if accuracy_percent >= 85:
                print_success(f"  üéâ –ü–†–ï–í–û–°–•–û–î–ù–û! –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Ü–µ–ª—å: {accuracy_percent:.1f}% —Ç–æ—á–Ω–æ—Å—Ç–∏!")
            elif accuracy_percent >= 80:
                print_success(f"  ‚úÖ –û–¢–õ–ò–ß–ù–û! –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy_percent:.1f}%")
            elif accuracy_percent >= 75:
                print_success(f"  ‚úÖ –•–û–†–û–®–û! –ö–∞—á–µ—Å—Ç–≤–æ —É–ª—É—á—à–µ–Ω–æ: {accuracy_percent:.1f}%")
            elif accuracy_percent >= 70:
                print_warning(f"  ‚ö† –ù–û–†–ú–ê–õ–¨–ù–û! –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy_percent:.1f}%")
            else:
                print_warning(f"  ‚ö† –¢–†–ï–ë–£–ï–¢–°–Ø –î–û–†–ê–ë–û–¢–ö–ê: {accuracy_percent:.1f}% —Ç–æ—á–Ω–æ—Å—Ç–∏")

            print_success(f"–†–∞–¥–∏–∫–∞–ª—å–Ω–æ –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_dir}")
            return final_accuracy >= 0.75

        except Exception as e:
            print_error(f"–û—à–∏–±–∫–∞ —Ä–∞–¥–∏–∫–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: {e}")
            import traceback
            traceback.print_exc()
            return False

    def analyze_sentiment_with_fuzzy(self, text: str):
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å fuzzy-–∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π"""
        try:
            # –ë–∞–∑–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ BERT
            inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=256).to(self.device)
            with torch.no_grad():
                outputs = self.model(**inputs)
                probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
                predicted_class = torch.argmax(probabilities, dim=1).item()
                confidence = probabilities[0][predicted_class].item()

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –Ω–æ–º–µ—Ä –∫–ª–∞—Å—Å–∞ –≤ –º–µ—Ç–∫—É
            class_mapping = {0: 'NEGATIVE', 1: 'NEUTRAL', 2: 'POSITIVE'}
            base_label = class_mapping.get(predicted_class, 'NEUTRAL')

            # –ü—Ä–∏–º–µ–Ω—è–µ–º fuzzy-–∫–æ—Ä—Ä–µ–∫—Ü–∏—é
            final_label, final_confidence, reason = self.fuzzy_enhancer.enhance_prediction(
                text, base_label, confidence
            )

            return {
                'text': text,
                'base_sentiment': base_label,
                'base_confidence': confidence,
                'final_sentiment': final_label,
                'final_confidence': final_confidence,
                'correction_reason': reason,
                'was_corrected': base_label != final_label
            }
        except Exception as e:
            print_warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return {
                'text': text,
                'base_sentiment': 'NEUTRAL',
                'base_confidence': 0.5,
                'final_sentiment': 'NEUTRAL',
                'final_confidence': 0.5,
                'correction_reason': 'error',
                'was_corrected': False
            }


def test_radical_model_with_fuzzy():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ª–µ–≥–∫–æ–π fuzzy-–∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π"""
    try:
        from transformers import pipeline

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        classifier = pipeline(
            "text-classification",
            model="./radical_fine_tuned_model",
            tokenizer="./radical_fine_tuned_model",
            device=0 if torch.cuda.is_available() else -1
        )

        fuzzy_enhancer = LightFuzzyEnhancer()

        test_cases = [
            "–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –∑–∞–≤–∏—Å–∞–µ—Ç –∏ –≤—ã–ª–µ—Ç–∞–µ—Ç",
            "–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª–µ–Ω–∏–∏ –æ—Ç–ª–∏—á–Ω–æ–µ, –º–µ–Ω–µ–¥–∂–µ—Ä—ã –≤–µ–∂–ª–∏–≤—ã–µ",
            "–ö—Ä–µ–¥–∏—Ç–Ω—É—é –∫–∞—Ä—Ç—É –æ—Ñ–æ—Ä–º–∏–ª–∏ –±—ã—Å—Ç—Ä–æ, –æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω",
            "–£–∂–∞—Å–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ, –Ω–∏–∫–æ–≥–¥–∞ –±–æ–ª—å—à–µ –Ω–µ –æ–±—Ä–∞—â—É—Å—å",
            "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —É–¥–æ–±–Ω–æ–µ, –Ω–æ –∏–Ω–æ–≥–¥–∞ —Ç–æ—Ä–º–æ–∑–∏—Ç",
            "–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ, –Ω–æ –∫–æ–º–∏—Å—Å–∏–∏ –≤—ã—Å–æ–∫–∏–µ",
            "–°–æ—Ç—Ä—É–¥–Ω–∏–∫ –ø–æ–º–æ–≥ –±—ã—Å—Ç—Ä–æ, —Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ",
            "–î–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Å –∫–µ—à–±—ç–∫–æ–º —Ä–∞–¥—É–µ—Ç, –∞ –≤–æ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–æ"
        ]

        print("\nüß™ –¢–ï–°–¢ –° FUZZY-–ö–û–†–†–ï–ö–¶–ò–ï–ô:")
        corrections_count = 0

        for text in test_cases:
            # –ë–∞–∑–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            base_result = classifier(text)[0]
            base_label = base_result['label']
            base_confidence = base_result['score']

            # Fuzzy-–∫–æ—Ä—Ä–µ–∫—Ü–∏—è
            final_label, final_confidence, reason = fuzzy_enhancer.enhance_prediction(
                text, base_label, base_confidence
            )

            was_corrected = base_label != final_label
            if was_corrected:
                corrections_count += 1

            base_icon = "üëç" if base_label == 'POSITIVE' else "üëé" if base_label == 'NEGATIVE' else "‚ûñ"
            final_icon = "üëç" if final_label == 'POSITIVE' else "üëé" if final_label == 'NEGATIVE' else "‚ûñ"

            correction_indicator = " üîÑ" if was_corrected else ""

            print(f"  {base_icon}‚Üí{final_icon}{correction_indicator} '{text}'")
            print(f"     BERT: {base_label} ({base_confidence:.3f})")
            print(f"     –§–∏–Ω–∞–ª—å–Ω–æ–µ: {final_label} ({final_confidence:.3f}) - {reason}")
            print()

        print_success(f"Fuzzy-–ª–æ–≥–∏–∫–∞ –∏—Å–ø—Ä–∞–≤–∏–ª–∞ {corrections_count} –∏–∑ {len(test_cases)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        return True

    except Exception as e:
        print_warning(f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å fuzzy –Ω–µ —É–¥–∞–ª–æ—Å—å: {e}")
        return False


def radical_fine_tune_sentiment_model():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞–¥–∏–∫–∞–ª—å–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è"""
    print_step("üöÄ –ó–ê–ü–£–°–ö –†–ê–î–ò–ö–ê–õ–¨–ù–û–ì–û –î–û–û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–¥–∏–∫–∞–ª—å–Ω–æ–≥–æ —Ç—é–Ω–µ—Ä–∞
    fine_tuner = RadicalSentimentFineTuner()

    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    if not fine_tuner.load_better_model():
        print_error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å")
        return False

    # 2. –ì–æ—Ç–æ–≤–∏–º –ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–ï –¥–∞–Ω–Ω—ã–µ
    training_texts, training_labels = fine_tuner.prepare_high_quality_dataset(
        "reviewforlearn.json",
        "resultforlearn.json"
    )

    if not training_texts:
        print_error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        return False

    # 3. –ë–ê–õ–ê–ù–°–ò–†–£–ï–ú –∫–ª–∞—Å—Å—ã
    balanced_texts, balanced_labels = fine_tuner.balance_dataset(training_texts, training_labels)

    print_success(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(balanced_texts)} —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")

    # 4. –ó–∞–ø—É—Å–∫–∞–µ–º –†–ê–î–ò–ö–ê–õ–¨–ù–û–ï –æ–±—É—á–µ–Ω–∏–µ
    success = fine_tuner.train_high_accuracy(
        training_texts=balanced_texts,
        training_labels=balanced_labels,
        output_dir="./radical_fine_tuned_model"
    )

    if success:
        print_success("üéâ –†–ê–î–ò–ö–ê–õ–¨–ù–û–ï –î–û–û–ë–£–ß–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å fuzzy-–ª–æ–≥–∏–∫–æ–π
        print_step("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –° FUZZY-–ö–û–†–†–ï–ö–¶–ò–ï–ô")
        test_radical_model_with_fuzzy()

        return True
    else:
        print_error("–†–∞–¥–∏–∫–∞–ª—å–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–µ –¥–æ—Å—Ç–∏–≥–ª–æ —Ü–µ–ª–µ–≤–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏")
        return False


def run_standard_sentiment_analysis():
    """–ó–∞–ø—É—Å–∫ –æ–±—ã—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ CSV"""
    try:
        print_step("1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        input_file = 'GaspromBank_professional_dataset.csv'
        if not os.path.exists(input_file):
            input_file = 'GaspromBank_dataset.csv'
            if not os.path.exists(input_file):
                print_error("–§–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return False

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = pd.read_csv(input_file, sep=';', encoding='windows-1251')
        print_success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –∏–∑ {input_file}")

        # –ü–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏
        text_column = 'text'
        if text_column not in df.columns:
            for col in df.columns:
                if 'text' in col.lower() or '–æ—Ç–∑—ã–≤' in col.lower():
                    text_column = col
                    break
            print_warning(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–ª–æ–Ω–∫–∞: {text_column}")

        print_step("2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

        # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        analyzer = RadicalSentimentFineTuner()

        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –µ—Å—Ç—å
        fine_tuned_path = "./radical_fine_tuned_model"
        if os.path.exists(fine_tuned_path):
            print("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –∑–∞–≥—Ä—É–∂–∞–µ–º...")
            try:
                analyzer.tokenizer = AutoTokenizer.from_pretrained(fine_tuned_path)
                analyzer.model = AutoModelForSequenceClassification.from_pretrained(fine_tuned_path)
                analyzer.model.to(analyzer.device)
                analyzer.fuzzy_enhancer = LightFuzzyEnhancer()
                print_success("–î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception as e:
                print_warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å: {e}")
                if not analyzer.load_better_model():
                    return False
        else:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
            if not analyzer.load_better_model():
                return False

        print_step("3. –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å fuzzy-–∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π")

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –í–°–ï –¥–∞–Ω–Ω—ã–µ
        texts_to_analyze = df[text_column].astype(str).tolist()

        print(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(texts_to_analyze)} –ø—Ä–∏–º–µ—Ä–æ–≤...")

        results = []
        for text in tqdm(texts_to_analyze, desc="–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"):
            result = analyzer.analyze_sentiment_with_fuzzy(text)
            results.append(result)

        print_step("4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ CSV")

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DataFrame
        df['sentiment_base'] = [r['base_sentiment'] for r in results]
        df['sentiment_final'] = [r['final_sentiment'] for r in results]
        df['sentiment_confidence'] = [r['final_confidence'] for r in results]
        df['was_corrected'] = [r['was_corrected'] for r in results]
        df['correction_reason'] = [r['correction_reason'] for r in results]

        # –°–æ–∑–¥–∞–µ–º –∏–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        base_name = os.path.splitext(input_file)[0]
        output_file = f"{base_name}_with_sentiment.csv"

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
        df.to_csv(output_file, sep=';', encoding='windows-1251', index=False)
        print_success(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_file}")

        print_step("5. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        base_sentiments = [r['base_sentiment'] for r in results]
        final_sentiments = [r['final_sentiment'] for r in results]
        corrections = sum(1 for r in results if r['was_corrected'])

        base_counts = Counter(base_sentiments)
        final_counts = Counter(final_sentiments)

        print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ê–ù–ê–õ–ò–ó–ê ({len(results)} –ø—Ä–∏–º–µ—Ä–æ–≤):")
        print(f"\n–ë–ê–ó–û–í–´–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø BERT:")
        for sentiment in ['POSITIVE', 'NEUTRAL', 'NEGATIVE']:
            count = base_counts.get(sentiment, 0)
            percentage = count / len(results) * 100
            icon = "üëç" if sentiment == 'POSITIVE' else "üëé" if sentiment == 'NEGATIVE' else "‚ûñ"
            print(f"  {icon} {sentiment}: {count} ({percentage:.1f}%)")

        print(f"\n–§–ò–ù–ê–õ–¨–ù–´–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø (—Å fuzzy):")
        for sentiment in ['POSITIVE', 'NEUTRAL', 'NEGATIVE']:
            count = final_counts.get(sentiment, 0)
            percentage = count / len(results) * 100
            icon = "üëç" if sentiment == 'POSITIVE' else "üëé" if sentiment == 'NEGATIVE' else "‚ûñ"
            print(f"  {icon} {sentiment}: {count} ({percentage:.1f}%)")

        print(f"\nüîÑ FUZZY-–ö–û–†–†–ï–ö–¶–ò–Ø:")
        print(f"  –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: {corrections} –∏–∑ {len(results)} –ø—Ä–∏–º–µ—Ä–æ–≤ ({corrections / len(results) * 100:.1f}%)")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π
        corrected_examples = [r for r in results if r['was_corrected']]
        if corrected_examples:
            print(f"\nüîç –ü–†–ò–ú–ï–†–´ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–• –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:")
            for i, example in enumerate(corrected_examples[:5]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                base_icon = "üëç" if example['base_sentiment'] == 'POSITIVE' else "üëé" if example[
                                                                                           'base_sentiment'] == 'NEGATIVE' else "‚ûñ"
                final_icon = "üëç" if example['final_sentiment'] == 'POSITIVE' else "üëé" if example[
                                                                                             'final_sentiment'] == 'NEGATIVE' else "‚ûñ"
                print(f"  {i + 1}. {base_icon}‚Üí{final_icon} '{example['text'][:80]}...'")
                print(f"     –ü—Ä–∏—á–∏–Ω–∞: {example['correction_reason']}")

        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö
        print_step("6. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã")

        demo_texts = [
            "–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –∑–∞–≤–∏—Å–∞–µ—Ç –∏ –≤—ã–ª–µ—Ç–∞–µ—Ç",
            "–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª–µ–Ω–∏–∏ –æ—Ç–ª–∏—á–Ω–æ–µ, –º–µ–Ω–µ–¥–∂–µ—Ä—ã –≤–µ–∂–ª–∏–≤—ã–µ",
            "–ö—Ä–µ–¥–∏—Ç–Ω—É—é –∫–∞—Ä—Ç—É –æ—Ñ–æ—Ä–º–∏–ª–∏ –±—ã—Å—Ç—Ä–æ, –æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω",
            "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —É–¥–æ–±–Ω–æ–µ, –Ω–æ –∏–Ω–æ–≥–¥–∞ —Ç–æ—Ä–º–æ–∑–∏—Ç"
        ]

        print("\nüß™ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ù–ê –¢–ï–°–¢–û–í–´–• –ü–†–ò–ú–ï–†–ê–•:")
        for text in demo_texts:
            result = analyzer.analyze_sentiment_with_fuzzy(text)
            base_icon = "üëç" if result['base_sentiment'] == 'POSITIVE' else "üëé" if result[
                                                                                      'base_sentiment'] == 'NEGATIVE' else "‚ûñ"
            final_icon = "üëç" if result['final_sentiment'] == 'POSITIVE' else "üëé" if result[
                                                                                        'final_sentiment'] == 'NEGATIVE' else "‚ûñ"
            correction_indicator = " üîÑ" if result['was_corrected'] else ""

            print(f"  {base_icon}‚Üí{final_icon}{correction_indicator} '{text}'")
            print(f"     BERT: {result['base_sentiment']} ({result['base_confidence']:.3f})")
            print(f"     –§–∏–Ω–∞–ª—å–Ω–æ–µ: {result['final_sentiment']} ({result['final_confidence']:.3f})")
            if result['was_corrected']:
                print(f"     –ü—Ä–∏—á–∏–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏: {result['correction_reason']}")
            print()

        return True

    except Exception as e:
        print_error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
        import traceback
        traceback.print_exc()
        return False


def analyze_and_save_sentiment_only():
    """–¢–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –±–µ–∑ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    try:
        print_step("–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        input_file = 'GaspromBank_professional_dataset.csv'
        if not os.path.exists(input_file):
            input_file = 'GaspromBank_dataset.csv'
            if not os.path.exists(input_file):
                print_error("–§–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return False

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = pd.read_csv(input_file, sep=';', encoding='windows-1251')
        print_success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –∏–∑ {input_file}")

        # –ü–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏
        text_column = 'text'
        if text_column not in df.columns:
            for col in df.columns:
                if 'text' in col.lower() or '–æ—Ç–∑—ã–≤' in col.lower():
                    text_column = col
                    break

        # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        analyzer = RadicalSentimentFineTuner()

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        fine_tuned_path = "./radical_fine_tuned_model"
        if os.path.exists(fine_tuned_path):
            analyzer.load_fine_tuned_model(fine_tuned_path)
        else:
            analyzer.load_better_model()

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        texts_to_analyze = df[text_column].astype(str).tolist()
        print(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(texts_to_analyze)} –ø—Ä–∏–º–µ—Ä–æ–≤...")

        results = []
        for text in tqdm(texts_to_analyze, desc="–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"):
            result = analyzer.analyze_sentiment_with_fuzzy(text)
            results.append(result)

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DataFrame
        df['sentiment'] = [r['final_sentiment'] for r in results]
        df['sentiment_confidence'] = [r['final_confidence'] for r in results]
        df['sentiment_was_corrected'] = [r['was_corrected'] for r in results]

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
        base_name = os.path.splitext(input_file)[0]
        output_file = f"{base_name}_with_sentiment.csv"
        df.to_csv(output_file, sep=';', encoding='windows-1251', index=False)

        print_success(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_file}")

        # –ö—Ä–∞—Ç–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        sentiments = [r['final_sentiment'] for r in results]
        sentiment_counts = Counter(sentiments)
        corrections = sum(1 for r in results if r['was_corrected'])

        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        for sentiment in ['POSITIVE', 'NEUTRAL', 'NEGATIVE']:
            count = sentiment_counts.get(sentiment, 0)
            percentage = count / len(results) * 100
            icon = "üëç" if sentiment == 'POSITIVE' else "üëé" if sentiment == 'NEGATIVE' else "‚ûñ"
            print(f"  {icon} {sentiment}: {count} ({percentage:.1f}%)")
        print(f"  üîÑ –ö–æ—Ä—Ä–µ–∫—Ü–∏–π: {corrections} ({corrections / len(results) * 100:.1f}%)")

        return True

    except Exception as e:
        print_error(f"–û—à–∏–±–∫–∞: {e}")
        return False


# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –≤—ã–±–æ—Ä–æ–º —Ä–µ–∂–∏–º–∞"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
        if len(sys.argv) > 1:
            if sys.argv[1] == "--radical-tune":
                print_step("üéØ –†–ï–ñ–ò–ú –†–ê–î–ò–ö–ê–õ–¨–ù–û–ì–û –î–û–û–ë–£–ß–ï–ù–ò–Ø –° FUZZY")
                success = radical_fine_tune_sentiment_model()
                sys.exit(0 if success else 1)
            elif sys.argv[1] == "--analyze-only":
                print_step("üìä –†–ï–ñ–ò–ú –ê–ù–ê–õ–ò–ó–ê –ò –°–û–•–†–ê–ù–ï–ù–ò–Ø")
                success = analyze_and_save_sentiment_only()
                sys.exit(0 if success else 1)

        # –ï—Å–ª–∏ –Ω–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –º–µ–Ω—é
        print("\n" + "=" * 60)
        print("üéØ –í–´–ë–ï–†–ò–¢–ï –†–ï–ñ–ò–ú –†–ê–ë–û–¢–´:")
        print("1 - –û–±—ã—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (—Å –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–µ–π)")
        print("2 - –†–ê–î–ò–ö–ê–õ–¨–ù–û–ï –¥–æ–æ–±—É—á–µ–Ω–∏–µ + FUZZY (90%+ —Ü–µ–ª—å)")
        print("3 - –¢–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV (–±—ã—Å—Ç—Ä–æ)")
        print("=" * 60)

        choice = input("–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä (1, 2 –∏–ª–∏ 3): ").strip()

        if choice == "2":
            success = radical_fine_tune_sentiment_model()
        elif choice == "1":
            success = run_standard_sentiment_analysis()
        elif choice == "3":
            success = analyze_and_save_sentiment_only()
        else:
            print_warning("–ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã.")
            return False

        if success:
            print_success("‚úÖ –û–ü–ï–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
        else:
            print_error("‚ùå –û–ü–ï–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ò–õ–ê–°–¨ –° –û–®–ò–ë–ö–ê–ú–ò")

        return success

    except Exception as e:
        print_error(f"–û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()

    if success:
        print("\n" + "=" * 80)
        print("\033[1;32m‚úÖ –ü–†–û–ì–†–ê–ú–ú–ê –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–ê!\033[0m")
        print("=" * 80)
    else:
        print("\n" + "=" * 80)
        print("\033[1;31m‚ùå –ü–†–û–ì–†–ê–ú–ú–ê –ó–ê–í–ï–†–®–ò–õ–ê–°–¨ –° –û–®–ò–ë–ö–ê–ú–ò\033[0m")
        print("=" * 80)
        sys.exit(1)