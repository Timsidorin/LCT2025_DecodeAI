# MLProcessing_service/main.py
from contextlib import asynccontextmanager
import json
import os
import subprocess
from datetime import datetime

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware

from MLProcessing_service.processed_repository import ProcessedReviewRepository
from models.ReviewInferenceModel import PredictResponse, PredictRequest, PredictionOutput
from core.config import configs
from core.database import get_async_session
from faststream.kafka.fastapi import KafkaRouter
from pydantic import BaseModel, Field
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RawReviewMessage(BaseModel):
    """–ú–æ–¥–µ–ª—å —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ Kafka —Ç–æ–ø–∏–∫–∞ raw_reviews"""
    data: list[dict] = Field(..., description="–°–ø–∏—Å–æ–∫ –æ—Ç–∑—ã–≤–æ–≤")


kafka_router = KafkaRouter(
    configs.BOOTSTRAP_SERVICE,
    schema_url="/asyncapi",
    include_in_schema=True
)


def sync_ml_analysis(data):
    """ –≤—ã–∑–æ–≤ ML –º–æ–¥–µ–ª–∏"""
    input_file_path = None
    output_file_path = None
    try:
        root_dir = os.path.abspath(os.path.join(os.getcwd(), ".."))
        input_file_path = os.path.join(root_dir, "api_input.json")
        output_file_path = os.path.join(root_dir, "api_output.json")

        input_data = {"data": data}
        with open(input_file_path, 'w', encoding='utf-8') as f:
            json.dump(input_data, f, ensure_ascii=False, indent=2)

        command = [
            "python",
            "analyze_single_review.py",
            "--input", "api_input.json",
            "--output", "api_output.json",
            "--workers", "8",
            "--batch-size", "8"
        ]

        result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            timeout=600,
            encoding='utf-8',
            cwd=root_dir
        )

        if result.returncode != 0:
            error_msg = f"ML –º–æ–¥–µ–ª—å –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –∫–æ–¥–æ–º {result.returncode}"
            if result.stderr:
                error_msg += f". STDERR: {result.stderr}"
            raise Exception(error_msg)

        if not os.path.exists(output_file_path):
            raise FileNotFoundError("ML –º–æ–¥–µ–ª—å –Ω–µ —Å–æ–∑–¥–∞–ª–∞ –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª")

        if os.path.getsize(output_file_path) == 0:
            raise ValueError("–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª –ø—É—Å—Ç")

        with open(output_file_path, 'r', encoding='utf-8') as f:
            ml_result = json.load(f)

        if isinstance(ml_result, dict) and "predictions" in ml_result:
            return ml_result["predictions"]
        elif isinstance(ml_result, list):
            return ml_result
        else:
            return [ml_result] if ml_result else []

    finally:
        for file_path in [input_file_path, output_file_path]:
            if file_path and os.path.exists(file_path):
                try:
                    os.unlink(file_path)
                except OSError:
                    pass


async def analyze_sentiment_and_topics_batch(reviews_data: list) -> list:
    """–ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–æ–≤ —á–µ—Ä–µ–∑ ML –º–æ–¥–µ–ª—å"""
    try:
        root_dir = os.path.abspath(os.path.join(os.getcwd(), ".."))
        script_path = os.path.join(root_dir, "analyze_single_review.py")

        if not os.path.exists(script_path):
            raise FileNotFoundError(f"ML —Å–∫—Ä–∏–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {script_path}")
        result = sync_ml_analysis(reviews_data)
        return result

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ ML –∞–Ω–∞–ª–∏–∑–µ: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")


def process_ml_result(review_data: dict, ml_prediction: dict) -> dict:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ML –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ë–î

    ML –º–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    {"id": 1, "topics": ["–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ", "–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ"],
     "sentiments": ["–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ", "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ"]}

    –í –ë–î —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è:
    rating: "positive"/"negative"/"neutral"
    product: "–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ"
    """
    topics = ml_prediction.get("topics", [])
    sentiments = ml_prediction.get("sentiments", [])
    product = topics[0] if topics else None

    sentiment_to_english = {
        "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ": "negative",
        "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ": "positive",
        "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ": "neutral"
    }

    rating = "neutral"
    if "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ" in sentiments:
        rating = "negative"
    elif all(s == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ" for s in sentiments) and len(sentiments) > 0:
        rating = "positive"
    elif "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ" in sentiments:
        rating = "neutral"

    return {
        "text": review_data["text"],
        "source": review_data.get("source", "API"),
        "gender": review_data.get("gender"),
        "city": review_data.get("city"),
        "region": review_data.get("region"),
        "region_code": review_data.get("region_code"),
        "datetime_review": datetime.fromisoformat(review_data["datetime_review"]),
        "rating": rating,
        "product": product
    }


@kafka_router.subscriber("raw_reviews")
async def process_raw_review(msg: RawReviewMessage):
    """
    –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ Kafka —Ç–æ–ø–∏–∫–∞ raw_reviews

    –§–æ—Ä–º–∞—Ç –≤—Ö–æ–¥—è—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è:
    {
        "data": [
            {
                "id": "uuid",
                "text": "—Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞",
                "gender": "–ú",
                "source": "banki.ru",
                "city": "–ú–æ—Å–∫–≤–∞",
                "region": "–≥ –ú–æ—Å–∫–≤–∞",
                "region_code": "RU-MOW",
                "datetime_review": "2025-10-01T20:00:00"
            }
        ]
    }
    """
    try:
        logger.info(f"üì® –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ Kafka: {len(msg.data)} –æ—Ç–∑—ã–≤–æ–≤")
        reviews_for_ml = []
        for idx, review in enumerate(msg.data):
            reviews_for_ml.append({
                "id": idx,
                "text": review["text"]
            })

        logger.info(f"ü§ñ –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ ML –º–æ–¥–µ–ª—å...")
        ml_results = await analyze_sentiment_and_topics_batch(reviews_for_ml)
        logger.info(f"ML –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(ml_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

        async for session in get_async_session():
            repo = ProcessedReviewRepository(session)

            for idx, review_data in enumerate(msg.data):
                try:
                    ml_prediction = ml_results[idx] if idx < len(ml_results) else {}
                    processed_data = process_ml_result(review_data, ml_prediction)
                    result = await repo.add_processed_review(**processed_data)

                    if result:
                        logger.info(
                            f"–û—Ç–∑—ã–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ processed_reviews: {result.uuid} "
                            f"(—Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {result.rating}, –ø—Ä–æ–¥—É–∫—Ç: {result.product})"
                        )
                    else:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç–∑—ã–≤–∞ –≤ –ë–î")

                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–∑—ã–≤–∞ {idx}: {str(e)}")
                    continue

            break

        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {str(e)}")
        import traceback
        traceback.print_exc()
        raise


@asynccontextmanager
async def lifespan(app: FastAPI):
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    try:
        await kafka_router.broker.start()
        logger.info("‚úÖ Kafka –ø–æ–¥–∫–ª—é—á–µ–Ω, —Å–ª—É—à–∞–µ–º —Ç–æ–ø–∏–∫ raw_reviews")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Kafka: {e}")

    try:
        yield
    finally:
        try:
            await kafka_router.broker.close()
            logger.info("‚úÖ Kafka –æ—Ç–∫–ª—é—á–µ–Ω")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∏—è Kafka: {e}")


app = FastAPI(
    title=configs.PROJECT_NAME,
    description="ML Processing Service - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–∑—ã–≤–æ–≤ —á–µ—Ä–µ–∑ ML –º–æ–¥–µ–ª—å",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(kafka_router)


@app.post("/predict", response_model=PredictResponse, description="–†—É—á–Ω–æ–π –ø—Ä–æ–≥–æ–Ω —á–µ—Ä–µ–∑ ML –º–æ–¥–µ–ª—å")
async def predict_sentiment_and_topics(request: PredictRequest):
    """
    Endpoint –¥–ª—è —Ä—É—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ML –º–æ–¥–µ–ª–∏
    –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø–æ—Ç–æ–∫–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    try:
        if len(request.data) == 0:
            raise HTTPException(status_code=400, detail="–°–ø–∏—Å–æ–∫ –æ—Ç–∑—ã–≤–æ–≤ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")

        reviews_data = []
        for review in request.data:
            if review.text.strip():
                reviews_data.append({
                    "id": review.id,
                    "text": review.text.strip()
                })

        if not reviews_data:
            raise HTTPException(status_code=400, detail="–ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ—Ç–∑—ã–≤–æ–≤ —Å –Ω–µ–ø—É—Å—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–º")

        ml_results = await analyze_sentiment_and_topics_batch(reviews_data)

        predictions = []
        results_by_id = {}

        for result in ml_results:
            if isinstance(result, dict) and "id" in result:
                results_by_id[result["id"]] = result

        for review in request.data:
            ml_result = results_by_id.get(review.id, {})
            prediction = PredictionOutput(
                id=review.id,
                topics=ml_result.get("topics", []),
                sentiments=ml_result.get("sentiments", [])
            )
            predictions.append(prediction)

        return PredictResponse(predictions=predictions)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host=configs.HOST, port=configs.PORT, reload=True)
