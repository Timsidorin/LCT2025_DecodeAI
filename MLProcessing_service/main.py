# MLProcessing_service/main.py
import json
import os
import subprocess
import uuid
import threading
from datetime import datetime
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from faststream.kafka.fastapi import KafkaRouter, KafkaMessage
from pydantic import BaseModel, Field
import logging

from core.database import get_async_session
from core.config import configs
from processed_repository import ProcessedReviewRepository
from models.ReviewInferenceModel import PredictResponse, PredictRequest, PredictionOutput

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

ML_MODEL_LOCK = threading.Lock()


class RawReviewMessage(BaseModel):
    data: list[dict] = Field(..., description="–°–ø–∏—Å–æ–∫ –æ—Ç–∑—ã–≤–æ–≤")


class KafkaMessageWrapper(BaseModel):
    msg: str


kafka_router = KafkaRouter(
    configs.BOOTSTRAP_SERVICE,
    schema_url="/asyncapi",
    include_in_schema=True
)


def sync_ml_analysis_optimized(data):
    input_file_path = None
    output_file_path = None

    try:
        temp_id = str(uuid.uuid4())[:8]
        root_dir = os.path.abspath(os.path.join(os.getcwd(), ".."))

        input_file_path = os.path.join(root_dir, f"api_input_{temp_id}.json")
        output_file_path = os.path.join(root_dir, f"api_output_{temp_id}.json")

        with open(input_file_path, 'w', encoding='utf-8') as f:
            json.dump({"data": data}, f, ensure_ascii=False, indent=2)

        command = [
            "python", "-u", "analyze_single_review.py",
            "--input", f"api_input_{temp_id}.json",
            "--output", f"api_output_{temp_id}.json",
            "--workers", "4", "--batch-size", "4"
        ]

        result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            timeout=120,
            encoding='utf-8',
            cwd=root_dir
        )

        if result.returncode != 0:
            error_msg = f"ML –º–æ–¥–µ–ª—å –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –∫–æ–¥–æ–º {result.returncode}"
            if result.stderr:
                error_msg += f". STDERR: {result.stderr}"
            raise Exception(error_msg)

        if not os.path.exists(output_file_path):
            raise FileNotFoundError("ML –º–æ–¥–µ–ª—å –Ω–µ —Å–æ–∑–¥–∞–ª–∞ –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª")

        with open(output_file_path, 'r', encoding='utf-8') as f:
            ml_result = json.load(f)

        if isinstance(ml_result, dict) and "predictions" in ml_result:
            return ml_result["predictions"]
        elif isinstance(ml_result, list):
            return ml_result
        else:
            return [ml_result] if ml_result else []

    except subprocess.TimeoutExpired:
        raise Exception("ML –º–æ–¥–µ–ª—å –ø—Ä–µ–≤—ã—Å–∏–ª–∞ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ ML –∞–Ω–∞–ª–∏–∑–µ: {str(e)}")
        raise
    finally:
        for file_path in [input_file_path, output_file_path]:
            if file_path and os.path.exists(file_path):
                try:
                    os.unlink(file_path)
                except OSError as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª {file_path}: {e}")


async def analyze_sentiment_and_topics_batch(reviews_data: list) -> list:
    try:
        logger.info(f"ü§ñ –ù–∞—á–∞–ª–æ ML –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è {len(reviews_data)} –æ—Ç–∑—ã–≤–æ–≤...")

        with ML_MODEL_LOCK:
            result = sync_ml_analysis_optimized(reviews_data)

        logger.info("ML –∞–Ω–∞–ª–∏–∑ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≤–µ—Ä—à–µ–Ω")
        return result

    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ ML –∞–Ω–∞–ª–∏–∑–µ: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")


def process_ml_result(review_data: dict, ml_prediction: dict) -> dict:
    topics = ml_prediction.get("topics", [])
    sentiments = ml_prediction.get("sentiments", [])

    sentiment_to_english = {
        "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ": "negative",
        "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ": "positive",
        "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ": "neutral"
    }

    first_sentiment = sentiments[0] if sentiments else "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ"

    return {
        "text": review_data["text"],
        "source": review_data.get("source", "API"),
        "gender": review_data.get("gender"),
        "city": review_data.get("city"),
        "region": review_data.get("region"),
        "region_code": review_data.get("region_code"),
        "datetime_review": datetime.fromisoformat(review_data["datetime_review"]),
        "rating": sentiment_to_english.get(first_sentiment, "neutral"),
        "product": topics[0] if topics else None
    }


@kafka_router.subscriber("raw_reviews")
async def process_raw_review(message: KafkaMessage):
    try:
        raw_body = message.body
        message_text = raw_body.decode('utf-8') if isinstance(raw_body, bytes) else str(raw_body)
        logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ Kafka, —Ä–∞–∑–º–µ—Ä: {len(message_text)} —Å–∏–º–≤–æ–ª–æ–≤")

        message_data = json.loads(message_text)
        reviews_data = None

        if "data" in message_data:
            reviews_data = message_data["data"]
        elif "msg" in message_data:
            inner_data = json.loads(message_data["msg"])
            reviews_data = inner_data.get("data") if "data" in inner_data else None

        if not reviews_data or not isinstance(reviews_data, list):
            logger.error("–î–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤–æ–≤ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∏–ª–∏ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Å–ø–∏—Å–∫–æ–º")
            return

        reviews_for_ml = []
        for idx, review in enumerate(reviews_data):
            if isinstance(review, dict) and "text" in review:
                reviews_for_ml.append({"id": idx, "text": review["text"]})

        if not reviews_for_ml:
            logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ—Ç–∑—ã–≤–æ–≤ —Å –≤–∞–ª–∏–¥–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º")
            return

        ml_results = await analyze_sentiment_and_topics_batch(reviews_for_ml)
        logger.info(f"‚úÖ ML –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(ml_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

        ml_results_by_id = {result["id"]: result for result in ml_results if isinstance(result, dict) and "id" in result}

        async for session in get_async_session():
            repo = ProcessedReviewRepository(session)
            saved_count = 0

            for idx, review_data in enumerate(reviews_data):
                try:
                    if not isinstance(review_data, dict) or "text" not in review_data:
                        continue

                    ml_prediction = ml_results_by_id.get(idx, {})
                    processed_data = process_ml_result(review_data, ml_prediction)

                    if await repo.add_processed_review(**processed_data):
                        saved_count += 1
                    else:
                        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç–∑—ã–≤–∞ {idx + 1} –≤ –ë–î")

                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–∑—ã–≤–∞ {idx + 1}: {str(e)}")
                    continue

            break

        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} –æ—Ç–∑—ã–≤–æ–≤")

    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {str(e)}")
        raise


@asynccontextmanager
async def lifespan(app: FastAPI):
    try:
        await kafka_router.broker.start()
        logger.info("Kafka –ø–æ–¥–∫–ª—é—á–µ–Ω, —Å–ª—É—à–∞–µ–º —Ç–æ–ø–∏–∫ raw_reviews")
        logger.info("ML Processing Service –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Kafka: {e}")

    try:
        yield
    finally:
        try:
            await kafka_router.broker.close()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∏—è Kafka: {e}")


app = FastAPI(title=configs.PROJECT_NAME, lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(kafka_router)


@app.post("/predict", response_model=PredictResponse, summary="–†—É—á–Ω–æ–π –ø—Ä–æ–≥–æ–Ω —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å")
async def predict_sentiment_and_topics(request: PredictRequest):
    try:
        if len(request.data) == 0:
            raise HTTPException(status_code=400, detail="–°–ø–∏—Å–æ–∫ –æ—Ç–∑—ã–≤–æ–≤ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")

        reviews_data = [
            {"id": review.id, "text": review.text.strip()}
            for review in request.data if review.text.strip()
        ]

        if not reviews_data:
            raise HTTPException(status_code=400, detail="–ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ—Ç–∑—ã–≤–æ–≤ —Å –Ω–µ–ø—É—Å—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–º")

        ml_results = await analyze_sentiment_and_topics_batch(reviews_data)
        results_by_id = {result["id"]: result for result in ml_results if isinstance(result, dict) and "id" in result}

        predictions = [
            PredictionOutput(
                id=review.id,
                topics=results_by_id.get(review.id, {}).get("topics", []),
                sentiments=results_by_id.get(review.id, {}).get("sentiments", [])
            )
            for review in request.data
        ]

        return PredictResponse(predictions=predictions)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ API endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host=configs.HOST, port=configs.PORT)