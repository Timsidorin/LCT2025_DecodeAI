# MLProcessing_service/main.py
from contextlib import asynccontextmanager
import json
import os
import subprocess
from datetime import datetime
import threading
import uuid

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware

from MLProcessing_service.processed_repository import ProcessedReviewRepository
from models.ReviewInferenceModel import PredictResponse, PredictRequest, PredictionOutput
from core.config import configs
from core.database import get_async_session
from faststream.kafka.fastapi import KafkaRouter
from pydantic import BaseModel, Field
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RawReviewMessage(BaseModel):
    """–ú–æ–¥–µ–ª—å —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ Kafka —Ç–æ–ø–∏–∫–∞ raw_reviews"""
    data: list[dict] = Field(..., description="–°–ø–∏—Å–æ–∫ –æ—Ç–∑—ã–≤–æ–≤")


ML_MODEL_LOCK = threading.Lock()

kafka_router = KafkaRouter(
    configs.BOOTSTRAP_SERVICE,
    schema_url="/asyncapi",
    include_in_schema=True
)


def sync_ml_analysis_optimized(data):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π subprocess –≤—ã–∑–æ–≤ ML –º–æ–¥–µ–ª–∏"""
    input_file_path = None
    output_file_path = None

    try:
        temp_id = str(uuid.uuid4())[:8]
        root_dir = os.path.abspath(os.path.join(os.getcwd(), ".."))

        input_file_path = os.path.join(root_dir, f"api_input_{temp_id}.json")
        output_file_path = os.path.join(root_dir, f"api_output_{temp_id}.json")

        input_data = {"data": data}
        with open(input_file_path, 'w', encoding='utf-8') as f:
            json.dump(input_data, f, ensure_ascii=False, indent=2)

        command = [
            "python", "-u",
            "analyze_single_review.py",
            "--input", f"api_input_{temp_id}.json",
            "--output", f"api_output_{temp_id}.json",
            "--workers", "4",
            "--batch-size", "4"
        ]

        start_time = datetime.now()

        result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            timeout=120,
            encoding='utf-8',
            cwd=root_dir
        )

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        if result.returncode != 0:
            error_msg = f"ML –º–æ–¥–µ–ª—å –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –∫–æ–¥–æ–º {result.returncode}"
            if result.stderr:
                error_msg += f". STDERR: {result.stderr}"
            logger.error(error_msg)
            raise Exception(error_msg)

        if not os.path.exists(output_file_path):
            raise FileNotFoundError("ML –º–æ–¥–µ–ª—å –Ω–µ —Å–æ–∑–¥–∞–ª–∞ –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª")

        if os.path.getsize(output_file_path) == 0:
            raise ValueError("–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª –ø—É—Å—Ç")

        with open(output_file_path, 'r', encoding='utf-8') as f:
            ml_result = json.load(f)

        logger.info(f"ML —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–ª—É—á–µ–Ω, —Ä–∞–∑–º–µ—Ä: {len(ml_result) if isinstance(ml_result, list) else 1}")

        if isinstance(ml_result, dict) and "predictions" in ml_result:
            return ml_result["predictions"]
        elif isinstance(ml_result, list):
            return ml_result
        else:
            return [ml_result] if ml_result else []

    except subprocess.TimeoutExpired:
        logger.error(f"ML –º–æ–¥–µ–ª—å –ø—Ä–µ–≤—ã—Å–∏–ª–∞ timeout {120} —Å–µ–∫—É–Ω–¥")
        raise Exception("ML –º–æ–¥–µ–ª—å –ø—Ä–µ–≤—ã—Å–∏–ª–∞ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ ML –∞–Ω–∞–ª–∏–∑–µ: {str(e)}")
        raise
    finally:
        for file_path in [input_file_path, output_file_path]:
            if file_path and os.path.exists(file_path):
                try:
                    os.unlink(file_path)
                    logger.debug(f"üóë–£–¥–∞–ª–µ–Ω –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {file_path}")
                except OSError as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª {file_path}: {e}")


async def analyze_sentiment_and_topics_batch(reviews_data: list) -> list:
    """–ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–æ–≤ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é ML –º–æ–¥–µ–ª—å"""
    try:
        total_start_time = datetime.now()
        logger.info(f"ü§ñ –ù–∞—á–∞–ª–æ ML –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è {len(reviews_data)} –æ—Ç–∑—ã–≤–æ–≤...")

        root_dir = os.path.abspath(os.path.join(os.getcwd(), ".."))
        script_path = os.path.join(root_dir, "analyze_single_review.py")

        if not os.path.exists(script_path):
            raise FileNotFoundError(f"ML —Å–∫—Ä–∏–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {script_path}")

        with ML_MODEL_LOCK:
            result = sync_ml_analysis_optimized(reviews_data)

        total_end_time = datetime.now()
        total_duration = (total_end_time - total_start_time).total_seconds()

        logger.info(f"ML –∞–Ω–∞–ª–∏–∑ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {total_duration:.2f} —Å–µ–∫—É–Ω–¥")
        logger.info(f"–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(reviews_data) / total_duration:.2f} –æ—Ç–∑—ã–≤–æ–≤/—Å–µ–∫")

        return result

    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ ML –∞–Ω–∞–ª–∏–∑–µ: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")


def sync_ml_analysis(data):
    return sync_ml_analysis_optimized(data)


def process_ml_result(review_data: dict, ml_prediction: dict) -> dict:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ML –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ë–î
    """
    topics = ml_prediction.get("topics", [])
    sentiments = ml_prediction.get("sentiments", [])


    product = topics[0] if topics else None

    rating = "neutral"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é

    if "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ" in sentiments:
        rating = "negative"
    elif all(s == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ" for s in sentiments) and len(sentiments) > 0:
        rating = "positive"
    elif "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ" in sentiments:
        rating = "neutral"

    return {
        "text": review_data["text"],
        "source": review_data.get("source", "API"),
        "gender": review_data.get("gender"),
        "city": review_data.get("city"),
        "region": review_data.get("region"),
        "region_code": review_data.get("region_code"),
        "datetime_review": datetime.fromisoformat(review_data["datetime_review"]),
        "rating": rating,
        "product": product
    }


@kafka_router.subscriber("raw_reviews")
async def process_raw_review(msg: RawReviewMessage):
    """
    –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ Kafka —Ç–æ–ø–∏–∫–∞ raw_reviews
    """
    try:
        request_start_time = datetime.now()
        logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ Kafka: {len(msg.data)} –æ—Ç–∑—ã–≤–æ–≤")

        reviews_for_ml = []
        for idx, review in enumerate(msg.data):
            reviews_for_ml.append({
                "id": idx,
                "text": review["text"]
            })

        ml_results = await analyze_sentiment_and_topics_batch(reviews_for_ml)

        db_start_time = datetime.now()
        async for session in get_async_session():
            repo = ProcessedReviewRepository(session)
            saved_count = 0
            error_count = 0

            for idx, review_data in enumerate(msg.data):
                try:
                    ml_prediction = ml_results[idx] if idx < len(ml_results) else {}
                    processed_data = process_ml_result(review_data, ml_prediction)

                    result = await repo.add_processed_review(**processed_data)

                    if result:
                        logger.info(
                            f"–û—Ç–∑—ã–≤ {idx + 1}/{len(msg.data)} —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {result.uuid} "
                            f"(—Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {result.rating}, –ø—Ä–æ–¥—É–∫—Ç: {result.product})"
                        )
                        saved_count += 1
                    else:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç–∑—ã–≤–∞ {idx + 1} –≤ –ë–î")
                        error_count += 1

                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–∑—ã–≤–∞ {idx + 1}: {str(e)}")
                    error_count += 1
                    continue

            break

        db_end_time = datetime.now()
        db_duration = (db_end_time - db_start_time).total_seconds()

        request_end_time = datetime.now()
        total_duration = (request_end_time - request_start_time).total_seconds()
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {str(e)}")
        import traceback
        traceback.print_exc()
        raise


@asynccontextmanager
async def lifespan(app: FastAPI):
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    try:
        await kafka_router.broker.start()
        logger.info("Kafka –ø–æ–¥–∫–ª—é—á–µ–Ω, —Å–ª—É—à–∞–µ–º —Ç–æ–ø–∏–∫ raw_reviews")
        logger.info("ML Processing Service –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Kafka: {e}")

    try:
        yield
    finally:
        try:
            await kafka_router.broker.close()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∏—è Kafka: {e}")


app = FastAPI(
    title=configs.PROJECT_NAME,
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(kafka_router)


@app.post("/predict", response_model=PredictResponse, description="–†—É—á–Ω–æ–π –ø—Ä–æ–≥–æ–Ω —á–µ—Ä–µ–∑ ML –º–æ–¥–µ–ª—å")
async def predict_sentiment_and_topics(request: PredictRequest):
    """
    """
    try:
        request_start_time = datetime.now()

        if len(request.data) == 0:
            raise HTTPException(status_code=400, detail="–°–ø–∏—Å–æ–∫ –æ—Ç–∑—ã–≤–æ–≤ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")

        reviews_data = []
        for review in request.data:
            if review.text.strip():
                reviews_data.append({
                    "id": review.id,
                    "text": review.text.strip()
                })

        if not reviews_data:
            raise HTTPException(status_code=400, detail="–ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ—Ç–∑—ã–≤–æ–≤ —Å –Ω–µ–ø—É—Å—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–º")
        ml_results = await analyze_sentiment_and_topics_batch(reviews_data)

        predictions = []
        results_by_id = {}

        for result in ml_results:
            if isinstance(result, dict) and "id" in result:
                results_by_id[result["id"]] = result

        for review in request.data:
            ml_result = results_by_id.get(review.id, {})
            prediction = PredictionOutput(
                id=review.id,
                topics=ml_result.get("topics", []),
                sentiments=ml_result.get("sentiments", [])
            )
            predictions.append(prediction)

        request_end_time = datetime.now()
        total_duration = (request_end_time - request_start_time).total_seconds()

        return PredictResponse(predictions=predictions)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ API endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host=configs.HOST, port=configs.PORT, reload=True)

